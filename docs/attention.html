<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 3 Attention | Cognitive Foundations</title>
  <meta name="description" content="Chapter 3 Attention | Cognitive Foundations is a free, open, collaboratively-authored textbook. It is an introduction to cognitive psychology written at the foundational level. This collaborative OER book project represents the work of dozens of authors and collaborators. The first edition brought multiple open texts together, edited into a single book with a consistent voice and formatting. For the second edition, a team of content experts reviewed and updated the materials." />
  <meta name="generator" content="bookdown 0.43 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 3 Attention | Cognitive Foundations" />
  <meta property="og:type" content="book" />
  <meta property="og:image" content="https://pilegard.github.io/cogfoundations/images/cover.png" />
  <meta property="og:description" content="Chapter 3 Attention | Cognitive Foundations is a free, open, collaboratively-authored textbook. It is an introduction to cognitive psychology written at the foundational level. This collaborative OER book project represents the work of dozens of authors and collaborators. The first edition brought multiple open texts together, edited into a single book with a consistent voice and formatting. For the second edition, a team of content experts reviewed and updated the materials." />
  <meta name="github-repo" content="pilegard/cogfoundations" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 3 Attention | Cognitive Foundations" />
  
  <meta name="twitter:description" content="Chapter 3 Attention | Cognitive Foundations is a free, open, collaboratively-authored textbook. It is an introduction to cognitive psychology written at the foundational level. This collaborative OER book project represents the work of dozens of authors and collaborators. The first edition brought multiple open texts together, edited into a single book with a consistent voice and formatting. For the second edition, a team of content experts reviewed and updated the materials." />
  <meta name="twitter:image" content="https://pilegard.github.io/cogfoundations/images/cover.png" />

<meta name="author" content="Edited by Celeste Pilegard" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  <link rel="shortcut icon" href="images/favicon.ico" type="image/x-icon" />
<link rel="prev" href="perception.html"/>
<link rel="next" href="working-memory-chapter.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-ZM78LRWCWG"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-ZM78LRWCWG');
</script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="assets/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="toc-logo"><a href="./"><img src="images/logo.png"></a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>About</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#about-the-book"><i class="fa fa-check"></i>About the Book</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#adoption-and-contact-information"><i class="fa fa-check"></i>Adoption and Contact Information</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#version-history"><i class="fa fa-check"></i>Version History</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#license-and-attributions"><i class="fa fa-check"></i>License and Attributions</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#chapter-1.-history-and-research-methods"><i class="fa fa-check"></i>Chapter 1. History and Research Methods</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#chapter-2.-perception"><i class="fa fa-check"></i>Chapter 2. Perception</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#chapter-3.-attention"><i class="fa fa-check"></i>Chapter 3. Attention</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#chapter-4.-working-memory"><i class="fa fa-check"></i>Chapter 4. Working Memory</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#chapter-5.-long-term-memory"><i class="fa fa-check"></i>Chapter 5. Long-Term Memory</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#chapter-6.-memory-in-context"><i class="fa fa-check"></i>Chapter 6. Memory in Context</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#chapter-7.-knowledge"><i class="fa fa-check"></i>Chapter 7. Knowledge</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#chapter-8.-language"><i class="fa fa-check"></i>Chapter 8. Language</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#chapter-9.-reasoning-and-decision-making"><i class="fa fa-check"></i>Chapter 9. Reasoning and Decision Making</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#chapter-10.-problem-solving"><i class="fa fa-check"></i>Chapter 10. Problem Solving</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="1" data-path="history-and-research-methods.html"><a href="history-and-research-methods.html"><i class="fa fa-check"></i><b>1</b> History and Research Methods</a>
<ul>
<li class="chapter" data-level="1.1" data-path="history-and-research-methods.html"><a href="history-and-research-methods.html#rise-of-cognitive-psychology"><i class="fa fa-check"></i><b>1.1</b> Rise of Cognitive Psychology</a>
<ul>
<li class="chapter" data-level="" data-path="history-and-research-methods.html"><a href="history-and-research-methods.html#experimental-psychologys-foundations"><i class="fa fa-check"></i>Experimental Psychology’s Foundations</a></li>
<li class="chapter" data-level="" data-path="history-and-research-methods.html"><a href="history-and-research-methods.html#the-growth-of-psychology"><i class="fa fa-check"></i>The Growth of Psychology</a></li>
<li class="chapter" data-level="" data-path="history-and-research-methods.html"><a href="history-and-research-methods.html#cognitive-revolution"><i class="fa fa-check"></i>Cognitive Revolution</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="history-and-research-methods.html"><a href="history-and-research-methods.html#research-methods-in-psychology"><i class="fa fa-check"></i><b>1.2</b> Research Methods in Psychology</a>
<ul>
<li class="chapter" data-level="" data-path="history-and-research-methods.html"><a href="history-and-research-methods.html#experimental-research"><i class="fa fa-check"></i>Experimental Research</a></li>
<li class="chapter" data-level="" data-path="history-and-research-methods.html"><a href="history-and-research-methods.html#correlational-designs"><i class="fa fa-check"></i>Correlational Designs</a></li>
<li class="chapter" data-level="" data-path="history-and-research-methods.html"><a href="history-and-research-methods.html#qualitative-designs"><i class="fa fa-check"></i>Qualitative Designs</a></li>
<li class="chapter" data-level="" data-path="history-and-research-methods.html"><a href="history-and-research-methods.html#quasi-experimental-designs"><i class="fa fa-check"></i>Quasi-Experimental Designs</a></li>
<li class="chapter" data-level="" data-path="history-and-research-methods.html"><a href="history-and-research-methods.html#longitudinal-studies"><i class="fa fa-check"></i>Longitudinal Studies</a></li>
<li class="chapter" data-level="" data-path="history-and-research-methods.html"><a href="history-and-research-methods.html#tradeoffs-in-research"><i class="fa fa-check"></i>Tradeoffs in Research</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="history-and-research-methods.html"><a href="history-and-research-methods.html#glossary"><i class="fa fa-check"></i><b>1.3</b> Glossary</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="perception.html"><a href="perception.html"><i class="fa fa-check"></i><b>2</b> Perception</a>
<ul>
<li class="chapter" data-level="2.1" data-path="perception.html"><a href="perception.html#sensation-and-perception"><i class="fa fa-check"></i><b>2.1</b> Sensation and Perception</a>
<ul>
<li class="chapter" data-level="" data-path="perception.html"><a href="perception.html#seeing"><i class="fa fa-check"></i>Seeing</a></li>
<li class="chapter" data-level="" data-path="perception.html"><a href="perception.html#the-sensing-eye-and-the-perceiving-visual-cortex"><i class="fa fa-check"></i>The Sensing Eye and the Perceiving Visual Cortex</a></li>
<li class="chapter" data-level="" data-path="perception.html"><a href="perception.html#perceiving-depth"><i class="fa fa-check"></i>Perceiving Depth</a></li>
<li class="chapter" data-level="" data-path="perception.html"><a href="perception.html#perceiving-form"><i class="fa fa-check"></i>Perceiving Form</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="perception.html"><a href="perception.html#perception-information-integration"><i class="fa fa-check"></i><b>2.2</b> Perception: Information Integration</a>
<ul>
<li class="chapter" data-level="" data-path="perception.html"><a href="perception.html#how-the-perceptual-system-interprets-the-environment"><i class="fa fa-check"></i>How the Perceptual System Interprets the Environment</a></li>
<li class="chapter" data-level="" data-path="perception.html"><a href="perception.html#illusions"><i class="fa fa-check"></i>Illusions</a></li>
<li class="chapter" data-level="" data-path="perception.html"><a href="perception.html#the-important-role-of-expectations-in-perception"><i class="fa fa-check"></i>The Important Role of Expectations in Perception</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="perception.html"><a href="perception.html#glossary-1"><i class="fa fa-check"></i><b>2.3</b> Glossary</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="attention.html"><a href="attention.html"><i class="fa fa-check"></i><b>3</b> Attention</a>
<ul>
<li class="chapter" data-level="3.1" data-path="attention.html"><a href="attention.html#what-is-attention"><i class="fa fa-check"></i><b>3.1</b> What is Attention?</a></li>
<li class="chapter" data-level="3.2" data-path="attention.html"><a href="attention.html#selective-attention"><i class="fa fa-check"></i><b>3.2</b> Selective Attention</a>
<ul>
<li class="chapter" data-level="" data-path="attention.html"><a href="attention.html#the-cocktail-party"><i class="fa fa-check"></i>The Cocktail Party</a></li>
<li class="chapter" data-level="" data-path="attention.html"><a href="attention.html#dichotic-listening-studies"><i class="fa fa-check"></i>Dichotic Listening Studies</a></li>
<li class="chapter" data-level="" data-path="attention.html"><a href="attention.html#models-of-selective-attention"><i class="fa fa-check"></i>Models of Selective Attention</a></li>
<li class="chapter" data-level="" data-path="attention.html"><a href="attention.html#selective-attention-beyond-the-auditory-domain"><i class="fa fa-check"></i>Selective Attention Beyond the Auditory Domain</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="attention.html"><a href="attention.html#controlling-attention"><i class="fa fa-check"></i><b>3.3</b> Controlling Attention</a>
<ul>
<li class="chapter" data-level="" data-path="attention.html"><a href="attention.html#sustaining-attention"><i class="fa fa-check"></i>Sustaining Attention</a></li>
<li class="chapter" data-level="" data-path="attention.html"><a href="attention.html#switching-attention"><i class="fa fa-check"></i>Switching Attention</a></li>
<li class="chapter" data-level="" data-path="attention.html"><a href="attention.html#multitasking"><i class="fa fa-check"></i>Multitasking</a></li>
<li class="chapter" data-level="" data-path="attention.html"><a href="attention.html#distracted-driving"><i class="fa fa-check"></i>Distracted Driving</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="attention.html"><a href="attention.html#glossary-2"><i class="fa fa-check"></i><b>3.4</b> Glossary</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="working-memory-chapter.html"><a href="working-memory-chapter.html"><i class="fa fa-check"></i><b>4</b> Short-term and Working Memory</a>
<ul>
<li class="chapter" data-level="4.1" data-path="working-memory-chapter.html"><a href="working-memory-chapter.html#short-term-memory"><i class="fa fa-check"></i><b>4.1</b> Short-Term Memory</a>
<ul>
<li class="chapter" data-level="" data-path="working-memory-chapter.html"><a href="working-memory-chapter.html#from-the-modal-model-to-baddeleys-working-memory-model"><i class="fa fa-check"></i>From the Modal Model to Baddeley’s Working Memory Model</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="working-memory-chapter.html"><a href="working-memory-chapter.html#working-memory"><i class="fa fa-check"></i><b>4.2</b> Working Memory</a>
<ul>
<li class="chapter" data-level="" data-path="working-memory-chapter.html"><a href="working-memory-chapter.html#the-episodic-buffer"><i class="fa fa-check"></i>The episodic buffer</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="working-memory-chapter.html"><a href="working-memory-chapter.html#glossary-3"><i class="fa fa-check"></i><b>4.3</b> Glossary</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="long-term-memory-chapter.html"><a href="long-term-memory-chapter.html"><i class="fa fa-check"></i><b>5</b> Long-term Memory</a>
<ul>
<li class="chapter" data-level="5.1" data-path="long-term-memory-chapter.html"><a href="long-term-memory-chapter.html#working-memory-vs.-long-term-memory"><i class="fa fa-check"></i><b>5.1</b> Working Memory Vs. Long-Term Memory</a>
<ul>
<li class="chapter" data-level="" data-path="long-term-memory-chapter.html"><a href="long-term-memory-chapter.html#the-serial-position-curve"><i class="fa fa-check"></i>The Serial Position Curve</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="long-term-memory-chapter.html"><a href="long-term-memory-chapter.html#structure"><i class="fa fa-check"></i><b>5.2</b> Structure</a>
<ul>
<li class="chapter" data-level="" data-path="long-term-memory-chapter.html"><a href="long-term-memory-chapter.html#explicit-memory"><i class="fa fa-check"></i>Explicit Memory</a></li>
<li class="chapter" data-level="" data-path="long-term-memory-chapter.html"><a href="long-term-memory-chapter.html#implicit-memory"><i class="fa fa-check"></i>Implicit Memory</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="long-term-memory-chapter.html"><a href="long-term-memory-chapter.html#encoding-retrieval-and-consolidation"><i class="fa fa-check"></i><b>5.3</b> Encoding, Retrieval, and Consolidation</a>
<ul>
<li class="chapter" data-level="" data-path="long-term-memory-chapter.html"><a href="long-term-memory-chapter.html#encoding"><i class="fa fa-check"></i>Encoding</a></li>
<li class="chapter" data-level="" data-path="long-term-memory-chapter.html"><a href="long-term-memory-chapter.html#retrieval"><i class="fa fa-check"></i>Retrieval</a></li>
<li class="chapter" data-level="" data-path="long-term-memory-chapter.html"><a href="long-term-memory-chapter.html#consolidation"><i class="fa fa-check"></i>Consolidation</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="long-term-memory-chapter.html"><a href="long-term-memory-chapter.html#glossary-4"><i class="fa fa-check"></i><b>5.4</b> Glossary</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="memory-in-context.html"><a href="memory-in-context.html"><i class="fa fa-check"></i><b>6</b> Memory in Context</a>
<ul>
<li class="chapter" data-level="6.1" data-path="memory-in-context.html"><a href="memory-in-context.html#kinds-of-memory-biases"><i class="fa fa-check"></i><b>6.1</b> Kinds of Memory Biases</a>
<ul>
<li class="chapter" data-level="" data-path="memory-in-context.html"><a href="memory-in-context.html#schematic-processing-distortions-based-on-expectations"><i class="fa fa-check"></i>Schematic Processing: Distortions Based on Expectations</a></li>
<li class="chapter" data-level="" data-path="memory-in-context.html"><a href="memory-in-context.html#source-monitoring"><i class="fa fa-check"></i>Source Monitoring</a></li>
<li class="chapter" data-level="" data-path="memory-in-context.html"><a href="memory-in-context.html#memory-contamination"><i class="fa fa-check"></i>Memory Contamination</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="memory-in-context.html"><a href="memory-in-context.html#memory-in-the-eyewitness-domain"><i class="fa fa-check"></i><b>6.2</b> Memory in the Eyewitness Domain</a></li>
<li class="chapter" data-level="6.3" data-path="memory-in-context.html"><a href="memory-in-context.html#forgetting"><i class="fa fa-check"></i><b>6.3</b> Forgetting</a>
<ul>
<li class="chapter" data-level="" data-path="memory-in-context.html"><a href="memory-in-context.html#encoding-failure"><i class="fa fa-check"></i>Encoding Failure</a></li>
<li class="chapter" data-level="" data-path="memory-in-context.html"><a href="memory-in-context.html#interference"><i class="fa fa-check"></i>Interference</a></li>
<li class="chapter" data-level="" data-path="memory-in-context.html"><a href="memory-in-context.html#cue-overload-principle"><i class="fa fa-check"></i>Cue-Overload Principle</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="memory-in-context.html"><a href="memory-in-context.html#glossary-5"><i class="fa fa-check"></i><b>6.4</b> Glossary</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="knowledge.html"><a href="knowledge.html"><i class="fa fa-check"></i><b>7</b> Knowledge</a>
<ul>
<li class="chapter" data-level="7.1" data-path="knowledge.html"><a href="knowledge.html#nature-of-categories"><i class="fa fa-check"></i><b>7.1</b> Nature of Categories</a>
<ul>
<li class="chapter" data-level="" data-path="knowledge.html"><a href="knowledge.html#typicality"><i class="fa fa-check"></i>Typicality</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="knowledge.html"><a href="knowledge.html#theories-of-concept-representation"><i class="fa fa-check"></i><b>7.2</b> Theories of Concept Representation</a></li>
<li class="chapter" data-level="7.3" data-path="knowledge.html"><a href="knowledge.html#organization-of-concepts"><i class="fa fa-check"></i><b>7.3</b> Organization of Concepts</a>
<ul>
<li class="chapter" data-level="" data-path="knowledge.html"><a href="knowledge.html#semantic-networks"><i class="fa fa-check"></i>Semantic Networks</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="knowledge.html"><a href="knowledge.html#mental-models"><i class="fa fa-check"></i><b>7.4</b> Mental Models</a>
<ul>
<li class="chapter" data-level="" data-path="knowledge.html"><a href="knowledge.html#a-theory-of-mind"><i class="fa fa-check"></i>A Theory of Mind</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="knowledge.html"><a href="knowledge.html#glossary-6"><i class="fa fa-check"></i><b>7.5</b> Glossary</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="language.html"><a href="language.html"><i class="fa fa-check"></i><b>8</b> Language</a>
<ul>
<li class="chapter" data-level="8.1" data-path="language.html"><a href="language.html#what-is-language"><i class="fa fa-check"></i><b>8.1</b> What is Language?</a>
<ul>
<li class="chapter" data-level="" data-path="language.html"><a href="language.html#linguistic-diversity"><i class="fa fa-check"></i>Linguistic Diversity</a></li>
<li class="chapter" data-level="" data-path="language.html"><a href="language.html#the-components-of-language"><i class="fa fa-check"></i>The Components of Language</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="language.html"><a href="language.html#mechanisms-of-language"><i class="fa fa-check"></i><b>8.2</b> Mechanisms of Language</a>
<ul>
<li class="chapter" data-level="" data-path="language.html"><a href="language.html#language-processing"><i class="fa fa-check"></i>Language Processing</a></li>
<li class="chapter" data-level="" data-path="language.html"><a href="language.html#language-production"><i class="fa fa-check"></i>Language Production</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="language.html"><a href="language.html#language-acquisition"><i class="fa fa-check"></i><b>8.3</b> Language Acquisition</a>
<ul>
<li class="chapter" data-level="" data-path="language.html"><a href="language.html#language-environment-and-cognitive-development"><i class="fa fa-check"></i>Language Environment and Cognitive Development</a></li>
<li class="chapter" data-level="" data-path="language.html"><a href="language.html#bilingualism"><i class="fa fa-check"></i>Bilingualism</a></li>
<li class="chapter" data-level="" data-path="language.html"><a href="language.html#adult-language-acquisition"><i class="fa fa-check"></i>Adult Language Acquisition</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="language.html"><a href="language.html#language-and-thought"><i class="fa fa-check"></i><b>8.4</b> Language and Thought</a></li>
<li class="chapter" data-level="8.5" data-path="language.html"><a href="language.html#glossary-7"><i class="fa fa-check"></i><b>8.5</b> Glossary</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="reasoning-and-decision-making.html"><a href="reasoning-and-decision-making.html"><i class="fa fa-check"></i><b>9</b> Reasoning and Decision Making</a>
<ul>
<li class="chapter" data-level="9.1" data-path="reasoning-and-decision-making.html"><a href="reasoning-and-decision-making.html#deductive-reasoning"><i class="fa fa-check"></i><b>9.1</b> Deductive reasoning</a>
<ul>
<li class="chapter" data-level="" data-path="reasoning-and-decision-making.html"><a href="reasoning-and-decision-making.html#categorical-syllogisms"><i class="fa fa-check"></i>Categorical syllogisms</a></li>
<li class="chapter" data-level="" data-path="reasoning-and-decision-making.html"><a href="reasoning-and-decision-making.html#conditional-syllogisms"><i class="fa fa-check"></i>Conditional syllogisms</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="reasoning-and-decision-making.html"><a href="reasoning-and-decision-making.html#inductive-reasoning"><i class="fa fa-check"></i><b>9.2</b> Inductive reasoning</a>
<ul>
<li class="chapter" data-level="" data-path="reasoning-and-decision-making.html"><a href="reasoning-and-decision-making.html#forms-of-inductive-reasoning"><i class="fa fa-check"></i>Forms of inductive reasoning</a></li>
<li class="chapter" data-level="" data-path="reasoning-and-decision-making.html"><a href="reasoning-and-decision-making.html#reliability-of-conclusions"><i class="fa fa-check"></i>Reliability of conclusions</a></li>
<li class="chapter" data-level="" data-path="reasoning-and-decision-making.html"><a href="reasoning-and-decision-making.html#processes-and-constraints"><i class="fa fa-check"></i>Processes and constraints</a></li>
<li class="chapter" data-level="" data-path="reasoning-and-decision-making.html"><a href="reasoning-and-decision-making.html#induction-vs.-deduction"><i class="fa fa-check"></i>Induction vs. deduction</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="reasoning-and-decision-making.html"><a href="reasoning-and-decision-making.html#decision-making"><i class="fa fa-check"></i><b>9.3</b> Decision making</a>
<ul>
<li class="chapter" data-level="" data-path="reasoning-and-decision-making.html"><a href="reasoning-and-decision-making.html#theories-of-decision-making"><i class="fa fa-check"></i>Theories of Decision Making</a></li>
<li class="chapter" data-level="" data-path="reasoning-and-decision-making.html"><a href="reasoning-and-decision-making.html#constructed-preferences"><i class="fa fa-check"></i>Constructed Preferences</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="problem-solving.html"><a href="problem-solving.html"><i class="fa fa-check"></i><b>10</b> Problem Solving</a>
<ul>
<li class="chapter" data-level="10.1" data-path="problem-solving.html"><a href="problem-solving.html#what-is-a-problem"><i class="fa fa-check"></i><b>10.1</b> What is a problem?</a>
<ul>
<li class="chapter" data-level="" data-path="problem-solving.html"><a href="problem-solving.html#well-defined-problems"><i class="fa fa-check"></i>Well-defined Problems</a></li>
<li class="chapter" data-level="" data-path="problem-solving.html"><a href="problem-solving.html#ill-defined-problems"><i class="fa fa-check"></i>Ill-defined Problems</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="problem-solving.html"><a href="problem-solving.html#restructuring-the-gestalt-approach"><i class="fa fa-check"></i><b>10.2</b> Restructuring: The Gestalt Approach</a>
<ul>
<li class="chapter" data-level="" data-path="problem-solving.html"><a href="problem-solving.html#how-is-a-problem-represented-in-the-mind"><i class="fa fa-check"></i>How is a problem represented in the mind?</a></li>
<li class="chapter" data-level="" data-path="problem-solving.html"><a href="problem-solving.html#insight"><i class="fa fa-check"></i>Insight</a></li>
<li class="chapter" data-level="" data-path="problem-solving.html"><a href="problem-solving.html#fixation"><i class="fa fa-check"></i>Fixation</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="problem-solving.html"><a href="problem-solving.html#solving-problems-by-analogy"><i class="fa fa-check"></i><b>10.3</b> Solving Problems by Analogy</a></li>
<li class="chapter" data-level="10.4" data-path="problem-solving.html"><a href="problem-solving.html#creativity"><i class="fa fa-check"></i><b>10.4</b> Creativity</a>
<ul>
<li class="chapter" data-level="" data-path="problem-solving.html"><a href="problem-solving.html#defining-creativity"><i class="fa fa-check"></i>Defining Creativity</a></li>
<li class="chapter" data-level="" data-path="problem-solving.html"><a href="problem-solving.html#creative-thinking"><i class="fa fa-check"></i>Creative Thinking</a></li>
</ul></li>
<li class="chapter" data-level="10.5" data-path="problem-solving.html"><a href="problem-solving.html#how-do-experts-solve-problems"><i class="fa fa-check"></i><b>10.5</b> How do Experts Solve Problems?</a>
<ul>
<li class="chapter" data-level="" data-path="problem-solving.html"><a href="problem-solving.html#knowledge-1"><i class="fa fa-check"></i>Knowledge</a></li>
<li class="chapter" data-level="" data-path="problem-solving.html"><a href="problem-solving.html#organization"><i class="fa fa-check"></i>Organization</a></li>
<li class="chapter" data-level="" data-path="problem-solving.html"><a href="problem-solving.html#analysis"><i class="fa fa-check"></i>Analysis</a></li>
<li class="chapter" data-level="" data-path="problem-solving.html"><a href="problem-solving.html#the-curse-of-expertise"><i class="fa fa-check"></i>The Curse of Expertise</a></li>
</ul></li>
<li class="chapter" data-level="10.6" data-path="problem-solving.html"><a href="problem-solving.html#glossary-8"><i class="fa fa-check"></i><b>10.6</b> Glossary</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Cognitive Foundations</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="attention" class="section level1 hasAnchor" number="3">
<h1><span class="header-section-number">Chapter 3</span> Attention<a href="attention.html#attention" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p><img src="images/3_attention/attnhead.png" width="100%" style="display: block; margin: auto;" /></p>
<p>Picture yourself driving down a highway. Cars are rushing past, music is playing, your passenger is talking, your phone might be buzzing with notifications, yet somehow you’re able to focus on the road ahead while filtering out most of the chaos around you. This ability to focus on what matters while ignoring distractions is attention in action.</p>
<p>But attention is more than simply “paying attention.” It’s a collection of mental processes that determine what gets into your awareness and what gets filtered out. Right now, as you read this, you’re probably not thinking about the feeling of your clothes against your skin, the sounds outside your window, or the temperature of the room… until now. Attention acts like a spotlight, illuminating some information while leaving the rest in darkness.</p>
<p>The catch? Your spotlight has limitations. Try to focus on too many things at once, and performance suffers. This is why texting while driving is dangerous, why studying with the TV on is ineffective, and why even hands-free phone conversations can impair driving. Understanding attention isn’t just about improving focus; it’s about recognizing the constraints of human consciousness and learning to work within them.</p>
<div id="learning-objectives-2" class="section level5 unnumbered hasAnchor learningobjectives">
<h5>LEARNING OBJECTIVES<a href="attention.html#learning-objectives-2" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<ol style="list-style-type: decimal">
<li>Explain why selective attention is important and how it can be studied.</li>
<li>Understand early dichotic listening experiments that informed how we think about selective attention, and models that have since been proposed to describe how we selectively attend to some things over others.</li>
<li>Understand that our cognitive system can control our attentional resources, while recognizing the limits and constraints we face.</li>
</ol>
</div>
<div id="what-is-attention" class="section level2 hasAnchor" number="3.1">
<h2><span class="header-section-number">3.1</span> What is Attention?<a href="attention.html#what-is-attention" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:attentionsign"></span>
<img src="images/3_attention/attentionsign.jpg" alt="Are you reading these words right here right now? If so, it’s only because you directed your attention toward them. *Photo by Justin Chrn on Unsplash*" width="50%" />
<p class="caption">
Figure 3.1: Are you reading these words right here right now? If so, it’s only because you directed your attention toward them. <em>Photo by Justin Chrn on Unsplash</em>
</p>
</div>
<p>Before we begin exploring attention in its various forms, take a moment to consider how you think about the concept. How would you define attention, or how do you use the term? We certainly use the word very frequently in our everyday language: “ATTENTION! USE ONLY AS DIRECTED!” warns the label on the medicine bottle, meaning be alert to possible danger. “Pay attention!” pleads the weary seventh-grade teacher, not warning about danger (with possible exceptions, depending on the teacher) but urging the students to focus on the task at hand. American psychologist and philosopher William James wrote extensively about attention in the late 1800s. An often quoted passage <span class="citation">(<a href="#ref-james1890principles">James, 1890</a>)</span> beautifully captures how intuitively obvious the concept of attention is, while it remains very difficult to define in concrete and measurable terms:</p>
<blockquote>
<p>Everyone knows what attention is. It is the taking possession by the mind, in clear and vivid form, of one out of what seem several simultaneously possible objects or trains of thought. Focalization, concentration of consciousness are of its essence. It implies withdrawal from some things in order to deal effectively with others. (pp. 381–382)</p>
</blockquote>
<p>Notice that this description touches on the conscious nature of attention, as well as the notion that what is in consciousness is often controlled voluntarily but can also be determined by external events that capture our attention. These events are often sensory in nature, such as the check engine light turning on in your car or the sound of someone calling your name from across the room. Implied in James’ description is the idea that we seem to have a limited capacity for information processing, and that we can only attend to or be consciously aware of a small amount of information at any given time. If someone captures your attention by calling your name, you are - at least momentarily - pulled away from what you were doing before they called you. This relates to the concept of selective attention; some information can be attended to while other information is blocked out or ignored. The first part of this chapter will address selective attention and some models that have been proposed to explain how we selectively attend to different sensory inputs.</p>
<p>As noted above, we often voluntarily control our attention (a process also known as <a href="attention.html#attentional-control">attentional control</a>). Sometimes we may need to have <em>sustained attention</em> or <em>vigilance</em> to complete a particular task. For example, a crucial issue in World War II was how long an individual could remain highly alert and accurate while watching a radar screen for enemy planes, and this problem led psychologists to study how attention works under such conditions. When watching for a rare event, it is easy to become distracted or allow concentration to lag. However, there are other times when we may need to be flexible and <em>switch our attention</em> to something else. For example, if you are reading while your partner is cooking dinner and the smoke alarm goes off, you should probably switch from paying attention to your book to helping him put out the fire. The second part of this chapter addresses how and how effectively we control our attention, including the consequences of overestimating our ability to attend to multiple sources of information or tasks at the same time, such as texting and driving.</p>
</div>
<div id="selective-attention" class="section level2 hasAnchor" number="3.2">
<h2><span class="header-section-number">3.2</span> Selective Attention<a href="attention.html#selective-attention" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="the-cocktail-party" class="section level3 unnumbered hasAnchor">
<h3>The Cocktail Party<a href="attention.html#the-cocktail-party" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><a href="attention.html#selective-attention">Selective attention</a> is the ability to select certain stimuli in the environment to process, while ignoring distracting information. One way to get an intuitive sense of how attention works is to consider situations in which attention is used. A party provides an excellent example for our purposes.</p>
<p>Imagine many people may be milling around, a dazzling variety of colors and sounds and smells, the buzz of many conversations. When walking around, you don’t have to be looking at the person talking; you may start listening with great interest to some gossip while pretending not to hear, and may easily switch to listening to another conversation that grabs your attention as new people walk by. However, once you are engaged in conversation with someone, you quickly become aware that you cannot keep listening to other conversations at the same time. You are also probably <em>not</em> aware of how tight your shoes feel or of the smell of a nearby flower arrangement.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:cocktailparty"></span>
<img src="images/3_attention/cocktailparty.jpg" alt="Beyond just hearing your name from the clamor at a party, other words or concepts, particularly unusual or significant ones to you, can also snag your attention. *Photo by Michael Discenza on Unsplash*" width="45%" />
<p class="caption">
Figure 3.2: Beyond just hearing your name from the clamor at a party, other words or concepts, particularly unusual or significant ones to you, can also snag your attention. <em>Photo by Michael Discenza on Unsplash</em>
</p>
</div>
<p>On the other hand, if someone behind you mentions your name, you typically notice it immediately and may start attending to that (much more interesting) conversation. This situation highlights an interesting set of observations. We have an amazing ability to select and track one voice or visual object, even when many things are competing for our attention. But at the same time, we seem to be limited in how much we can attend to at one time, which in turn suggests that attention is crucial in selecting what is important. How does it all work?</p>
</div>
<div id="dichotic-listening-studies" class="section level3 unnumbered hasAnchor">
<h3>Dichotic Listening Studies<a href="attention.html#dichotic-listening-studies" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>This cocktail party scenario is the quintessential example of selective attention, and it is essentially what some early researchers tried to replicate under controlled laboratory conditions as a starting point for understanding the role of attention in perception <span class="citation">(e.g., <a href="#ref-Cherry1953">Cherry, 1953</a>; <a href="#ref-Moray1959">Moray, 1959</a>)</span>. In particular, they used <a href="attention.html#dichotic-listening">dichotic listening</a> and <a href="attention.html#shadowing">shadowing</a> tasks to evaluate the selection process. Dichotic listening simply refers to the situation when two messages are presented simultaneously to an individual, with one message in each ear. In order to control which message the person attends to, the individual is asked to repeat back or “shadow” one of the messages as he hears it. For example, let’s say that a story about a camping trip is presented to John’s left ear, and a story about Abe Lincoln is presented to his right ear. The typical dichotic listening task would have John repeat the story presented to one ear as he hears it. Can he do that without being distracted by the information in the other ear?</p>
<p>People can become pretty good at the shadowing task, and they can easily report the content of the message that they attended to. But what happens to the message they ignored? Typically, people can tell you if the ignored message sounded masculine or feminine, or other physical characteristics of the speech, but they cannot tell you what the message was about. In fact, many studies have shown that people in a shadowing task were not aware of a change in the language of the message (e.g., from English to German; <span class="citation">Cherry (<a href="#ref-Cherry1953">1953</a>)</span>), and they didn’t even notice when the same word was repeated in the unattended ear more than 35 times <span class="citation">(<a href="#ref-Moray1959">Moray, 1959</a>)</span>! Only the basic physical characteristics, such as the pitch of the unattended message, could be reported.</p>
<p>On the basis of these types of experiments, we clearly have a limited capacity for processing information for meaning, making the selection process all the more important. How does this selection process work?</p>
</div>
<div id="models-of-selective-attention" class="section level3 unnumbered hasAnchor">
<h3>Models of Selective Attention<a href="attention.html#models-of-selective-attention" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="broadbents-filter-model" class="section level4 unnumbered hasAnchor">
<h4>Broadbent’s Filter Model<a href="attention.html#broadbents-filter-model" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Many researchers have investigated how selection occurs and what happens to ignored information. Donald Broadbent was one of the first to try to characterize the selection process. His Filter Model was based on the dichotic listening tasks described above as well as other types of experiments <span class="citation">(<a href="#ref-Broadbent1958">Broadbent &amp; Dal Martello, 1958</a>)</span>. He found that people select information on the basis of <em>physical features</em>: e.g., the sensory channel (or ear) that a message was coming in, the pitch of the voice, the color or font of a visual message. People seemed vaguely aware of the physical features of the unattended information, but had no knowledge of the meaning. As a result, Broadbent argued that selection occurs <em>very early</em>, with no additional processing for the unselected information. A flowchart of the model might look like Figure <a href="attention.html#fig:broadbent">3.3</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:broadbent"></span>
<img src="images/3_attention/broadbent.png" alt="Broadbent Filter Model. This figure shows information coming in both the left and right ears. Some basic sensory information, such as pitch, is processed, but an internal filter only allows the information from one ear to be processed further. Only the information from the left ear is transferred to short-term memory (STM) and conscious awareness, and then further processed for meaning. Under this model, ignored information never makes it beyond a basic physical analysis." width="80%" />
<p class="caption">
Figure 3.3: Broadbent Filter Model. This figure shows information coming in both the left and right ears. Some basic sensory information, such as pitch, is processed, but an internal filter only allows the information from one ear to be processed further. Only the information from the left ear is transferred to short-term memory (STM) and conscious awareness, and then further processed for meaning. Under this model, ignored information never makes it beyond a basic physical analysis.
</p>
</div>
</div>
<div id="treismans-attenuation-model" class="section level4 unnumbered hasAnchor">
<h4>Treisman’s Attenuation Model<a href="attention.html#treismans-attenuation-model" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Broadbent’s model intuitively makes sense, but you may have noticed that one problem is that it cannot account for all aspects of the Cocktail Party Effect. What doesn’t fit? The fact is that you tend to hear your own name when it is spoken by someone, even if you are deeply engaged in a conversation. We mentioned earlier that people in a shadowing experiment were unaware of a word in the unattended ear that was repeated many times — and yet many people noticed their own name in the unattended ear even it occurred only once.</p>
<p>Anne <span class="citation">Treisman (<a href="#ref-Treisman1960">1960</a>)</span> carried out a number of dichotic listening experiments in which she presented two different stories to the two ears. In line with the standard procedure, she asked people to shadow the message in one ear. As the stories progressed, however, she switched the stories to the opposite ears. Treisman found that individuals spontaneously followed the story, or the content of the message, when it shifted from the left ear to the right ear. Then they realized they were shadowing the wrong ear and switched back.</p>
<p>Results like this, and the fact that you tend to hear meaningful information even when you aren’t paying attention to it, suggest that we <em>do</em> monitor the unattended information to some degree on the basis of its meaning. Therefore, Broadbent’s Filter Model can’t be right because it suggests that unattended information is completely blocked at the sensory analysis level. Instead, Treisman suggested that selection starts at the physical or perceptual level, but that the unattended information is not blocked completely, it is just weakened or <em>attenuated</em>. As a result, highly meaningful or pertinent information in the unattended ear will get through the filter for further processing at the level of meaning. A flowchart of her model might look like Figure <a href="attention.html#fig:treisman">3.4</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:treisman"></span>
<img src="images/3_attention/treisman.png" alt="Treisman Attenuation Model, an early selection model. This figure shows information coming in both ears, but in contrast to the early selection model, there is no filter that completely blocks nonselected information. Instead, selection of the left ear information strengthens that material, while the nonselected information in the right ear is weakened. However, if the preliminary analysis shows that the nonselected informatio is especially pertinent or meaningful (such as your own name) then the Attenuation Control will instead strengthen the more meaningful information." width="80%" />
<p class="caption">
Figure 3.4: Treisman Attenuation Model, an early selection model. This figure shows information coming in both ears, but in contrast to the early selection model, there is no filter that completely blocks nonselected information. Instead, selection of the left ear information strengthens that material, while the nonselected information in the right ear is weakened. However, if the preliminary analysis shows that the nonselected informatio is especially pertinent or meaningful (such as your own name) then the Attenuation Control will instead strengthen the more meaningful information.
</p>
</div>
</div>
<div id="late-selection-models" class="section level4 unnumbered hasAnchor">
<h4>Late Selection Models<a href="attention.html#late-selection-models" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Other selective attention models have been proposed as well. A <em>late selection</em> or <em>response selection</em> model proposed by <span class="citation">Deutsch &amp; Deutsch (<a href="#ref-Deutsch1963">1963</a>)</span> suggests that all information in the unattended ear is processed on the basis of meaning, not just the selected or highly pertinent information (Figure <a href="attention.html#fig:lateselection">3.5</a>). However, only the information that is relevant for the task response gets into conscious awareness. This model is consistent with ideas of subliminal perception; in other words, that you don’t have to be aware of or attending a message for it to be fully processed for meaning.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:lateselection"></span>
<img src="images/3_attention/lateselection.png" alt="Deutsch and Deutsch late selection model. This figure shows a similar structure to the early selection model, with the major difference being that the location of the selective filter has changed, here being later on in the process. Here, the model makes the assumption that analysis of meaning occurs before selection occurs, but only the selected information becomes conscious." width="80%" />
<p class="caption">
Figure 3.5: Deutsch and Deutsch late selection model. This figure shows a similar structure to the early selection model, with the major difference being that the location of the selective filter has changed, here being later on in the process. Here, the model makes the assumption that analysis of meaning occurs before selection occurs, but only the selected information becomes conscious.
</p>
</div>
</div>
<div id="load-theory-of-attention" class="section level4 unnumbered hasAnchor">
<h4>Load Theory of Attention<a href="attention.html#load-theory-of-attention" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Why did researchers keep coming up with different models? Because no model really seemed to account for all the data, some of which indicates that nonselected information is blocked completely, whereas some suggests that it can be processed for meaning. The load theory of attention addresses this apparent inconsistency, suggesting that the stage at which selection occurs can change depending on the task. <span class="citation">Johnston &amp; Heinz (<a href="#ref-Johnston1978">1978</a>)</span> demonstrated that under some conditions, we can select what to attend to at a very early stage and we do not process the content of the unattended message very much at all. Analyzing physical information, such as attending to information based on whether it sounds like a masculine or feminine voice, is relatively easy; it occurs automatically, rapidly, and doesn’t take much effort. Under the right conditions, we can select what to attend to on the basis of the meaning of the messages. However, the late selection option—processing the content of all messages before selection—is more difficult and requires more effort. The benefit, though, is that we have the flexibility to change how we deploy our attention depending upon what we are trying to accomplish, which is one of the greatest strengths of our cognitive system.</p>
</div>
</div>
<div id="selective-attention-beyond-the-auditory-domain" class="section level3 unnumbered hasAnchor">
<h3>Selective Attention Beyond the Auditory Domain<a href="attention.html#selective-attention-beyond-the-auditory-domain" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>This discussion of selective attention has focused on experiments using auditory material, but the same principles hold for other sensory systems as well. <span class="citation">Neisser (<a href="#ref-neisser1979control">1979</a>)</span> investigated some of the same questions with visual materials by superimposing two semi-transparent video clips and asking viewers to attend to just one series of actions. As with the auditory materials, viewers often were unaware of what went on in the other video, despite it being clearly visible. Twenty years later, <span class="citation">Simons &amp; Chabris (<a href="#ref-Simons1999">1999</a>)</span> explored and expanded these findings using similar techniques, and triggered a flood of new work in an area referred to as <a href="attention.html#inattentional-blindness">inattentional blindness</a>. In the original study, participants were instructed to complete a task that required paying close attention to certain features of a video clip; in doing so, many of them completely missed other features, such as a man in a gorilla costume walking into the scene <span class="citation">(<a href="#ref-Simons1999">Simons &amp; Chabris, 1999</a>)</span>.</p>
<div id="subliminal-perception" class="section level5 unnumbered hasAnchor fyi">
<h5>Subliminal Perception<a href="attention.html#subliminal-perception" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>The idea of <em>subliminal perception</em> — that stimuli presented below the threshold for awareness can influence thoughts, feelings, or actions — is a fascinating and kind of creepy one. Can messages you are unaware of, embedded in movies or ads or the music playing in the grocery store, really influence what you buy? Many such claims of the power of subliminal perception have been made. One of the most famous came from a market researcher who claimed that the message “Eat Popcorn” briefly flashed throughout a movie increased popcorn sales by more than 50%, although he later admitted that the study was made up <span class="citation">(<a href="#ref-Merikle2004">Merikle, 2004</a>)</span>. Psychologists have worked hard to investigate whether this is a valid phenomenon. But studying subliminal perception is more difficult than it might seem, because of the difficulty of establishing what the threshold for consciousness is or of even determining what type of threshold is important. For example, <span class="citation">Cheesman &amp; Merikle (<a href="#ref-Cheesman1984">1984</a>)</span> made an important distinction between objective and subjective thresholds <span class="citation">(see also <a href="#ref-Cheesman1986">Cheesman &amp; Merikle, 1986</a>)</span>. The bottom line is that there is some evidence that individuals can be influenced by stimuli they are not aware of, but how complex the stimuli can be or the extent to which unconscious material can affect behavior is not settled <span class="citation">(e.g., <a href="#ref-Bargh2014">Bargh, 2014</a>; <a href="#ref-Greenwald1992">Greenwald, 1992</a>; <a href="#ref-harris2013two">Harris et al., 2013</a>; <a href="#ref-Merikle2004">Merikle, 2004</a>)</span>.</p>
</div>
</div>
</div>
<div id="controlling-attention" class="section level2 hasAnchor" number="3.3">
<h2><span class="header-section-number">3.3</span> Controlling Attention<a href="attention.html#controlling-attention" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>As mentioned in the previous section, one of the greatest strengths of our cognitive system is the ability to control how we deploy our cognitive resources to achieve our goals, also known as <a href="attention.html#cognitive-control">cognitive control</a>.In the attention domain, we can try to devote all our attention to one thing or we can try to switch our attention to something else. But how good are we at each of these processes? And what happens when we try to do too much at once?</p>
<div id="sustaining-attention" class="section level3 unnumbered hasAnchor">
<h3>Sustaining Attention<a href="attention.html#sustaining-attention" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Imagine trying to do your homework with many external and internal distractors: your phone buzzes from an incoming text message, a thought pops into your head about what you want to eat for dinner, a colorful hummingbird flies by your window. If you have a goal to finish your homework assignment before the deadline, you may have to actively focus your attention on your homework despite these distractors. One part of cognitive control is <a href="attention.html#inhibitory-control">inhibitory control</a>, or the suppression of goal-irrelevant stimuli (<em>attentional inhibition</em>) or responses (<em>response inhibition</em>) <span class="citation">(<a href="#ref-Tiego2018">Tiego et al., 2018</a>)</span>. Attentional inhibition is thought to be an important aspect of sustaining attention.</p>
<div id="stroop-experiments" class="section level4 unnumbered hasAnchor">
<h4>Stroop Experiments<a href="attention.html#stroop-experiments" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>A common task used to study attention is the Stroop task, named for J.R. Stroop who described it in one of the most highly cited experimental psychology papers ever published <span class="citation">(<a href="#ref-Stroop1935">Stroop, 1935</a>)</span>. In the classic Stroop task, participants are shown words in different colors, and instructed to say out loud the color of the word (not the word itself) as quickly and accurately as they can. That is, their task is to pay attention to the ink color, and ignore anything else that might distract them. Sometimes words match the color they are printed in, such as the words on the left in Figure <a href="attention.html#fig:stroop">3.6</a>. Other times words are printed in a color that differs from their meanings, such as the words on the right.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:stroop"></span>
<img src="images/3_attention/stroop.png" alt="Example of congruent (left) and incongruent (right) stimuli in a classic Stroop paradigm." width="60%" />
<p class="caption">
Figure 3.6: Example of congruent (left) and incongruent (right) stimuli in a classic Stroop paradigm.
</p>
</div>
<p>When the word meaning matches its ink color, or they are <em>congruent</em>, the task is pretty easy and participants respond relatively quickly and accurately. However, when the word meaning doesn’t match its ink color, or they are <em>incongruent</em>, participants tend to respond slower and make more errors (often by reading out the word, rather than its color). Try it yourself! Even with the simple example above, you might notice that you get tripped up with the incongruent stimuli. This is because with incongruent stimuli there is <em>interference</em> between processing the physical features of the word (color) and its semantics (meaning), and we need to inhibit the irrelevant yet salient semantic information to succeed at the task. Even if we try to attend to just one thing, we can still be thrown off if other things are distracting enough.</p>
</div>
</div>
<div id="switching-attention" class="section level3 unnumbered hasAnchor">
<h3>Switching Attention<a href="attention.html#switching-attention" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In our previous example about doing homework, we called attention-grabbing items or events “distractors.” However, sometimes our goals change or important new information comes up, and we <em>want</em> or <em>need</em> to switch our attention to something else. In this case, such inputs aren’t distractions, but rather helpful cues to switch our attention. Up until now, it may have sounded like having your attention pulled away or actively switching attention is quite easy and natural. However, it turns out that switching attention is cognitively demanding and can impair performance.</p>
<div id="task-switching-experiments" class="section level4 unnumbered hasAnchor">
<h4>Task Switching Experiments<a href="attention.html#task-switching-experiments" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>A large body of work studies <a href="attention.html#cognitive-flexibility">cognitive flexibility</a>, or how we adapt our cognition to new or changing environments or goals. A typical task switching experiment involves first training participants to complete two or more simple tasks that relate to the same set of stimuli, and then having them switch back and forth between them <span class="citation">(<a href="#ref-Monsell2003">Monsell, 2003</a>)</span>. For example, researchers can train participants to classify the number (e.g., odd or even) <em>or</em> the letter (e.g., vowel or consonant) when shown a number-letter pair. If a subject had to complete the number classification task for these stimuli – E1, 8Z, D3, 7U – the correct responses would be “odd”, “even”, “odd”, “odd”. But if they were cued to switch back and forth between the two tasks for the same stimuli in this example, the correct responses would be “odd”, “consonant”, “odd”, “vowel”.</p>
<p>In order to successfully switch in a task-switching experiment, it is thought that some mental processes must happen, which can include switching attention between stimuli or concepts, retrieving different goal states and task-set rules, and activating or adjusting responses <span class="citation">(<a href="#ref-Monsell2003">Monsell, 2003</a>)</span>. Many different task-switching paradigms have been used in psychology and cognitive neuroscience to understand how we switch between doing different tasks. One reliable behavioral finding is that participants are slower and less accurate in their responding on switch trials compared to non-switch trials. This suggests that switching is cognitively demanding and comes at a cost.</p>
</div>
</div>
<div id="multitasking" class="section level3 unnumbered hasAnchor">
<h3>Multitasking<a href="attention.html#multitasking" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In spite of the evidence of our limited capacity, many of us like to think that we can do several things at once. Some people claim to be able to <em>multitask</em> without any problem: reading a textbook while watching television or talking with friends, talking on the phone while playing video games, even texting while driving. The fact is that sometimes we <em>seem</em> to juggle several things at once, particularly when some or all of the tasks are “easy.” For example, as we talk to someone while walking down the street, we don’t need to think consciously about what muscle to contract to take our next step. In fact, paying attention to automated skills can lead to a breakdown in performance, or “choking” <span class="citation">(e.g., <a href="#ref-Beilock2001">Beilock &amp; Carr, 2001</a>)</span>. But what about higher level, more mentally demanding tasks? Is it possible to perform multiple complex tasks <em>at the same time</em>?</p>
<p>In one study, two participants were trained to take dictation for spoken words while reading unrelated material for comprehension <span class="citation">(<a href="#ref-Spelke1976">Spelke et al., 1976</a>)</span>. To establish a baseline performance and determine the amount of cognitive resources needed for each task, the participants first performed each task separately. Then they performed both tasks simultaneously. Next, they completed extensive practice of one hour per day, five days a week, for 17 weeks. Remarkably, the participants were able to learn dictation for lists of words and read for comprehension without any decline in performance for either task. The authors suggested that this may indicate there are no fixed limits on our attentional capacity. However, when the participants were asked to switch to different tasks, such as reading aloud instead of silently, performance was initially impaired. Therefore, the ability to multitask appears to be specific to well-learned tasks.</p>
<p>Unless a task is fully automated, many researchers suggest that “multitasking” doesn’t exist – even when you think you are multitasking, you are really just rapidly switching your attention back and forth. A body of research using dual-task paradigms suggests that earlier estimates of our capacity for doing two or more things at the same time were overly optimistic; some cognitive operations cause “bottlenecks” that require exclusive use of a cognitive resource and therefore cannot be done concurrently <span class="citation">(<a href="#ref-Pashler1993">Pashler, 1993</a>)</span>. This has been supported in experimental psychology and applied to real-world situations, including distracted driving (see next section) and emergency medicine. One review paper argued that it is impossible to multitask unless behaviors are completely automatic, and that in emergency medicine, physicians are instead rapidly switching between small tasks, which can come at a cost <span class="citation">(<a href="#ref-Skaugset2016">Skaugset et al., 2016</a>)</span>.</p>
</div>
<div id="distracted-driving" class="section level3 unnumbered hasAnchor">
<h3>Distracted Driving<a href="attention.html#distracted-driving" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In today’s technology-driven society, questions regarding multitasking while using electronic devices have become increasingly relevant. Specifically, research investigating the effects of multitasking while driving—under controlled conditions—has produced some surprising results. While distractions such as applying makeup, tending to children in the back seat, fiddling a CD player, or eating a bowl of cereal while driving can impair performance, we often overestimate our ability to multitask behind the wheel. Despite this, cars are being built with ever more advanced technological capabilities that further encourage multitasking. Given these factors, it is important to ask how effective we truly are at dividing our attention in such situations.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:driving"></span>
<img src="images/3_attention/driving.jpeg" alt="If you look at your phone for just 5 seconds while driving at 55mph, that means you have driven the length of a football field without looking at the road. *Photo by Alexandre Boucher on Unsplash*" width="50%" />
<p class="caption">
Figure 3.7: If you look at your phone for just 5 seconds while driving at 55mph, that means you have driven the length of a football field without looking at the road. <em>Photo by Alexandre Boucher on Unsplash</em>
</p>
</div>
<p>Most people acknowledge the distraction caused by texting while driving and the reason seems obvious: Your eyes are off the road and your hands and at least one hand (often both) are engaged while texting. However, the problem is not simply one of occupied hands or eyes, but rather that the cognitive demands on our limited capacity systems can seriously impair driving performance <span class="citation">(<a href="#ref-Strayer2011">Strayer et al., 2011</a>)</span>. The effect of a cell phone conversation on performance (such as not noticing someone’s brake lights or responding more slowly to them) is just as significant when the individual is having a conversation with a hands-free device as with a handheld phone; the same impairments do not occur when listening to the radio or a book on tape <span class="citation">(<a href="#ref-Strayer2001">Strayer &amp; Johnston, 2001</a>)</span>. Moreover, studies using eye-tracking devices have shown that drivers are less likely to later recognize objects that they did look at when using a cell phone while driving <span class="citation">(<a href="#ref-Strayer2007">Strayer &amp; Drews, 2007</a>)</span>. These findings demonstrate that cognitive distractions such as cell phone conversations can produce inattentional blindness, or a lack of awareness of what is right before your eyes <span class="citation">(see also <a href="#ref-Simons1999">Simons &amp; Chabris, 1999</a>)</span>. Sadly, although we may think that we can multitask while driving, in fact the percentage of people who can truly perform cognitive tasks without impairing their driving performance is estimated to be only about 2% <span class="citation">(<a href="#ref-Watson2010">Watson &amp; Strayer, 2010</a>)</span>.</p>
<div id="attention-deficithyperactivity-disorder-adhd" class="section level5 unnumbered hasAnchor fyi">
<h5>Attention-Deficit/Hyperactivity Disorder (ADHD)<a href="attention.html#attention-deficithyperactivity-disorder-adhd" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>By the end of their first decade of life, typically developing children have mastered the complex cognitive operations required to comply with the rules, such as stopping themselves from acting impulsively, paying attention to parents and teachers in the face of distraction, and sitting still despite boredom. For children with Attention-Deficit/Hyperactivity Disorder (ADHD), exercising self-control is a unique challenge. Some people used to believe that children with ADHD were willfully noncompliant due to moral or motivational deficits <span class="citation">(<a href="#ref-Still1902">Still, 1902</a>)</span>. However, scientists now know that noncompliance observed in ADHD can be explained by many factors, including neurological dysfunction.</p>
<p>ADHD is the most commonly diagnosed childhood behavior disorder, affecting 3% to 7% of children in the United States, according to the American Psychiatric <span class="citation">Association (<a href="#ref-APA2000">2000</a>)</span>. The core symptoms of ADHD are organized into two clusters: hyperactivity/impulsivity and inattention. Hyperactive and impulsive symptoms are closely related, the former involving moving perpetually (even when stillness is expected) and the latter including acting without considering repercussions. Inattentive symptoms describe difficulty with organization and task follow-through, as well as a tendency to be distracted by external stimuli. Broadly speaking, boys are more likely than girls to experience symptoms from the hyperactive and impulsive cluster <span class="citation">(<a href="#ref-Hartung1998">Hartung &amp; Widiger, 1998</a>)</span>, while girls are more likely to experience symptoms from the inattentive cluster <span class="citation">(<a href="#ref-Quinn2014">Quinn &amp; Madhoo, 2014</a>)</span>. Gender differences in how ADHD presents, combined with other factors such as how parents, teachers, or clinicians notice and interpret ADHD symptoms, have contributed to many women and girls with ADHD not being diagnosed or treated <span class="citation">(<a href="#ref-Quinn2014">Quinn &amp; Madhoo, 2014</a>)</span>.</p>
</div>
<div id="key-takeaways-2" class="section level5 unnumbered hasAnchor takeaways">
<h5>Key Takeaways<a href="attention.html#key-takeaways-2" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<ul>
<li>It may be useful to think of attention as a mental resource, one that is needed to focus on and fully process important information, especially when there is a lot of distracting “noise” threatening to obscure the message.</li>
<li>Our selective attention system allows us to find or track an object or conversation in the midst of distractions. Whether the selection process occurs early or late in the analysis of those events has been the focus of considerable research, and in fact how selection occurs may very well depend on the specific conditions.</li>
<li>With respect to controlling our attention, in general we can only perform one cognitively demanding task at a time, and even then, we can be distracted from performing that task if the conditions are right. Switching back and forth between different tasks also comes at a cost.</li>
<li>When we focus our attention on one task or source of information, we may not even be aware of unattended events even though they might seem too obvious to miss. This type of inattention blindness can occur even in well-learned tasks, such as driving while talking on a cell phone.</li>
</ul>
</div>
<div id="exercises-2" class="section level5 unnumbered hasAnchor exercises">
<h5>Exercises<a href="attention.html#exercises-2" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<ol style="list-style-type: decimal">
<li>Discuss: Discuss the implications of the different models of selective attention for everyday life. For instance, what advantages and disadvantages would be associated with being able to filter out all unwanted information at a very early stage in processing? What are the implications of processing all ignored information fully, even if you aren’t consciously aware of that information?</li>
<li>Practice: Think of examples of when you feel you can successfully multitask and when you can’t. What aspects of the tasks or the situation seem to influence divided attention performance? How accurate do you think you are in judging your own multitasking ability?</li>
<li>Apply: What are the public policy implications of current evidence of inattentional blindness as a result of distracted driving? Should this evidence influence traffic safety laws? What additional studies of distracted driving would you propose?</li>
</ol>
</div>
</div>
</div>
<div id="glossary-2" class="section level2 hasAnchor" number="3.4">
<h2><span class="header-section-number">3.4</span> Glossary<a href="attention.html#glossary-2" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="attentional-control" class="section level5 unnumbered hasAnchor">
<h5>attentional control<a href="attention.html#attentional-control" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>Our ability to choose what we pay attention to</p>
</div>
<div id="cognitive-control" class="section level5 unnumbered hasAnchor">
<h5>cognitive control<a href="attention.html#cognitive-control" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>The ability to regulate how we deploy our cognitive resources to achieve our goals</p>
</div>
<div id="cognitive-flexibility" class="section level5 unnumbered hasAnchor">
<h5>cognitive flexibility<a href="attention.html#cognitive-flexibility" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>How we adapt our cognition to new or changing environments or goals</p>
</div>
<div id="dichotic-listening" class="section level5 unnumbered hasAnchor">
<h5>dichotic listening<a href="attention.html#dichotic-listening" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>An experimental task in which two messages are presented to different ears.</p>
</div>
<div id="inattentional-blindness" class="section level5 unnumbered hasAnchor">
<h5>inattentional blindness<a href="attention.html#inattentional-blindness" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>The failure to notice a fully visible object when attention is devoted to something else.</p>
</div>
<div id="inhibitory-control" class="section level5 unnumbered hasAnchor">
<h5>inhibitory control<a href="attention.html#inhibitory-control" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>The suppression of goal-irrelevant stimuli</p>
</div>
<div id="limited-capacity" class="section level5 unnumbered hasAnchor">
<h5>limited capacity<a href="attention.html#limited-capacity" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>The notion that humans have limited mental resources that can be used at a given time.</p>
</div>
<div id="selective-attention-1" class="section level5 unnumbered hasAnchor">
<h5>selective attention<a href="attention.html#selective-attention-1" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>The ability to select certain stimuli in the environment to process, while ignoring distracting information.</p>
</div>
<div id="shadowing" class="section level5 unnumbered hasAnchor">
<h5>shadowing<a href="attention.html#shadowing" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>A task in which the individual is asked to repeat an auditory message as it is presented.</p>
</div>
<div id="subliminal-perception-1" class="section level5 unnumbered hasAnchor">
<h5>subliminal perception<a href="attention.html#subliminal-perception-1" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>The ability to process information for meaning when the individual is not consciously aware of that information.</p>

</div>
</div>
</div>
<h3>References<a href="references.html#references" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="refs" class="references csl-bib-body hanging-indent" entry-spacing="0" line-spacing="2">
<div id="ref-APA2000" class="csl-entry">
Association, A. P. (2000). <em>Diagnostic and statistical manual of mental disorders (4eme ed.) washington</em>.
</div>
<div id="ref-Bargh2014" class="csl-entry">
Bargh, J. A. (2014). Our unconscious mind. <em>Sci. Am.</em>, <em>310</em>(1), 30–37.
</div>
<div id="ref-Beilock2001" class="csl-entry">
Beilock, S. L., &amp; Carr, T. H. (2001). On the fragility of skilled performance: What governs choking under pressure? <em>J. Exp. Psychol. Gen.</em>, <em>130</em>(4), 701–725.
</div>
<div id="ref-Broadbent1958" class="csl-entry">
Broadbent, D. E., &amp; Dal Martello, M. F. (1958). Perception and communication. <em>Psychological Science</em>.
</div>
<div id="ref-Cheesman1984" class="csl-entry">
Cheesman, J., &amp; Merikle, P. M. (1984). Priming with and without awareness. <em>Percept. Psychophys.</em>, <em>36</em>(4), 387–395.
</div>
<div id="ref-Cheesman1986" class="csl-entry">
Cheesman, J., &amp; Merikle, P. M. (1986). Distinguishing conscious from unconscious perceptual processes. <em>Can. J. Psychol.</em>, <em>40</em>(4), 343–367.
</div>
<div id="ref-Cherry1953" class="csl-entry">
Cherry, E. C. (1953). Some experiments on the recognition of speech, with one and with two ears. <em>Journal of the Acoustical Society of America</em>, <em>25</em>, 975–979.
</div>
<div id="ref-Deutsch1963" class="csl-entry">
Deutsch, J. A., &amp; Deutsch, D. (1963). Attention: Some theoretical considerations. <em>Psychol. Rev.</em>, <em>70</em>(1), 80–90.
</div>
<div id="ref-Greenwald1992" class="csl-entry">
Greenwald, A. G. (1992). New look 3: Unconscious cognition reclaimed. <em>Am. Psychol.</em>, <em>47</em>(6), 766–779.
</div>
<div id="ref-harris2013two" class="csl-entry">
Harris, C. R., Coburn, N., Rohrer, D., &amp; Pashler, H. (2013). Two failures to replicate high-performance-goal priming effects. <em>PloS One</em>, <em>8</em>(8), e72467.
</div>
<div id="ref-Hartung1998" class="csl-entry">
Hartung, C. M., &amp; Widiger, T. A. (1998). Gender differences in the diagnosis of mental disorders: Conclusions and controversies of the DSM–IV. <em>Psychological Bulletin</em>, <em>123</em>(3), 260.
</div>
<div id="ref-james1890principles" class="csl-entry">
James, W. (1890). <em>Principles of psychology</em>.
</div>
<div id="ref-Johnston1978" class="csl-entry">
Johnston, W. A., &amp; Heinz, S. P. (1978). Flexibility and capacity demands of attention. <em>J. Exp. Psychol. Gen.</em>, <em>107</em>(4), 420–435.
</div>
<div id="ref-Merikle2004" class="csl-entry">
Merikle, P. (2004). Subliminal perception. In <em>Encyclopedia of psychology, vol. 7</em> (pp. 497–499). American Psychological Association.
</div>
<div id="ref-Monsell2003" class="csl-entry">
Monsell, S. (2003). Task switching. <em>Trends in Cognitive Sciences</em>, <em>7</em>(3), 134–140.
</div>
<div id="ref-Moray1959" class="csl-entry">
Moray, N. (1959). Attention in dichotic listening: Affective cues and the influence of instructions. <em>Q. J. Exp. Psychol.</em>, <em>11</em>(1), 56–60.
</div>
<div id="ref-neisser1979control" class="csl-entry">
Neisser, U. (1979). The control of information pickup in selective looking. In <em>Perception and its development</em> (pp. 201–219). Psychology Press.
</div>
<div id="ref-Pashler1993" class="csl-entry">
Pashler, H. (1993). Doing two things at the same time. <em>American Scientist</em>, <em>81</em>(1), 48–55.
</div>
<div id="ref-Quinn2014" class="csl-entry">
Quinn, P. O., &amp; Madhoo, M. (2014). A review of attention-deficit/hyperactivity disorder in women and girls: Uncovering this hidden diagnosis. <em>The Primary Care Companion for CNS Disorders</em>, <em>16</em>(3), 27250.
</div>
<div id="ref-Simons1999" class="csl-entry">
Simons, D. J., &amp; Chabris, C. F. (1999). Gorillas in our midst: Sustained inattentional blindness for dynamic events. <em>Perception</em>, <em>28</em>(9), 1059–1074.
</div>
<div id="ref-Skaugset2016" class="csl-entry">
Skaugset, L. M., Farrell, S., Carney, M., Wolff, M., Santen, S. A., Perry, M., &amp; Cico, S. J. (2016). Can you multitask? Evidence and limitations of task switching and multitasking in emergency medicine. <em>Annals of Emergency Medicine</em>, <em>68</em>(2), 189–195.
</div>
<div id="ref-Spelke1976" class="csl-entry">
Spelke, E., Hirst, W., &amp; Neisser, U. (1976). Skills of divided attention. <em>Cognition</em>, <em>4</em>(3), 215–230.
</div>
<div id="ref-Still1902" class="csl-entry">
Still, G. F. (1902). The goulstonian lectures. <em>Some Abnormal Psychical Conditions in Children</em>, 1008–1012.
</div>
<div id="ref-Strayer2007" class="csl-entry">
Strayer, D. L., &amp; Drews, F. A. (2007). Cell-phone induced inattention blindness. <em>Current Directions in Psychological Science</em>, <em>16</em>, 128–131.
</div>
<div id="ref-Strayer2001" class="csl-entry">
Strayer, D. L., &amp; Johnston, W. A. (2001). Driven to distraction: Dual-task studies of simulated driving and conversing on a cellular telephone. <em>Psychol. Sci.</em>, <em>12</em>(6), 462–466.
</div>
<div id="ref-Strayer2011" class="csl-entry">
Strayer, D. L., Watson, J. M., &amp; Drews, F. A. (2011). Cognitive distraction while multitasking in the automobile. In <em>Advances in research and theory</em> (pp. 29–58). Elsevier.
</div>
<div id="ref-Stroop1935" class="csl-entry">
Stroop, J. R. (1935). Studies of interference in serial verbal reactions. <em>Journal of Experimental Psychology</em>, <em>18</em>(6), 643.
</div>
<div id="ref-Tiego2018" class="csl-entry">
Tiego, J., Testa, R., Bellgrove, M. A., Pantelis, C., &amp; Whittle, S. (2018). A hierarchical model of inhibitory control. <em>Frontiers in Psychology</em>, <em>9</em>, 1339.
</div>
<div id="ref-Treisman1960" class="csl-entry">
Treisman, A. M. (1960). Contextual cues in selective listening. <em>Quarterly Journal of Experimental Psychology</em>, <em>12</em>(4), 242–248.
</div>
<div id="ref-Watson2010" class="csl-entry">
Watson, J. M., &amp; Strayer, D. L. (2010). Supertaskers: Profiles in extraordinary multitasking ability. <em>Psychon. Bull. Rev.</em>, <em>17</em>(4), 479–485.
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="perception.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="working-memory-chapter.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
  "sharing": {
    "github": true,
    "facebook": false,
    "twitter": false,
    "linkedin": false,
    "weibo": false,
    "instapaper": false,
    "vk": false,
    "whatsapp": false,
    "all": false
  },
  "fontsettings": {
    "theme": "white",
    "family": "sans",
    "size": 2
  },
  "edit": {
    "link": "https://github.com/pilegard/cogfoundations/edit/main/03-attention.Rmd",
    "text": "Suggest an edit"
  },
  "history": {
    "link": null,
    "text": null
  },
  "view": {
    "link": null,
    "text": null
  },
  "download": ["cogfoundations.pdf", "https://github.com/pilegard/cogfoundations/raw/main/03-attention.Rmd"],
  "search": {
    "engine": "fuse",
    "options": null
  },
  "toc": {
    "collapse": "section"
  }
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
