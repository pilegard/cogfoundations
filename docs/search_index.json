[["index.html", "Cognitive Foundations About About the Book License and Attributions", " Cognitive Foundations Edited by Celeste Pilegard Current version rendered 2024-07-25 About About the Book This collaborative book project is led by Celeste Pilegard, but represents the work of dozens of authors and collaborators listed below. Dr. Pilegard is an Associate Teaching Professor in the Department of Psychology at the University of California, San Diego. Correspondence can be sent to Celeste Pilegard. Please use this Google form to let us know if you’ve used the book in your course. If you notice an error or would like to suggest an improvement, please do one of the following: (1) use the GitHub site to open an issue or submit a pull request with the suggested change, (2) use this Google form, or (3) send an email with “Cognitive Foundations OER” in the subject line. This book was formatted using bookdown, an R package. ISBN 979-8-9912306-0-5 corresponds to the PDF version of Cognitive Foundations v2.0, published July 25, 2024, permanently available at https://github.com/pilegard/cogfoundations/releases/tag/v2.0. The updated version of Cognitive Foundations is available at https://pilegard.github.io/cogfoundations/. Second Edition The second edition of this book builds on the first edition with updates, improvements, and edits by a team of UC San Diego graduate student content experts: Catherine Arnett (Language) Pria Daniel (Attention) Mohan Gupta (Long-Term Memory) Leo Kleiman-Lynch (Reasoning and Decision Making) Hayden Schill (Perception, Working Memory) Anne Yilmaz (Memory in Context) Additionally, Annie S. Ditta (UC Riverside) completed the second edition edits to the Problem Solving chapter. M. Gupta and C. Pilegard converted the book to bookdown. C. Pilegard redesigned the book layout, updated figures throughout, updated the chapter on History, and edited all chapters. We thank John Wixted for his feedback on portions of the chapters on memory. The second edition of this book was supported by a Course Development and Instructional Improvement Program grant awarded to C. Pilegard. The program is sponsored by the Division of Undergraduate Education at the University of California, San Diego. First Edition The first edition of this book was aggregated from multiple Open Educational Resources by Celeste Pilegard. All adaptations, revisions, and transformations of source material were completed by Celeste Pilegard. The original authors of the work remixed for the first edition of the book are listed below: Mehgan Andrade, College of the Canyons Mara Aruguete, Lincoln University David B. Baker, University of Akron Laura Bryant, Eastern Gateway Community College Barbara Chappell, Walden University Kathryn Dumper, Bainbridge State College Frances Friedrich, University of Utah William J. Jenkins, Mercer University Arlene Lacombe, Saint Joseph’s University Cara Laney, Reed College Julie Lazzara, Paradise Valley Community College Elizabeth F. Loftus, University of California, Irvine Marilyn D. Lovett, Spelman College Tammy McClain, West Liberty University Richard Milich, University of Kentucky Gregory Murphy, New York University Barbara B. Oswald, Miami University Marion Perlmutter, University of Michigan Walter Roberts, University of Kentucky Christie Napa Scollon, Singapore Management University Heather Sperry, University of Akron Rose M. Spielman, Formerly of Quinnipiac University Charles Stangor, University of Maryland Mark D. Thomas, Albany State University Jennifer Walinga, Royal Roads University Neil Walker, College of the Canyons Authors of the Cognitive Psychology and Cognitive Neuroscience Wikibook Wikipedia authors C. Pilegard authored original portions of the first edition where noted below. Many thanks to the original authors for their work and for choosing open licenses. License and Attributions Unless otherwise noted, this work is licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. To view a copy of this license, visit http://creativecommons.org/licenses/by-nc-sa/4.0/ or send a letter to Creative Commons, PO Box 1866, Mountain View, CA 94042, USA. Front cover photo by Nathan Dumlao on Unsplash. Licenses and attributions for individual chapters are noted in the following sections. Chapter 1. History and Research Methods First edition Rise of Cognitive Psychology Source: Spielman, R. M., Dumper, K., Jenkins, W., Lacombe, A., Lovett, M., &amp; Perlmutter, M. (2014). Psychology. Houston, Tx: OpenStax. Psychology by Spielman et al. is licensed under a Creative Commons Attribution License. Condensed from original Source: Baker, D. B. &amp; Sperry, H. (2019). History of psychology. In R. Biswas-Diener &amp; E. Diener (Eds), Noba textbook series: Psychology. Champaign, IL: DEF publishers. Retrieved from http://noba.to/j8xkgcz5 History of Psychology by David B. Baker and Heather Sperry is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. Condensed from original Research Methods in Psychology Source: Scollon, C. N. (2019). Research designs. In R. Biswas-Diener &amp; E. Diener (Eds), Noba textbook series: Psychology. Champaign, IL: DEF publishers. Retrieved from http://noba.to/acxb2thy Research Designs by Christie Napa Scollon is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License Condensed from original; Example experiment under “Experimental research” changed to Mueller and Oppenheimer (2014). Second edition Paragraph on ongoing improvements to cognitive psychology added to Rise of Cognitive Psychology section. Example experiment under “Experimental Research” changed to Padilla et al. (2022). Chapter 2. Perception First edition Perception Source: Stangor, C. and Walinga, J. Introduction to Psychology – 1st Canadian Edition. Victoria, B.C.: BCcampus. Retrieved from: https://opentextbc.ca/introductiontopsychology/ Introduction to Psychology - 1st Canadian Edition by Charles Stangor is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. Changes and additions (c) 2014 Jennifer Walinga, licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 3.0 Unported License. Condensed from Walinga version; American spellings used; Imperial measurements used; some content adapted to suit course. Second edition Revisions to first edition version: Structure and framing reorganized. More recent content on multisensory integration added. Computer vision added. Cover photo by Mathilda Khoo on Unsplash. Chapter 3. Attention First edition Attention Source: Friedrich, F. (2019). Attention. In R. Biswas-Diener &amp; E. Diener (Eds), Noba textbook series: Psychology. Champaign, IL: DEF publishers. Retrieved from http://noba.to/uv9x8df5 Attention by Frances Friedrich is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. Condensed from original version; some content adapted to suit course. Source: Milich, R. &amp; Roberts, W. (2022). Adhd and behavior disorders in children. In R. Biswas-Diener &amp; E. Diener (Eds), Noba textbook series: Psychology. Champaign, IL: DEF publishers. Retrieved from http://noba.to/cpxg6b27 ADHD and Behavior Disorders in Children by Richard Milich and Walter Roberts are licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. Condensed from original version; some content adapted to suit course. Second edition Revisions to first edition version: Chapter edited throughout for clarity. Section on Controlling Attention added. Sections on Divided Attention and Multitasking condensed and added to Controlling Attention. Box about Attention-Deficit/Hyperactivity Disorder (ADHD) added. Learning Objectives, Key Takeaways, Exercises, and Glossary added. Cover photo by chuttersnap on Unsplash. Chapter 4. Working Memory First edition Working Memory Source: Multiple authors. Memory. In Cognitive Psychology and Cognitive Neuroscience. Wikibooks. Retrieved from https://en.wikibooks.org/wiki/Cognitive_Psychology_and_Cognitive_Neuroscience Wikibooks are licensed under the Creative Commons Attribution-ShareAlike License. Cognitive Psychology and Cognitive Neuroscience is licensed under the GNU Free Documentation License. Condensed from original version. American spellings used. Content added or changed to reflect American perspective and references. Context and transitions added throughout. Substantially edited, adapted, and (in some parts) rewritten for clarity and course relevance. Chapter introduction added. Content added including transition from STM to WM approach, description of episodic buffer, description and evidence for working memory components, addition of episodic buffer. Second edition Revisions to first edition version: Edits for clarity and more examples added. Section on working memory and distinctiveness added. Language updated to reflect Wixted (2024) on the Atkinson/Shiffrin model. Cover photo by Matt Briney on Unsplash. Chapter 5. Long-Term Memory First edition Long term memory Source: Stangor, C. and Walinga, J. (2014). Introduction to Psychology – 1st Canadian Edition. Victoria, B.C.: BCcampus. Retrieved from: https://opentextbc.ca/introductiontopsychology/ Introduction to Psychology - 1st Canadian Edition by Charles Stangor is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. Changes and additions (c) 2014 Jennifer Walinga, licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 3.0 Unported License. Condensed from Walinga version; American spellings used; Imperial measurements used; some content adapted to suit course. Serial position curve information from: Andrade, M., &amp; Walker, N. (n.d.) Cognitive Psychology. Cognitive Psychology by Mehgan Andrade and Neil Walker is licensed under a Creative Commons Attribution4.0 International License. Encoding, Retrieval, and Consolidation Source: The following entries accessed from http:/www.en.wikipedia.org/ served as sources for this chapter: Memory Rehearsal; Levels-of-processing Effect; Testing Effect; Encoding Specificity Principle; Transfer-Appropriate Processing; Memory Consolidation. Wikipedia text is licensed under the Creative Commons Attribution-ShareAlike License. Chapter introduction added. Transitions and images added. Edited for content and clarity throughout. Some encoding specificity principle information from: Andrade, M., &amp; Walker, N. (n.d.) Cognitive Psychology. Cognitive Psychology by Mehgan Andrade and Neil Walker is licensed under a Creative Commons Attribution4.0 International License. Second edition Revisions to first edition version: Simplified the differences between computers and brains. Additional section of the testing effect and an explanation of a possible mechanism. Addition of the spacing effect and the current mechanisms that we know contribute to it. Clarification of memory consolidation and its relation to sleep. Sleep does not cause more learning, it only reduces interference from other memories. Addition of targeted memory reactivation box during sleep, where it is possible to increase the memory strength during sleep. Language updated to reflect Wixted (2024) on the Atkinson/Shiffrin model. Cover photo by Julian Dik on Unsplash. Chapter 6. Memory in Context First edition Memory in Context Kinds of Memory Biases; Misinformation Effect Source: Laney, C. &amp; Loftus, E. F. (2019). Eyewitness testimony and memory biases. In R. Biswas-Diener &amp; E. Diener (Eds), Noba textbook series: Psychology. Champaign, IL: DEF publishers. Retrieved from http://noba.to/uy49tm37 Eyewitness Testimony and Memory Biases by Cara Laney and Elizabeth F. Loftus is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. Condensed from original Schematic Processing; Source Monitoring; Flashbulb Memories Stangor, C. and Walinga, J. (2014). Introduction to Psychology – 1st Canadian Edition. Victoria, B.C.: BCcampus. Retrieved from: https://opentextbc.ca/introductiontopsychology/ Introduction to Psychology - 1st Canadian Edition by Charles Stangor is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. Changes and additions (c) 2014 Jennifer Walinga, licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 3.0 Unported License. Condensed from original; American spellings used; cultural references updated for American audience Forgetting Source: Spielman, R. M. OpenStax, Psychology. OpenStax CNX. http://cnx.org/contents/4abf04bf-93a0-45c3-9cbc-2cefd46e68cc@12.2. Psychology by Spielman (+ multiple authors) is licensed under a Creative Commons Attribution 4.0 International License Condensed from original Second edition Revisions to first edition version: Edits for clarity and additional context throughout. Added content on distinction between retrieval and recognition. Emphasis added to demonstrate role of memory contamination over time to reflect more recent research. Citations and characterization of eyewitness memory research substantially updated. Cover photo by Alex Grodkiewicz on Unsplash. Chapter 7. Knowledge First edition Knowledge Introduction through Theories of Concept Representation Source: Murphy, G. (2019). Categories and concepts. In R. Biswas-Diener &amp; E. Diener (Eds), Noba textbook series: Psychology. Champaign, IL: DEF publishers. Retrieved from http://noba.to/6vu4cpkt Categories and Concepts by Gregory Murphy is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. Condensed from original version. Concept Organization Source: Multiple authors. Memory. In Cognitive Psychology and Cognitive Neuroscience. Wikibooks. Retrieved from https://en.wikibooks.org/wiki/Cognitive_Psychology_and_Cognitive_Neuroscience\\ Wikibooks are licensed under the Creative Commons Attribution-ShareAlike License. Cognitive Psychology and Cognitive Neuroscience is licensed under the GNU Free Documentation License. Condensed from original version. American spellings used. Content added or changed to reflect American perspective and references. Context and transitions added throughout. Substantially edited, adapted, and (in some parts) rewritten for clarity and course relevance. Cover photo by Alli Elder on Unsplash. Chapter 8. Language First edition Language Source: Stangor, C. and Walinga, J. (2014). Introduction to Psychology – 1st Canadian Edition. Victoria, B.C.: BCcampus. Retrieved from: https://opentextbc.ca/introductiontopsychology/ Introduction to Psychology - 1st Canadian Edition by Charles Stangor is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. Changes and additions (c) 2014 Jennifer Walinga, licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 3.0 Unported License. Condensed from original; American spellings used; cultural references updated for American audience. Bilingualism reverted to Strangor version. Sentence Processing section from Wikipedia entry (http:/www.en.wikipedia.org/), “Sentence Processing.” Wikipedia text is licensed under the Creative Commons Attribution-ShareAlike License. Second edition Revisions to first edition version: Content added to “What is Language”, “Linguistic Diversity”, and “Language Acquisition” sections from source: Anderson, C., Bjorkman, B., Denis, D., Doner, J., Grant, M., Sanders, N., &amp; Taniguchi, A. (2022). Essentials of Linguistics, 2nd edition. eCampusOntario. Essentials of Linguistics, 2nd edition Copyright © 2022 by Catherine Anderson; Bronwyn Bjorkman; Derek Denis; Julianne Doner; Margaret Grant; Nathan Sanders; and Ai Taniguchi is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License, except where otherwise noted. Edited for clarity and restructured throughout. Citations and context updated. Figures added. “Language and Thought” section added. Original writing added throughout. Cover photo by Sushil Nash on Unsplash. Chapter 9. Reasoning and Decision Making First edition Reasoning and Decision Making Source: Multiple authors. Memory. In Cognitive Psychology and Cognitive Neuroscience. Wikibooks. Retrieved from https://en.wikibooks.org/wiki/Cognitive_Psychology_and_Cognitive_Neuroscience Wikibooks are licensed under the Creative Commons Attribution-ShareAlike License. Cognitive Psychology and Cognitive Neuroscience is licensed under the GNU Free Documentation License. Condensed from original version. American spellings used. Content added or changed to reflect American perspective and references. Context and transitions added throughout. Substantially edited, adapted, and (in some parts) rewritten for clarity and course relevance. Second edition Revisions to first edition version: Sections 9.1 and 9.2 edited mostly for clarity, with subsection on confirmation bias edited for content as well. Section 9.3 on decision making entirely removed and rewritten from scratch, including new subsections on theories of decision making and constructed preferences. Cover photo by Qurratul Ayin Sadia on Unsplash. Chapter 10. Problem Solving First edition Problem Solving Source: Multiple authors. Memory. In Cognitive Psychology and Cognitive Neuroscience. Wikibooks. Retrieved from https://en.wikibooks.org/wiki/Cognitive_Psychology_and_Cognitive_Neuroscience Wikibooks are licensed under the Creative Commons Attribution-ShareAlike License. Cognitive Psychology and Cognitive Neuroscience is licensed under the GNU Free Documentation License. Condensed from original version. American spellings used. Content added or changed to reflect American perspective and references. Context and transitions added throughout. Substantially edited, adapted, and (in some parts) rewritten for clarity and course relevance. Second edition New sections added: Creativity, Fixation on Examples, Overcoming Mental Fixation, Curse of Expertise. All figures updated. Cover photo by Zoe Holling on Unsplash. "],["history-and-research-methods.html", "Chapter 1 History and Research Methods 1.1 Rise of Cognitive Psychology 1.2 Research Methods in Psychology 1.3 Glossary", " Chapter 1 History and Research Methods Figure 1.1: Around the turn of the 20th century, futurists imagined what a classroom might look like in the year 2000. Illustration by Jean-Marc Côté, Wikimedia Commons. Philosophers have wondered about the mind at least as far back as Socrates. Yet the scientific study of the mind only began much more recently. What changed, and what tools can we use to study the mind? LEARNING OBJECTIVES Describe the precursors to the establishment of the science of cognitive psychology. Identify key individuals and events in the history of cognitive psychology. Articulate the difference between correlational and experimental designs. Understand how experiments help us to infer causality. List a strength and weakness of different research designs. 1.1 Rise of Cognitive Psychology Precursors to American psychology can be found in philosophy and physiology. Philosophers such as John Locke (1632–1704) and Thomas Reid (1710–1796) promoted empiricism, the idea that all knowledge comes from experience. The work of Locke, Reid, and others emphasized the role of the human observer and the primacy of the senses in defining how the mind comes to acquire knowledge. In American colleges and universities in the early 1800s, these principles were taught as courses on mental and moral philosophy. Most often these courses taught about the mind based on the faculties of intellect, will, and the senses (Fuchs, 2000). Figure 1.2: The earliest records of a psychological experiment go all the way back to the Pharaoh Psamtik I of Egypt in the 7th Century B.C. *Image: Neithsabes, CC0 Public Domain Analytic introspection The formal development of modern psychology is usually credited to the work of German physician, physiologist, and philosopher Wilhelm Wundt (1832–1920). Wundt helped to establish the field of experimental psychology by serving as a strong promoter of the idea that psychology could be an experimental field and by providing classes, textbooks, and a laboratory for training students. In 1875, he joined the faculty at the University of Leipzig and quickly began to make plans for the creation of a program of experimental psychology. In 1879, he complemented his lectures on experimental psychology with a laboratory experience: an event that has served as the popular date for the establishment of the science of psychology. Figure 1.3: Wilhelm Wundt is considered one of the founding figures of modern psychology. CC0 Public Domain The response to the new science was immediate and global. Wundt attracted students from around the world to study the new experimental psychology and work in his lab. Students were trained to offer detailed self-reports of their reactions to various stimuli, a procedure known as introspection. The goal was to identify the elements of consciousness. In addition to the study of sensation and perception, research was done on mental chronometry, more commonly known as reaction time. The work of Wundt and his students demonstrated that the mind could be measured and the nature of consciousness could be revealed through scientific means. It was an exciting proposition, and one that found great interest in America. After the opening of Wundt’s lab in 1879, it took just four years for the first psychology laboratory to open in the United States (L. T. Benjamin, 2007). The Growth of Psychology Throughout the first half of the 20th century, psychology continued to grow and flourish in America. It was large enough to accommodate varying points of view on the nature of mind and behavior. Gestalt psychology is a good example. The Gestalt movement began in Germany with the work of Max Wertheimer (1880–1943). Opposed to the reductionist approach of Wundt’s laboratory psychology, Wertheimer and his colleagues Kurt Koffka (1886–1941), Wolfgang Kohler (1887–1967), and Kurt Lewin (1890–1947) believed that studying the whole of any experience was richer than studying individual aspects of that experience. The saying “the whole is greater than the sum of its parts” is a Gestalt perspective. Consider that a melody is an additional element beyond the collection of notes that comprise it. The Gestalt psychologists proposed that the mind often processes information simultaneously rather than sequentially. For instance, when you look at a photograph, you see a whole image, not just a collection of pixels of color. Using Gestalt principles, Wertheimer and his colleagues also explored the nature of learning and thinking. Most of the German Gestalt psychologists were Jewish and were forced to flee the Nazi regime due to the threats posed on both academic and personal freedoms. In America, they were able to introduce a new audience to the Gestalt perspective, demonstrating how it could be applied to perception and learning (Wertheimer, 1938). In many ways, the work of the Gestalt psychologists served as a precursor to the rise of cognitive psychology in America (L. T. Benjamin, 2007). Behaviorism emerged early in the 20th century and became a major force in American psychology. Championed by psychologists such as John B. Watson (1878–1958) and B. F. Skinner (1904–1990), behaviorism rejected any reference to mind and viewed overt and observable behavior as the proper subject matter of psychology. Through the scientific study of behavior, it was hoped that laws of learning could be derived that would promote the prediction and control of behavior. Russian physiologist Ivan Pavlov (1849–1936) influenced early behaviorism in America. His work on conditioned learning, popularly referred to as classical conditioning, provided support for the notion that learning and behavior were controlled by events in the environment and could be explained with no reference to mind or consciousness (Fancher, 1987). Cognitive Revolution Behaviorism’s emphasis on objectivity and focus on external behavior had pulled psychologists’ attention away from the mind for a prolonged period of time. The early work of the humanistic psychologists redirected attention to the individual human as a whole, and as a conscious and self-aware being. By the 1950s, new disciplinary perspectives in linguistics, neuroscience, and computer science were emerging, and these areas revived interest in the mind as a focus of scientific inquiry. This particular perspective has come to be known as the cognitive revolution (Miller, 2003). By 1967, Ulric Neisser published the first textbook entitled Cognitive Psychology, which served as a core text in cognitive psychology courses around the country (Henley &amp; Thorne, 2005). Cognitive psychology is the study of mental processes such as attention, memory, perception, language use, problem solving, creativity, and thinking. Much of the work derived from cognitive psychology has been integrated into various other modern disciplines of psychological study including social psychology, personality psychology, abnormal psychology, developmental psychology, educational psychology, and economics. Although no one person is entirely responsible for starting the cognitive revolution, Noam Chomsky was very influential in the early days of this movement. Chomsky (1928–), an American linguist, was dissatisfied with the influence that behaviorism had had on psychology. He believed that psychology’s focus on behavior was short-sighted and that the field had to re-incorporate mental functioning into its purview if it were to offer any meaningful contributions to understanding behavior (Miller, 2003). European psychology had never really been as influenced by behaviorism as had American psychology; and thus, the cognitive revolution helped reestablish lines of communication between European psychologists and their American counterparts. Furthermore, psychologists began to cooperate with scientists in other fields, like anthropology, linguistics, computer science, and neuroscience, among others. This interdisciplinary approach often was referred to as the cognitive sciences, and the influence and prominence of this particular perspective resonates in modern-day psychology (Miller, 2003). The field of cognitive psychology continues to grow and improve; for example, researchers such as Ayanna Thomas are working to confront a foundational assumption in cognitive psychology, shaped by a history of scientific racism, that cognition can be understood without respect to context and culture (Thomas et al., 2023). As new generations of cognitive psychologists enter the field, our understanding of the human mind will continue to improve. As philosopher of science Naomi Oreskes explains, “How is it that science is self-correcting? — It is not so much that science corrects itself, but that scientists correct each other” (Oreskes, 2019, p. 51). According to Lee McIntyre, scientists accomplish this by “a commitment to two principles: (1) We care about empirical evidence. (2) We are willing to change our theories in light of new evidence” (McIntyre, 2019, pp. 47–48). Throughout this book you will see examples of how our scientific understanding of the mind has evolved over time; indeed, future editions of this textbook will surely update our current knowledge as new evidence emerges through scientific inquiry. Next, we will look at the research methods psychologists use to ask questions about the world. 1.2 Research Methods in Psychology One of the important steps in scientific inquiry is to test our research questions, otherwise known as hypotheses. However, there are many ways to test hypotheses in psychological research. Which method you choose will depend on the type of questions you are asking, as well as what resources are available to you. All methods have limitations, which is why the best research uses a variety of methods. Most psychological research can be divided into two types: experimental and correlational research. Experimental Research Imagine you work for a state health authority during the COVID-19 pandemic. You want to create a graph that will visualize COVID-19 mortality data in order to help people make judgments about pandemic risks. You have can choose between two ways to display the same information: you can create a graph that shows how many people have died each week from COVID-19 during the pandemic, or you can create a graph that shows, cumulatively, how many people have died from COVID-19 over the same time period (Figure 1.4). Which method of displaying information will help people understand their risk? As long as you’re giving people the information, does it really matter? Figure 1.4: Two ways of displaying the same information. Graph A shows how many people died each week from COVID-19. Graph B shows cumulative deaths from COVID-19 over the same time period. Does the method of displaying information matter? Experiments can help us find out. Figures from Padilla et al. (2022); CC-BY 4.0. During the height of the COVID-19 pandemic, Lace Padilla, a psychology researcher at Northeastern University, set out with her colleagues to test the difference between these graphing methods (Padilla et al., 2022). Participants in her experiment were shown a COVID-19 data visualization. Half of the participants were shown a visualization that showed deaths per week, and the other half saw a visualization that showed cumulative deaths. Afterward, participants answered questions about their perception of their risks during the pandemic. In an experiment, researchers manipulate, or cause changes, in the independent variable, and observe or measure any impact of those changes in the dependent variable. The independent variable is the one under the experimenter’s control, or the variable that is intentionally altered between groups. In the case of Professor Padilla’s experiment, the independent variable was whether participants saw a graph that showed deaths per week or cumulative deaths. The dependent variable is the variable that is not manipulated at all, or the one where the effect happens. One way to help remember this is that the dependent variable “depends” on what happens to the independent variable. In our example, the participants’ risk perception (the dependent variable in this experiment) depends on the type of data visualization they see (the independent variable). Thus, any observed changes or group differences in risk perception can be attributed to data visualization method. What Professor Padilla and her colleagues found was that the people who saw a graph with cumulative deaths perceived greater pandemic risks than those that saw a graph that showed deaths per week. In other words, the data visualization method causes a difference in risk perception Do you find this surprising? But wait! Doesn’t risk perception depend on a lot of different factors—for instance, how cautious a person is in general, or how much background knowledge they have? How can we accurately conclude that the data visualization method causes differences in risk perception, as in the case of Professor Padilla’s experiment? The most important thing about experiments is random assignment. Participants don’t get to pick which condition they are in (e.g., participants didn’t choose what type of graph they saw). The experimenter assigns them to a particular condition based on the flip of a coin or any other random method. Why do researchers do this? Random assignment makes it so the groups, on average, are similar on all characteristics except what the experimenter manipulates. By randomly assigning people to conditions (deaths per week vs. cumulative deaths), some people who are naturally very cautious already should end up in each condition, as should some people who like to take risks. Likewise, some people who pay a lot of attention to pandemic news should end up in each condition, as should some people who are less informed. As a result, the distribution of all these factors will generally be consistent across the two groups, and this means that on average the two groups will be relatively equivalent on all these factors. Random assignment is critical to experimentation because if the only difference between the two groups is the independent variable, we can infer that the independent variable is the cause of any observable difference (e.g., in their perception of risk). Other considerations In addition to using random assignment, you should avoid introducing confounds into your experiments. Confounds are things that could undermine your ability to draw causal inferences. For example, if you wanted to test if a new happy pill will make people happier, you could randomly assign participants to take the happy pill or not (the independent variable) and compare these two groups on their self-reported happiness (the dependent variable). However, if some participants know they are getting the happy pill, they might develop expectations that influence their self-reported happiness. This is sometimes known as a placebo effect. Sometimes a person just knowing that he or she is receiving special treatment or something new is enough to actually cause changes in behavior or perception: In other words, even if the participants in the happy pill condition were to report being happier, we wouldn’t know if the pill was actually making them happier or if it was the placebo effect—an example of a confound. A related idea is participant demand. This occurs when participants try to behave in a way they think the experimenter wants them to behave. Placebo effects and participant demand often occur unintentionally. Even experimenter expectations can influence the outcome of a study. For example, if the experimenter knows who took the happy pill and who did not, and the dependent variable is the experimenter’s observations of people’s happiness, then the experimenter might perceive improvements in the happy pill group that are not really there. One way to prevent these confounds from affecting the results of a study is to use a double-blind procedure. In a double-blind procedure, neither the participant nor the experimenter knows which condition the participant is in. For example, when participants are given the happy pill or the fake pill, they don’t know which one they are receiving. This way the participants shouldn’t experience the placebo effect, and will be unable to behave as the researcher expects (participant demand). Likewise, the researcher doesn’t know which pill each participant is taking (at least in the beginning—later, the researcher will get the results for data-analysis purposes), which means the researcher’s expectations can’t influence his or her observations. Therefore, because both parties are “blind” to the condition, neither will be able to behave in a way that introduces a confound. At the end of the day, the only difference between groups will be which pills the participants received, allowing the researcher to determine if the happy pill actually caused people to be happier. Correlational Designs When scientists passively observe and measure phenomena it is called correlational research. Here, we do not intervene and change behavior, as we do in experiments. In correlational research, we identify patterns of relationships, but we usually cannot infer what causes what. Importantly, with correlational research, you can examine only two variables at a time, no more and no less. So, what if you wanted to test whether spending on others is related to happiness, but you don’t have $20 to give to each participant? You could use a correlational design—which is exactly what Elizabeth Dunn, a professor at the University of British Columbia, did in a study (Dunn et al., 2008). She asked people how much of their income they spent on others or donated to charity, and later she asked them how happy they were. Do you think these two variables were related? Yes, they were! The more money people reported spending on others, the happier they were. More details about the correlation To find out how well two variables correspond, we can plot the relation between the two scores on what is known as a scatterplot (Figure 1.5). In the scatterplot, each dot represents a data point. (In this case it’s individuals, but it could be some other unit.) Importantly, each dot provides us with two pieces of information—in this case, information about how good the person rated the past month (x-axis) and how happy the person felt in the past month (y-axis). Which variable is plotted on which axis does not matter. Figure 1.5: Scatterplot of the association between happiness and ratings of the past month, a positive correlation (r = .81). Each dot represents an individual. The association between two variables can be summarized statistically using the correlation coefficient (abbreviated as r). A correlation coefficient provides information about the direction and strength of the association between two variables. For the example above, the direction of the association is positive. This means that people who perceived the past month as being good reported feeling more happy, whereas people who perceived the month as being bad reported feeling less happy. With a positive correlation, the two variables go up or down together. In a scatterplot, the dots form a pattern that extends from the bottom left to the upper right (just as they do in Figure 1). The r value for a positive correlation is indicated by a positive number (although, the positive sign is usually omitted). Here, the r value is .81. A negative correlation is one in which the two variables move in opposite directions. That is, as one variable goes up, the other goes down. Figure 1.6 shows the association between the average height of males in a country (y-axis) and the pathogen prevalence (or commonness of disease; x-axis) of that country. In this scatterplot, each dot represents a country. Notice how the dots extend from the top left to the bottom right. What does this mean in real-world terms? It means that people are shorter in parts of the world where there is more disease. The r value for a negative correlation is indicated by a negative number—that is, it has a minus (–) sign in front of it. Here, it is –.83. Figure 1.6: Scatterplot showing the association between average male height and pathogen prevalence, a negative correlation (r = –.83). Each dot represents a country. (Chiao, 2009) The strength of a correlation has to do with how well the two variables align. Recall that in Professor Dunn’s correlational study, spending on others positively correlated with happiness: The more money people reported spending on others, the happier they reported to be. At this point you may be thinking to yourself, I know a very generous person who gave away lots of money to other people but is miserable! Or maybe you know of a very stingy person who is happy as can be. Yes, there might be exceptions. If an association has many exceptions, it is considered a weak correlation. If an association has few or no exceptions, it is considered a strong correlation. A strong correlation is one in which the two variables always, or almost always, go together. In the example of happiness and how good the month has been, the association is strong. The stronger a correlation is, the tighter the dots in the scatterplot will be arranged along a sloped line. Problems with the correlation If generosity and happiness are positively correlated, should we conclude that being generous causes happiness? Similarly, if height and pathogen prevalence are negatively correlated, should we conclude that disease causes shortness? From a correlation alone, we can’t be certain. For example, in the first case it may be that happiness causes generosity, or that generosity causes happiness. Or, a third variable might cause both happiness and generosity, creating the illusion of a direct link between the two. For example, wealth could be the third variable that causes both greater happiness and greater generosity. This is why correlation does not mean causation—an often repeated phrase among psychologists. Qualitative Designs Just as correlational research allows us to study topics we can’t experimentally manipulate (e.g., whether you have a large or small income), there are other types of research designs that allow us to investigate these harder-to-study topics. Qualitative designs, including participant observation, case studies, and narrative analysis are examples of such methodologies. Quasi-Experimental Designs What if you want to study the effects of marriage on a variable? For example, does marriage make people happier? Can you randomly assign some people to get married and others to remain single? Of course not. So how can you study these important variables? You can use a quasi-experimental design. A quasi-experimental design is similar to experimental research, except that random assignment to conditions is not used. Instead, we rely on existing group memberships (e.g., married vs. single). We treat these as the independent variables, even though we don’t assign people to the conditions and don’t manipulate the variables. As a result, with quasi-experimental designs causal inference is more difficult. For example, married people might differ on a variety of characteristics from unmarried people. If we find that married participants are happier than single participants, it will be hard to say that marriage causes happiness, because the people who got married might have already been happier than the people who have remained single. Figure 1.7: What is a reasonable way to study the effects of marriage on happiness? Image: Nina Matthews Photography, https://goo.gl/IcmLqg, CC BY-NC-SA, https://goo.gl/HSisdg Longitudinal Studies Another powerful research design is the longitudinal study. Longitudinal studies track the same people over time. Some longitudinal studies last a few weeks, some a few months, some a year or more. Some studies that have contributed a lot to psychology followed the same people over decades. For example, one study followed more than 20,000 Germans for two decades. From these longitudinal data, psychologist Rich Lucas et al. (2003) was able to determine that people who end up getting married indeed start off a bit happier than their peers who never marry. Longitudinal studies like this provide valuable evidence for testing many theories in psychology, but they can be quite costly to conduct, especially if they follow many people for many years. Tradeoffs in Research Even though there are serious limitations to correlational and quasi-experimental research, they are not poor cousins to experiments and longitudinal designs. In addition to selecting a method that is appropriate to the question, many practical concerns may influence the decision to use one method over another. One of these factors is simply resource availability—how much time and money do you have to invest in the research? (Tip: If you’re doing a senior honor’s thesis, do not embark on a lengthy longitudinal study unless you are prepared to delay graduation!) Often, we survey people even though it would be more precise—but much more difficult—to track them longitudinally. Especially in the case of exploratory research, it may make sense to opt for a cheaper and faster method first. Then, if results from the initial study are promising, the researcher can follow up with a more intensive method. Beyond these practical concerns, another consideration in selecting a research design is the ethics of the study. For example, in cases of brain injury or other neurological abnormalities, it would be unethical for researchers to inflict these impairments on healthy participants. Nonetheless, studying people with these injuries can provide great insight into human psychology (e.g., if we learn that damage to a particular region of the brain interferes with emotions, we may be able to develop treatments for emotional irregularities). In addition to brain injuries, there are numerous other areas of research that could be useful in understanding the human mind but which pose challenges to a true experimental design—such as the experiences of war, long-term isolation, abusive parenting, or prolonged drug use. However, none of these are conditions we could ethically experimentally manipulate and randomly assign people to. Therefore, ethical considerations are another crucial factor in determining an appropriate research design. Key Takeaways People have asked questions about the mind for centuries, but only relatively recently took a scientific approach. Psychology, and cognitive psychology especially, is a young science. In order to be a savvy consumer of research, you need to understand the pros and cons of different methods and the distinctions among them. Plus, understanding how psychologists systematically go about answering research questions will help you to solve problems in other domains, both personal and professional, not just in psychology. Exercises Discussion: How were early researchers important to the development of psychology as a science? Practice: Make a list of the schools of thought that preceded the cognitive revolution and write a short description of each. Compare: What are some key differences between experimental and correlational research? 1.3 Glossary behaviorism The study of behavior. confounds Factors that undermine the ability to draw causal inferences from an experiment. consciousness Awareness of ourselves and our environment. correlation Measures the association between two variables, or how they go together. dependent variable The variable the researcher measures but does not manipulate in an experiment. empiricism The belief that knowledge comes from experience. experimenter expectations When the experimenter’s expectations influence the outcome of a study. independent variable The variable the researcher manipulates and controls in an experiment. introspection A method of focusing on internal processes. longitudinal study A study that follows the same group of individuals over time. participant demand When participants behave in a way that they think the experimenter wants them to behave. placebo effect When receiving special treatment or something new affects human behavior. quasi-experimental design An experiment that does not require random assignment to conditions. random assignment Assigning participants to receive different conditions of an experiment by chance. References Benjamin, L. T. (2007). A brief history of modern psychology. Blackwell Publishing. Chiao, J. (2009). Culture–gene coevolution of individualism – collectivism and the serotonin transporter gene. Proceedings of the Royal Society B, 277, 529–537. https://doi.org/10.1098/rspb.2009.1650 Dunn, E. W., Aknin, L. B., &amp; Norton, M. I. (2008). Spending money on others promotes happiness. Science, 319(5870), 1687–1688. https://doi.org/10.1126/science.1150952 Fancher, R. E. (1987). The intelligence men: Makers of the IQ controversy. W.W. Norton &amp; Company. Fuchs, A. H. (2000). Contributions of american mental philosophers to psychology in the united states. History of Psychology, 3, 3–19. Henley, T. B., &amp; Thorne, B. M. (2005). The lost millennium: Psychology during the middle ages. The Psychological Record, 55(1), 103–113. Lucas, R. E., Clark, A. E., Georgellis, Y., &amp; Diener, E. (2003). Re-examining adaptation and the setpoint model of happiness: Reactions to changes in marital status. Journal of Personality and Social Psychology, 84, 527–539. McIntyre, L. (2019). The scientific attitude: Defending science from denial, fraud, and pseudoscience. Mit Press. Miller, G. A. (2003). The cognitive revolution: A historical perspective. Trends in Cognitive Sciences, 7(3), 141–144. Oreskes, N. (2019). Why trust science? Padilla, L., Hosseinpour, H., Fygenson, R., Howell, J., Chunara, R., &amp; Bertini, E. (2022). Impact of COVID-19 forecast visualizations on pandemic risk perceptions. Scientific Reports, 12(1), 2014. Thomas, A. K., McKinney de Royston, M., &amp; Powell, S. (2023). Color-evasive cognition: The unavoidable impact of scientific racism in the founding of a field. Current Directions in Psychological Science, 32(2), 137–144. Wertheimer, M. (1938). Gestalt theory. In W. D. Ellis (Ed.), A source book of gestalt psychology (pp. 1–11). Harcourt. "],["perception.html", "Chapter 2 Perception 2.1 Sensation and Perception 2.2 Perception: Information Integration 2.3 Glossary", " Chapter 2 Perception The study of sensation and perception is exceedingly important for our everyday lives because the knowledge generated by psychologists is used in so many ways to help so many people. Psychologists work closely with mechanical and electrical engineers, with experts in defense and military contractors, and with clinical, health, and sports psychologists to help them apply this knowledge to their everyday practices. LEARNING OBJECTIVES Review and summarize the capacities and limitations of human sensation. Identify the key structures of the eye and the role they play in vision. Describe how sensation and perception work together through sensory interaction, selective attention, sensory adaptation, and perceptual constancy. Give examples of how our expectations may influence our perception, resulting in illusions and potentially inaccurate judgments. 2.1 Sensation and Perception The ability to detect and interpret the events that are occurring around us allows us to respond to these stimuli appropriately (Gibson et al., 2000). In most cases the system is successful, but it is not perfect. In this chapter we will discuss the strengths and limitations of these capacities, focusing on both sensation — the stimulation of sensory receptor cells, which is converted to neural impulses — and perception — our experience as a result of that stimulation. Sensation and perception work seamlessly together to allow us to experience the world through our eyes, ears, nose, tongue, and skin, but also to combine what we are currently learning from the environment with what we already know about it (prior knowledge) in order to make judgments and to choose appropriate behaviors. Humans possess powerful sensory capacities that allow us to sense the kaleidoscope of sights, sounds, smells, and tastes that surround us. Our eyes detect light energy and our ears pick up sound waves. Our skin senses touch, pressure, hot, and cold. Our tongues react to the molecules of the foods we eat, and our noses detect scents in the air. The human sensory and perceptual systems are wired for accuracy, and people are exceedingly good at making use of the wide variety of information available to them (Stoffregen &amp; Bardy, 2001). In many ways, our senses are quite remarkable. The human eye can detect the equivalent of a single candle flame burning 30 miles away and can distinguish among more than 300,000 different colors. The human ear can detect sounds as low as 20 hertz (vibrations per second) and as high as 20,000 hertz, and it can hear the tick of a clock about 20 feet away in a quiet room. We can taste a teaspoon of sugar dissolved in two gallons of water, and we are able to smell one drop of perfume diffused in a three-room apartment. We can feel the wing of a bee on our cheek dropped from one centimeter above (Galanter, 1962). Seeing Whereas other animals rely primarily on hearing, smell, or touch to understand the world around them, humans rely in large part on vision. Thus, vision will be the primary sense we focus on in this chapter. A large part of our cerebral cortex is devoted to seeing, and we have substantial visual skills. Seeing begins when light falls on the eyes, initiating the process of transduction, the conversion of light detected by receptor cells to electrical impulses that are transported to the brain. Once this visual information reaches the visual cortex, it is processed by a variety of neurons that detect colors, shapes, and motion, and that create meaningful perceptions out of the incoming stimuli. The brain does this in part by combining incoming information with our expectations and prior knowledge about the world. The Sensing Eye and the Perceiving Visual Cortex As you can see in Figure 2.1, light enters the eye through the cornea, a clear covering that protects the eye and begins to focus the incoming light. The light then passes through the pupil, a small opening in the center of the eye. The pupil is surrounded by the iris, the colored part of the eye that controls the size of the pupil by constricting or dilating in response to light intensity. When we enter a dark movie theater on a sunny day, for instance, muscles in the iris open the pupil and allow more light to enter. Complete adaptation to the dark may take up to 20 minutes. Figure 2.1: Anatomy of the Human Eye. Behind the pupil is the lens, a structure that focuses the incoming light on the retina, the layer of tissue at the back of the eye that contains photoreceptor cells. Rays from the top of the image strike the bottom of the retina and vice versa, and rays from the left side of the image strike the right part of the retina and vice versa, causing the image on the retina to be upside down. Behind the pupil is the lens, a structure that focuses the incoming light on the retina, the layer of tissue at the back of the eye that contains photoreceptor cells. Rays from the top of the image strike the bottom of the retina and vice versa, and rays from the left side of the image strike the right part of the retina and vice versa, causing the image on the retina to be upside down. The retina contains layers of neurons specialized to respond to light. As light falls on the retina, it first activates receptor cells known as rods and cones. The activation of these cells then spreads to the bipolar cells and then to the ganglion cells, which gather together and converge, like the strands of a rope, forming the optic nerve. The optic nerve is a collection of millions of ganglion neurons that sends vast amounts of visual information, via a structure in the middle of the brain called the thalamus, to the visual cortex, which starts at the back of the brain (thus validating to the phrase, “I have eyes in the back of my head”). Because the retina and the optic nerve are active processors and analyzers of visual information, it is appropriate to think of these structures as an extension of the brain itself. Rods are sensory receptor neurons that specialize in detecting black, white, and gray colors. There are about 120 million rods in each eye. The rods do not provide a lot of detail about the images we see, but because they are highly sensitive to shorter-waved (darker) and weak light, they help us see in dim light — for instance, at night. Because the rods are located primarily around the edges of the retina, they are particularly active in peripheral vision (when you need to see something at night, try looking slightly to the side of what you want to see in order to activate more of your highly sensitive rod receptors). Cones are sensory receptor neurons that are specialized in detecting fine detail and colors. The five million or so cones in each eye enable us to see in color, but they operate best in bright light. The cones are located primarily in and around the fovea, which is the central point of the retina. To demonstrate the difference between rods and cones in attention to detail, choose a word in this text and focus on it. Do you notice that the words a few inches to the side seem more blurred? This is because the word you are focusing on strikes the detail-oriented cones, while the words surrounding it strike the less-detail-oriented rods, which are located on the periphery. Margaret Livingstone (2000) (Figure 2.2) found an interesting effect that demonstrates the different processing capacities of the eye’s rods and cones — namely, that the Mona Lisa’s smile, which is widely referred to as “elusive,” is perceived differently depending on how one looks at the painting. Because Leonardo da Vinci painted the smile in low-detail brush strokes, the smile is actually better perceived by the rods in our peripheral vision than by the cones. Livingstone found that people rated the Mona Lisa as more cheerful when they were instructed to focus on her eyes than they did when they were asked to look directly at her mouth. As Livingstone put it, “She smiles until you look at her mouth, and then it fades, like a dim star that disappears when you look directly at it.” Figure 2.2: Mona Lisa’s smile. The brain’s visual cortex is made up of specialized neurons that turn the sensations they receive from the optic nerve into meaningful representations of the world. Because there are no photoreceptor cells at the place where the optic nerve leaves the retina, a hole or blind spot in our vision is created (see Figure 2.3). When both of our eyes are open, we don’t experience a “hole” in our awareness because our eyes are constantly moving, and one eye makes up for what the other eye misses. The visual system is also designed to deal with this problem if only one eye is open — the visual cortex simply fills in the small hole in our vision with similar patterns from the surrounding areas, and we never notice the difference. The visual system’s ability to cope with the blind spot is another example of how sensation and perception work together to create meaningful experience. Figure 2.3: Blind Spot Demonstration. You can get an idea of the extent of your blind spot (the place where the optic nerve leaves the retina) by trying this: close your left eye and stare with your right eye at the cross in the diagram. You should be able to see the elephant image to the right (don’t look at it, just notice that it is there). If you can’t see the elephant, move closer or farther away until you can. Now slowly move so that you are closer to the image while you keep looking at the cross. At one distance (around a foot or so depending on your zoom), the elephant will completely disappear from view because its image has fallen on the blind spot. Perceiving Depth Depth perception is the ability to perceive three-dimensional space and to accurately judge distance. Without depth perception, we would be unable to drive a car, thread a needle, or simply navigate our way around the supermarket (Howard, 2002). Depth perception is the result of our use of depth cues, messages from our bodies and the external environment that supply us with information about space and distance. Binocular depth cues are depth cues that are created by retinal image disparity — that is, the space between our eyes — and which thus require the coordination of both eyes. One outcome of retinal disparity is that the images projected on each eye are slightly different from each other. The visual cortex automatically merges the two images into one, enabling us to perceive depth. Three-dimensional movies make use of retinal disparity by using 3-D glasses that the viewer wears to create a different image on each eye. The perceptual system quickly, easily, and unconsciously turns the disparity into 3-D. An important binocular depth cue is convergence, the inward turning of our eyes that is required to focus on objects that are less than about 50 feet away from us. The visual cortex uses the size of the convergence angle between the eyes to judge the object’s distance. You will be able to feel your eyes converging if you slowly bring a finger closer to your nose while continuing to focus on it. When you close one eye, you no longer feel the tension — convergence is a binocular depth cue that requires both eyes to work. Although the best cues to depth occur when both eyes work together, we are able to see depth even with one eye closed. Monocular depth cues are depth cues that help us perceive depth using only one eye (Sekuler &amp; Blake, 2006). Some of the most important cues are summarized in Table 2.1. Table 2.1: Monocular Depth Cues That Help Us Judge Depth at a Distance. Name Description Example Image Position We tend to see objects higher up in our field of vision as farther away. The fence posts at right appear farther away not only because they become smaller but also because they appear higher up in the picture. Relative size Assuming that the objects in a scene are the same size, smaller objects are perceived as father away. at right, the cars in the distance appear smaller than those nearer to us. Linear perspective Parallel lines appear to converge at a distance. We know that the tracks at right are parallel. When they appear closer together, we determine they are farther away. Light and shadow The eye receives more reflected light from objects that are closer to us. Normally, light comes from above, so darker images are in shadow. We see the images at right as extending and indented according to their shadowing. If we invert the picture, the images will reverse. Interposition When one object overlaps another object, we view it as closer. At right, because the blue star covers the pink bar, it is seen as closer than the yellow moon. Aerial perspective Objects that appear hazy, or that are covered with smog or dust, appear farther away. The artist who pained the picture on the right used aerial perspective to make the clouds more hazy and this appear farther away. Perceiving Form One of the important functions of the visual system is the perception of form. German psychologists in the 1930s and 1940s, including Max Wertheimer (1880-1943), Kurt Koffka (1886-1941), and Wolfgang Köhler (1887-1967), argued that we create forms out of their component sensations based on the idea of the gestalt, a meaningfully organized whole. The idea of the gestalt is that the “whole is more than the sum of its parts.” Some examples of how gestalt principles lead us to see more than what is actually there are summarized in Table 2.2. Table 2.2: Summary of Gestalt Principles of Form Perception Principle Description Example Image Figure and ground We structure input so that we always see a figure (image) against a ground (background). At right, you may see a vase or you may see two faces, but in either case, you will organize the image as a figure against a ground. Similarity Stimuli that are similar to each other tend to be grouped together. You are more likely to see three similar columns among the XYX characters at right than you are to see four rows. Proximity We tend to group nearby figures together. Do you see four or eight images at right? Principles of proximity suggest that you might see only four. Continuity We tend to perceive stimuli in smooth, continuous ways rather than in more discontinuous ways. At right, most people see a line of dots that moves from the lower left to the upper right, rather than a line that moves from the left and then suddenly turns down. The principle of continuity leads us to see most lines as following the smoothest possible path. Closure We tend to fill in gaps in an incomplete image to create a complete, whole object. Closure leads us to see a single spherical object at right rather than a set of unrelated cones. 2.2 Perception: Information Integration The eyes, ears, nose, tongue, and skin sense the world around us, and in some cases perform preliminary information processing on the incoming data. But by and large, what we end up “seeing” or experiencing is a result of our brain’s interpretation of the sensory information coming in, rather than a direct read out of that information. When we look out the window at a view of the countryside, or when we look at the face of a good friend, we don’t just see a jumble of colors and shapes — we see, instead, an image of a countryside or an image of a friend (Goodale &amp; Milner, 2006). How our brain interprets and integrates sensory information in a way that leads to our everyday experience largely depends on attention, working memory, and other cognitive processes that will be discussed in future chapters of this book. How the Perceptual System Interprets the Environment This process of understanding involves the automatic operation of a variety of essential perceptual processes. One of these is sensory interaction — the working together of different senses to create experience. For example, sensory interaction is involved when taste, smell, and texture combine to create the flavor we experience in food. It is also involved when we enjoy a movie because of the way the images and the music work together. Figure 2.4: Watch The McGurk Effect [YouTube]: http://www.youtube.com/watch?v=jtsfidRq2tw Although you might think that we understand speech only through our sense of hearing, it turns out that the visual aspect of speech is also important. One example of sensory interaction is shown in the McGurk effect — an error in perception that occurs when we misperceive sounds because the audio and visual parts of the speech are mismatched. You can witness the effect yourself by viewing the video in Figure 2.4. Other examples of sensory interaction include the experience of nausea that can occur when the sensory information being received from the eyes and the body does not match information from the vestibular system (Flanagan et al., 2004) and synesthesia — an experience in which one sensation (e.g., seeing a number) creates experiences in another (e.g., hearing a sound). Most people do not experience synesthesia, but those who do link their perceptions in unusual ways, for instance, by experiencing color when they taste a particular food or by hearing sounds when they see certain objects (Ramachandran et al., 2005). A more recent example of sensory interaction illustrates how sounds can directly shape our visual perception (Williams et al., 2022). Researchers showed people noisy images (e.g., a blurry image of a plane/bird) paired with naturalisitic sounds. Perception – whether they saw it as a plane or bird – was shaped by the sound that the image was paired with. If the image was paired with the sounds of a bird, people were more likely to see the image as a bird instead of a plane. This research shows that our perceptual experience of one sense (e.g., vision) is shaped by other senses (e.g., hearing). A second fundamental process of perception is sensory adaptation — a decreased sensitivity to a stimulus after prolonged and constant exposure. When you step into a swimming pool, the water initially feels cold, but after a while you stop noticing it. After prolonged exposure to the same stimulus, our sensitivity toward it diminishes and we no longer perceive it. The ability to adapt to the things that don’t change around us is essential to our survival, as it leaves our sensory receptors free to detect the important and informative changes in our environment and to respond accordingly. We ignore the sounds that our car makes every day, which leaves us free to pay attention to the sounds that are different from normal, and thus likely to need our attention. Our sensory receptors are alert to novelty and are fatigued after constant exposure to the same stimulus. If sensory adaptation occurs with all senses, why doesn’t an image fade away after we stare at it for a period of time? The answer is that, although we are not aware of it, our eyes are constantly flitting from one angle to the next, making thousands of tiny movements (called saccades) every minute. This constant eye movement guarantees that the image we are viewing always falls on fresh receptor cells. What would happen if we could stop the movement of our eyes? Psychologists have devised a way of testing the sensory adaptation of the eye by attaching an instrument that ensures a constant image is maintained on the eye’s inner surface. Participants are fitted with a contact lens that has a miniature slide projector attached to it. Because the projector follows the exact movements of the eye, the same image is always projected, stimulating the same spot, on the retina. Within a few seconds, interesting things begin to happen. The image will begin to vanish, then reappear, only to disappear again, either in pieces or as a whole. Even the eye experiences sensory adaptation (Yarbus, 1967). One of the major problems in perception is to ensure that we always perceive the same object in the same way, even when the sensations it creates on our receptors change dramatically. The ability to perceive a stimulus as constant despite changes in sensation is known as perceptual constancy. Consider our image of a door as it swings. When it is closed, we see it as rectangular, but when it is open, we see only its edge and it appears as a line. But we never perceive the door as changing shape as it swings — perceptual mechanisms take care of the problem for us by allowing us to see a constant shape. The visual system also corrects for color constancy. Imagine that you are wearing blue jeans and a bright white T-shirt. When you are outdoors, both colors will be at their brightest, but you will still perceive the white T-shirt as bright and the blue jeans as darker. When you go indoors, the light shining on the clothes will be significantly dimmer, but you will still perceive the T-shirt as bright. This is because we put colors in context and see that, compared with its surroundings, the white T-shirt reflects the most light (McCann, 1992). In the same way, a green leaf on a cloudy day may reflect the same wavelength of light as a brown tree branch does on a sunny day. Nevertheless, we still perceive the leaf as green and the branch as brown. Why are computers still so bad at perception? Computer vision refers to machines or algorithms that are built to mimic the human sensation and perception system. As we’ve learned, perception does not work like a camera, where we experience exactly what comes in through our senses. Instead, what we perceive is influenced by many factors, such as other sensory input, prior experiences, and expectations. Programming all of these components into a computer is difficult, and one reason why computer vision isn’t as good as we might expect it to be. Another complication is that there are still significant gaps in our understanding of the human perceptual system. For instance, consider this: if perception is an integrative process that takes time, whatever we see now is no longer what is in front of us. Yet, humans can do amazing time-sensitive feats like hitting a 90-mph fastball in a baseball game. It appears then that a fundamental function of visual perception is not just to know what is happening around you now, but actually to make an accurate inference about what you are about to see next (Enns &amp; Lleras, 2008), so that you can keep up with the world. Understanding how this future-oriented, predictive function of perception is achieved in the brain is a largely unsolved challenge, and just another piece of the puzzle that computer vision models will have to solve as well. Illusions Figure 2.5: Optical Illusions as a Result of Brightness Constancy (Left) and Color Constancy (Right). Look carefully at the snakelike pattern on the left. Are the green strips really brighter than the background? Cover the white curves and you’ll see they are not. Square A in the right-hand image looks very different from square B, even though they are exactly the same. Although our perception is very accurate, it is not perfect. Illusions occur when the perceptual processes that normally help us correctly perceive the world around us are fooled by a particular situation so that we see something that does not exist or that is incorrect. Figure 2.5 presents two situations in which our normally accurate perceptions of visual constancy have been fooled. Another well-known illusion is the Mueller-Lyer illusion (see Figure 2.6). The line segment in the bottom arrow looks longer to us than the one on the top, even though they are both actually the same length. It is likely that the illusion is, in part, the result of the failure of monocular depth cues — the bottom line looks like an edge that is normally farther away from us, whereas the top one looks like an edge that is normally closer. Figure 2.6: The Mueller-Lyer illusion makes the line segment at the top of the left picture appear shorter than the one at the bottom. The illusion is caused, in part, by the monocular distance cue of depth — the bottom line looks like an edge that is normally farther away from us, whereas the top one looks like an edge that is normally closer. The Ponzo illusion operates on the same principle. As you can see in Figure 2.7, the top yellow bar seems longer than the bottom one, but if you measure them you’ll see that they are exactly the same length. The monocular depth cue of linear perspective leads us to believe that, given two similar objects, the distant one can only cast the same size retinal image as the closer object if it is larger. The topmost bar therefore appears longer. Figure 2.7: The Ponzo Illusion. The Ponzo illusion is caused by a failure of the monocular depth cue of linear perspective. Both bars are the same size, even though the top one looks larger. Illusions demonstrate that our perception of the world around us may be influenced by our prior knowledge. But the fact that some illusions exist in some cases does not mean that the perceptual system is generally inaccurate — in fact, humans normally become so closely in touch with their environment that the physical body and the particular environment that we sense and perceive becomes embodied — that is, built into and linked with our cognition, such that the world around us becomes part of our brain (Calvo &amp; Gomila, 2008). The close relationship between people and their environments means that, although illusions can be created in the lab and under some unique situations, they may be less common with active observers in the real world (Runeson, 1988). The Important Role of Expectations in Perception Our emotions, mindset, expectations, and the contexts in which our sensations occur all have a profound influence on perception. People who are warned that they are about to taste something bad rate what they do taste more negatively than people who are told that the taste won’t be so bad (Nitschke et al., 2006), and people perceive a child and adult pair as looking more alike when they are told that they are parent and child (Bressan &amp; Dal Martello, 2002). Similarly, participants who see images of the same baby rate it as stronger and bigger when they are told it is a boy as opposed to when they are told it is a girl (Stern &amp; Karraker, 1989), and research participants who learn that a child is from a lower-class background perceive the child’s scores on an intelligence test as lower than people who see the same test taken by a child they are told is from an upper-class background (Darley &amp; Gross, 1983a). Plassmann et al. (2008) found that wines were rated more positively and caused greater brain activity in brain areas associated with pleasure when they were said to cost more than when they were said to cost less. And even experts can be fooled: professional referees tended to assign more penalty cards to soccer teams for videotaped fouls when they were told that the team had a history of aggressive behavior than when they had no such expectation (Jones et al., 2002). Psychology in Everyday Life: How Understanding Sensation and Perception Can Save Lives Human factors is the field of psychology that uses psychological knowledge, including the principles of sensation and perception, to improve the development of technology. Human factors has worked on a variety of projects, ranging from nuclear reactor control centers and airplane cockpits to cell phones and websites (Proctor &amp; Van Zandt, 2008). For instance, knowledge of the visual system also helped engineers create new kinds of displays, such as those used on notebook computers and music players, and better understand how using cell phones while driving may contribute to automobile accidents (Lee &amp; Strayer, 2004). Human factors also has made substantial contributions to airline safety. About two-thirds of accidents on commercial airplane flights are caused by human error (Nickerson, 1998a). During takeoff, travel, and landing, the pilot simultaneously communicates with ground control, maneuvers the plane, scans the horizon for other aircraft, and operates controls. The need for a usable interface that works easily and naturally with the pilot’s visual perception is essential. Psychologist Kraft (1978) hypothesized that as planes land, with no other distance cues visible, pilots may be subjected to a type of moon illusion, in which the city lights beyond the runway appear much larger on the retina than they really are, deceiving the pilot into landing too early. Kraft’s findings caused airlines to institute new flight safety measures, where copilots must call out the altitude progressively during the descent, which has probably decreased the number of landing accidents. Figure 2.8 presents images of an airplane instrument panel before and after it was redesigned by human factors psychologists. On the left is the initial design, in which the controls were crowded and cluttered, in no logical sequence, each control performing one task. The controls were more or less the same in color, and the gauges were not easy to read. The redesigned digital cockpit shows a marked improvement in usability. More of the controls are color-coded and multifunctional so that there is less clutter on the dashboard. Screens make use of LCD and 3-D graphics. Text sizes are changeable — increasing readability — and many of the functions have become automated, freeing up the pilots’ concentration for more important activities. One important aspect of the redesign was based on the principles of sensory adaptation. Displays that are easy to see in darker conditions quickly become unreadable when the sun shines directly on them. It takes the pilot a relatively long time to adapt to the suddenly much brighter display. Furthermore, perceptual contrast is important. The display cannot be so bright at night that the pilot is unable to see targets in the sky or on the land. Human factors psychologists used these principles to determine the appropriate stimulus intensity needed on these displays so that pilots would be able to read them accurately and quickly under a wide range of conditions. The psychologists accomplished this by developing an automatic control mechanism that senses the ambient light visible through the front cockpit windows and detects the light falling on the display surface, and then automatically adjusts the intensity of the display for the pilot (Silverstein et al., 1990; Silverstein &amp; Merrifield, 1985). Figure 2.8: Airplane instrument panel before and after it was redesigned by human factors psychologists. Key Takeaways Sensation is the process of receiving information from the environment through our sensory organs. Perception is the process of interpreting and organizing the incoming information so that we can understand it and react accordingly. The retina has two types of photoreceptor cells: rods, which detect brightness and respond to black and white, and cones, which respond to red, green, and blue. Colour blindness occurs when people lack function in the red- or green-sensitive cones. Feature detector neurons in the visual cortex help us recognize objects, and some neurons respond selectively to faces and other body parts. The ability to perceive depth occurs as the result of binocular and monocular depth cues. Perceptual constancy allows us to perceive an object as the same, despite changes in sensation. Cognitive illusions are examples of how our expectations can influence our perceptions. Exercises Practice: List some ways that the processes of visual perception help you engage in an everyday activity, such as driving a car or riding a bicycle. Discussion: What are some cases where your expectations about what you thought you were going to experience influenced your perceptions of what you actually experienced? 2.3 Glossary binocular depth cues Depth cues that are created by retinal image disparity — that is, the space between our eyes — and which thus require the coordination of both eyes blind spot A hole in our vision where the optic nerve leaves the retina computer vision Machines or algorithms that are built to mimic the human sensation and perception system cones Visual neurons that are specialized in detecting fine detail and colours convergence the inward turning of our eyes that is required to focus on objects that are less than about 50 feet away from us cornea A clear covering that protects the eye and begins to focus the incoming light depth cues Messages from our bodies and the external environment that supply us with information about space and distance depth perception The ability to perceive three-dimensional space and to accurately judge distance embodied The particular environment that we sense and perceive becomes built into and linked with our cognition fovea The central point of the retina gestalt A meaningfully organized whole human factors The field of psychology that uses psychological knowledge, including the principles of sensation and perception, to improve the development of technology iris The coloured part of the eye that controls the size of the pupil by constricting or dilating in response to light intensity lens A structure that focuses the incoming light on the retina McGurk Effect An error in perception that occurs when we misperceive sounds because the audio and visual parts of the speech are mismatched monocular depth cues Depth cues that help us perceive depth using only one eye Mueller-Lyer Illusion An illusion in which one line segment looks longer than another based on converging or diverging angles at the ends of the lines. optic nerve A collection of millions of ganglion neurons that sends vast amounts of visual information to the brain perception The process of interpreting and organizing the incoming information so that we can understand it and react accordingly perceptual constancy The ability to perceive a stimulus as constant despite changes in sensation pupil A small opening in the centre of the eye retina The layer of tissue at the back of the eye that contains photoreceptor cells rods Visual neurons that specialize in detecting black, white, and gray colours saccades The rapid shifting of the eyes from one fixation point to another sensation The process of receiving information from the environment through our sensory organs sensory adaptation A decreased sensitivity to a stimulus after prolonged and constant exposure sensory interaction The working together of different senses to create experience synesthesia An experience in which one sensation (e.g., seeing a number) creates experiences in another (e.g., hearing a sound) transduction The conversion of light detected by receptor cells to electrical impulses that are transported to the brain References Bressan, P., &amp; Dal Martello, M. F. (2002). Talis pater, talis filius: Perceived resemblance and the belief in genetic relatedness. Psychological Science, 13, 213–218. Calvo, P., &amp; Gomila, T. (2008). Handbook of cognitive science: An embodied approach. Elsevier. Darley, J. M., &amp; Gross, P. H. (1983a). A hypothesis-confirming bias in labeling effects. Journal of Personality and Social Psychology, 44, 20–33. Enns, J. T., &amp; Lleras, A. (2008). What’s next? New evidence for prediction in human vision. Trends in Cognitive Sciences, 12(9), 327–333. Flanagan, M. B., May, J. G., &amp; Dobie, T. G. (2004). The role of vection, eye movements, and postural instability in the etiology of motion sickness. Journal of Vestibular Research: Equilibrium and Orientation, 14(4), 335–346. Galanter, E. (1962). Contemporary psychophysics. In r. Brown, e. Galanter, e. H. Hess, &amp; g. Mandler (eds.), new directions in psychology. Holt, Rinehart; Winston. Gibson, E. J., Pick, A. D., et al. (2000). An ecological approach to perceptual learning and development. Oxford University Press, USA. Goodale, M., &amp; Milner, D. (2006). One brain-two visual systems. Psychologist, 19(11), 660–663. Howard, I. P. (2002). Seeing in depth, vol. 1: Basic mechanisms. University of Toronto Press. Jones, M. V., Paull, G. C., &amp; Erskine, J. (2002). The impact of a team’s aggressive reputation on the decisions of association football referees. Journal of Sports Sciences, 20, 991–1000. Kraft, C. (1978). A psychophysical approach to air safety: Simulator studies of visual illusions in night approaches. In h. L. Pick, h. W. Leibowitz, j. E. Singer, a. Steinschneider, &amp; h. W. Steenson (eds.), psychology: From research to practice (2nd ed.). Plenum Press. Lee, J., &amp; Strayer, D. (2004). Preface to the special section on driver distraction. Human Factors, 46(4), 583. Livingstone, M. S. (2000). Is it warm? Is it real? Or just low spatial frequency? Science, 290(5495), 1299–1299. McCann, J. J. (1992). Rules for color constancy. Ophthalmic and Physiologic Optics, 12(2), 175–177. Nickerson, R. S. (1998a). Applied psychology: An international review. Applied Experimental Psychology, 47, 155–173. Nitschke, J. B., Dixon, G. E., Sarinopoulos, I., Short, S. J., Cohen, J. D., Smith, E. E., Kosslyn, S. M., Rose, R. M., &amp; Davidson, R. J. (2006). Altering expectancy dampens neural response to aversive taste in primary taste cortex. Nature Neuroscience, 9(3), 435–442. Plassmann, H., O’doherty, J., Shiv, B., &amp; Rangel, A. (2008). Marketing actions can modulate neural representations of experienced pleasantness. Proceedings of the National Academy of Sciences, 105(3), 1050–1054. Proctor, R. W., &amp; Van Zandt, T. (2008). Human factors in simple and complex systems. (2nd ed.). CRC Press. Ramachandran, V. S., Hubbard, E. M., Robertson, L. C., &amp; Sagiv, N. (2005). The emergence of the human mind: Some clues from synesthesia. In synesthesia: Perspectives from cognitive neuroscience. (2nd ed., pp. 147–190). Oxford University Press. Runeson, S. (1988). The distorted room illusion, equivalent configurations, and the specificity of static optic arrays. Journal of Experimental Psychology: Human Perception and Performance, 14(2), 295–304. Sekuler, R., &amp; Blake, R. (2006). Perception. McGraw-Hill Companies,Incorporated. https://books.google.com/books?id=jKVFAAAAYAAJ Silverstein, L. D., Krantz, J. H., Gomer, F. E., Yeh, Y., &amp; Monty, R. W. (1990). The effects of spatial sampling and luminance quantization on the image quality of color matrix displays. Journal of the Optical Society of America, Part A(7), 1955–1968. Silverstein, L. D., &amp; Merrifield, R. M. (1985). The development and evaluation of color systems for airborne applications: Phase i fundamental visual, perceptual, and display systems considerations (tech. Report DOT/FAA/PM085019) (5th ed., pp. 1955–1968). Federal Aviation Administration. Stern, M., &amp; Karraker, K. H. (1989). Sex stereotyping of infants: A review of gender labeling studies. Sex Roles, 20(9–10), 501–522. Stoffregen, T. A., &amp; Bardy, B. G. (2001). On specification and the senses. Behavioral and Brain Sciences, 24(2), 195–213. Williams, J. R., Markov, Y. A., Tiurina, N. A., &amp; Störmer, V. S. (2022). What you see is what you hear: Sounds alter the contents of visual perception. Psychological Science, 09567976221121348. Yarbus, A. L. (1967). Eye movements and vision. Plenum Press. "],["attention.html", "Chapter 3 Attention 3.1 What is Attention? 3.2 Selective Attention 3.3 Controlling Attention 3.4 Glossary", " Chapter 3 Attention We use the term “attention“ all the time, but what processes or abilities does that concept really refer to? This chapter will focus on how attention allows us to select certain parts of our environment and ignore other parts, and how effectively we can control our attentional resources. A key concept is the idea that we are limited in how much we can do at any one time. So we will also consider what happens when someone tries to do several things at once, such as driving while using electronic devices. LEARNING OBJECTIVES Explain why selective attention is important and how it can be studied. Understand early dichotic listening experiments that informed how we think about selective attention, and models that have since been proposed to describe how we selectively attend to some things over others. Understand that our cognitive system can control our attentional resources, while recognizing the limits and constraints we face. 3.1 What is Attention? Figure 3.1: Are you reading these words right here right now? If so, it’s only because you directed your attention toward them. Photo by Justin Chrn on Unsplash Before we begin exploring attention in its various forms, take a moment to consider how you think about the concept. How would you define attention, or how do you use the term? We certainly use the word very frequently in our everyday language: “ATTENTION! USE ONLY AS DIRECTED!” warns the label on the medicine bottle, meaning be alert to possible danger. “Pay attention!” pleads the weary seventh-grade teacher, not warning about danger (with possible exceptions, depending on the teacher) but urging the students to focus on the task at hand. American psychologist and philosopher William James wrote extensively about attention in the late 1800s. An often quoted passage (James, 1890) beautifully captures how intuitively obvious the concept of attention is, while it remains very difficult to define in concrete and measurable terms: Everyone knows what attention is. It is the taking possession by the mind, in clear and vivid form, of one out of what seem several simultaneously possible objects or trains of thought. Focalization, concentration of consciousness are of its essence. It implies withdrawal from some things in order to deal effectively with others. (pp. 381–382) Notice that this description touches on the conscious nature of attention, as well as the notion that what is in consciousness is often controlled voluntarily but can also be determined by external events that capture our attention. These events are often sensory in nature, such as the check engine light turning on in your car or the sound of someone calling your name from across the room. Implied in James’ description is the idea that we seem to have a limited capacity for information processing, and that we can only attend to or be consciously aware of a small amount of information at any given time. If someone captures your attention by calling your name, you are - at least momentarily - pulled away from what you were doing before they called you. This relates to the concept of selective attention; some information can be attended to while other information is blocked out or ignored. The first part of this chapter will address selective attention and some models that have been proposed to explain how we selectively attend to different sensory inputs. As noted above, we often voluntarily control our attention (a process also known as attentional control). Sometimes we may need to have sustained attention or vigilance to complete a particular task. For example, a crucial issue in World War II was how long an individual could remain highly alert and accurate while watching a radar screen for enemy planes, and this problem led psychologists to study how attention works under such conditions. When watching for a rare event, it is easy to become distracted or allow concentration to lag. However, there are other times when we may need to be flexible and switch our attention to something else. For example, if you are reading while your partner is cooking dinner and the smoke alarm goes off, you should probably switch from paying attention to your book to helping him put out the fire. The second part of this chapter addresses how and how effectively we control our attention, including the consequences of overestimating our ability to attend to multiple sources of information or tasks at the same time, such as texting and driving. 3.2 Selective Attention The Cocktail Party Selective attention is the ability to select certain stimuli in the environment to process, while ignoring distracting information. One way to get an intuitive sense of how attention works is to consider situations in which attention is used. A party provides an excellent example for our purposes. Imagine many people may be milling around, a dazzling variety of colors and sounds and smells, the buzz of many conversations. When walking around, you don’t have to be looking at the person talking; you may start listening with great interest to some gossip while pretending not to hear, and may easily switch to listening to another conversation that grabs your attention as new people walk by. However, once you are engaged in conversation with someone, you quickly become aware that you cannot keep listening to other conversations at the same time. You are also probably not aware of how tight your shoes feel or of the smell of a nearby flower arrangement. Figure 3.2: Beyond just hearing your name from the clamor at a party, other words or concepts, particularly unusual or significant ones to you, can also snag your attention. Photo by Michael Discenza on Unsplash On the other hand, if someone behind you mentions your name, you typically notice it immediately and may start attending to that (much more interesting) conversation. This situation highlights an interesting set of observations. We have an amazing ability to select and track one voice or visual object, even when many things are competing for our attention. But at the same time, we seem to be limited in how much we can attend to at one time, which in turn suggests that attention is crucial in selecting what is important. How does it all work? Dichotic Listening Studies This cocktail party scenario is the quintessential example of selective attention, and it is essentially what some early researchers tried to replicate under controlled laboratory conditions as a starting point for understanding the role of attention in perception (e.g., Cherry, 1953; Moray, 1959). In particular, they used dichotic listening and shadowing tasks to evaluate the selection process. Dichotic listening simply refers to the situation when two messages are presented simultaneously to an individual, with one message in each ear. In order to control which message the person attends to, the individual is asked to repeat back or “shadow” one of the messages as he hears it. For example, let’s say that a story about a camping trip is presented to John’s left ear, and a story about Abe Lincoln is presented to his right ear. The typical dichotic listening task would have John repeat the story presented to one ear as he hears it. Can he do that without being distracted by the information in the other ear? People can become pretty good at the shadowing task, and they can easily report the content of the message that they attended to. But what happens to the message they ignored? Typically, people can tell you if the ignored message sounded masculine or feminine, or other physical characteristics of the speech, but they cannot tell you what the message was about. In fact, many studies have shown that people in a shadowing task were not aware of a change in the language of the message (e.g., from English to German; Cherry (1953)), and they didn’t even notice when the same word was repeated in the unattended ear more than 35 times (Moray, 1959)! Only the basic physical characteristics, such as the pitch of the unattended message, could be reported. On the basis of these types of experiments, we clearly have a limited capacity for processing information for meaning, making the selection process all the more important. How does this selection process work? Models of Selective Attention Broadbent’s Filter Model Many researchers have investigated how selection occurs and what happens to ignored information. Donald Broadbent was one of the first to try to characterize the selection process. His Filter Model was based on the dichotic listening tasks described above as well as other types of experiments (Broadbent &amp; Dal Martello, 1958). He found that people select information on the basis of physical features: e.g., the sensory channel (or ear) that a message was coming in, the pitch of the voice, the color or font of a visual message. People seemed vaguely aware of the physical features of the unattended information, but had no knowledge of the meaning. As a result, Broadbent argued that selection occurs very early, with no additional processing for the unselected information. A flowchart of the model might look like Figure 3.3. Figure 3.3: Broadbent Filter Model. This figure shows information coming in both the left and right ears. Some basic sensory information, such as pitch, is processed, but an internal filter only allows the information from one ear to be processed further. Only the information from the left ear is transferred to short-term memory (STM) and conscious awareness, and then further processed for meaning. Under this model, ignored information never makes it beyond a basic physical analysis. Treisman’s Attenuation Model Broadbent’s model intuitively makes sense, but you may have noticed that one problem is that it cannot account for all aspects of the Cocktail Party Effect. What doesn’t fit? The fact is that you tend to hear your own name when it is spoken by someone, even if you are deeply engaged in a conversation. We mentioned earlier that people in a shadowing experiment were unaware of a word in the unattended ear that was repeated many times — and yet many people noticed their own name in the unattended ear even it occurred only once. Anne Treisman (1960) carried out a number of dichotic listening experiments in which she presented two different stories to the two ears. In line with the standard procedure, she asked people to shadow the message in one ear. As the stories progressed, however, she switched the stories to the opposite ears. Treisman found that individuals spontaneously followed the story, or the content of the message, when it shifted from the left ear to the right ear. Then they realized they were shadowing the wrong ear and switched back. Results like this, and the fact that you tend to hear meaningful information even when you aren’t paying attention to it, suggest that we do monitor the unattended information to some degree on the basis of its meaning. Therefore, Broadbent’s Filter Model can’t be right because it suggests that unattended information is completely blocked at the sensory analysis level. Instead, Treisman suggested that selection starts at the physical or perceptual level, but that the unattended information is not blocked completely, it is just weakened or attenuated. As a result, highly meaningful or pertinent information in the unattended ear will get through the filter for further processing at the level of meaning. A flowchart of her model might look like Figure 3.4. Figure 3.4: Treisman Attenuation Model, an early selection model. This figure shows information coming in both ears, but in contrast to the early selection model, there is no filter that completely blocks nonselected information. Instead, selection of the left ear information strengthens that material, while the nonselected information in the right ear is weakened. However, if the preliminary analysis shows that the nonselected informatio is especially pertinent or meaningful (such as your own name) then the Attenuation Control will instead strengthen the more meaningful information. Late Selection Models Other selective attention models have been proposed as well. A late selection or response selection model proposed by Deutsch &amp; Deutsch (1963) suggests that all information in the unattended ear is processed on the basis of meaning, not just the selected or highly pertinent information (Figure 3.5). However, only the information that is relevant for the task response gets into conscious awareness. This model is consistent with ideas of subliminal perception; in other words, that you don’t have to be aware of or attending a message for it to be fully processed for meaning. Figure 3.5: Deutsch and Deutsch late selection model. This figure shows a similar structure to the early selection model, with the major difference being that the location of the selective filter has changed, here being later on in the process. Here, the model makes the assumption that analysis of meaning occurs before selection occurs, but only the selected information becomes conscious. Load Theory of Attention Why did researchers keep coming up with different models? Because no model really seemed to account for all the data, some of which indicates that nonselected information is blocked completely, whereas some suggests that it can be processed for meaning. The load theory of attention addresses this apparent inconsistency, suggesting that the stage at which selection occurs can change depending on the task. Johnston &amp; Heinz (1978) demonstrated that under some conditions, we can select what to attend to at a very early stage and we do not process the content of the unattended message very much at all. Analyzing physical information, such as attending to information based on whether it sounds like a masculine or feminine voice, is relatively easy; it occurs automatically, rapidly, and doesn’t take much effort. Under the right conditions, we can select what to attend to on the basis of the meaning of the messages. However, the late selection option—processing the content of all messages before selection—is more difficult and requires more effort. The benefit, though, is that we have the flexibility to change how we deploy our attention depending upon what we are trying to accomplish, which is one of the greatest strengths of our cognitive system. Selective Attention Beyond the Auditory Domain This discussion of selective attention has focused on experiments using auditory material, but the same principles hold for other sensory systems as well. Neisser (1979) investigated some of the same questions with visual materials by superimposing two semi-transparent video clips and asking viewers to attend to just one series of actions. As with the auditory materials, viewers often were unaware of what went on in the other video, despite it being clearly visible. Twenty years later, Simons &amp; Chabris (1999) explored and expanded these findings using similar techniques, and triggered a flood of new work in an area referred to as inattentional blindness. In the original study, participants were instructed to complete a task that required paying close attention to certain features of a video clip; in doing so, many of them completely missed other features, such as a man in a gorilla costume walking into the scene (Simons &amp; Chabris, 1999). Subliminal Perception The idea of subliminal perception — that stimuli presented below the threshold for awareness can influence thoughts, feelings, or actions — is a fascinating and kind of creepy one. Can messages you are unaware of, embedded in movies or ads or the music playing in the grocery store, really influence what you buy? Many such claims of the power of subliminal perception have been made. One of the most famous came from a market researcher who claimed that the message “Eat Popcorn” briefly flashed throughout a movie increased popcorn sales by more than 50%, although he later admitted that the study was made up (Merikle, 2004). Psychologists have worked hard to investigate whether this is a valid phenomenon. But studying subliminal perception is more difficult than it might seem, because of the difficulty of establishing what the threshold for consciousness is or of even determining what type of threshold is important. For example, Cheesman &amp; Merikle (1984) made an important distinction between objective and subjective thresholds (see also Cheesman &amp; Merikle, 1986). The bottom line is that there is some evidence that individuals can be influenced by stimuli they are not aware of, but how complex the stimuli can be or the extent to which unconscious material can affect behavior is not settled (e.g., Bargh, 2014; Greenwald, 1992; Harris et al., 2013; Merikle, 2004). 3.3 Controlling Attention As mentioned in the previous section, one of the greatest strengths of our cognitive system is the ability to control how we deploy our cognitive resources to achieve our goals, also known as cognitive control.In the attention domain, we can try to devote all our attention to one thing or we can try to switch our attention to something else. But how good are we at each of these processes? And what happens when we try to do too much at once? Sustaining Attention Imagine trying to do your homework with many external and internal distractors: your phone buzzes from an incoming text message, a thought pops into your head about what you want to eat for dinner, a colorful hummingbird flies by your window. If you have a goal to finish your homework assignment before the deadline, you may have to actively focus your attention on your homework despite these distractors. One part of cognitive control is inhibitory control, or the suppression of goal-irrelevant stimuli (attentional inhibition) or responses (response inhibition) (Tiego et al., 2018). Attentional inhibition is thought to be an important aspect of sustaining attention. Stroop Experiments A common task used to study attention is the Stroop task, named for J.R. Stroop who described it in one of the most highly cited experimental psychology papers ever published (Stroop, 1935). In the classic Stroop task, participants are shown words in different colors, and instructed to say out loud the color of the word (not the word itself) as quickly and accurately as they can. That is, their task is to pay attention to the ink color, and ignore anything else that might distract them. Sometimes words match the color they are printed in, such as the words on the left in Figure 3.6. Other times words are printed in a color that differs from their meanings, such as the words on the right. Figure 3.6: Example of congruent (left) and incongruent (right) stimuli in a classic Stroop paradigm. When the word meaning matches its ink color, or they are congruent, the task is pretty easy and participants respond relatively quickly and accurately. However, when the word meaning doesn’t match its ink color, or they are incongruent, participants tend to respond slower and make more errors (often by reading out the word, rather than its color). Try it yourself! Even with the simple example above, you might notice that you get tripped up with the incongruent stimuli. This is because with incongruent stimuli there is interference between processing the physical features of the word (color) and its semantics (meaning), and we need to inhibit the irrelevant yet salient semantic information to succeed at the task. Even if we try to attend to just one thing, we can still be thrown off if other things are distracting enough. Switching Attention In our previous example about doing homework, we called attention-grabbing items or events “distractors.” However, sometimes our goals change or important new information comes up, and we want or need to switch our attention to something else. In this case, such inputs aren’t distractions, but rather helpful cues to switch our attention. Up until now, it may have sounded like having your attention pulled away or actively switching attention is quite easy and natural. However, it turns out that switching attention is cognitively demanding and can impair performance. Task Switching Experiments A large body of work studies cognitive flexibility, or how we adapt our cognition to new or changing environments or goals. A typical task switching experiment involves first training participants to complete two or more simple tasks that relate to the same set of stimuli, and then having them switch back and forth between them (Monsell, 2003). For example, researchers can train participants to classify the number (e.g., odd or even) or the letter (e.g., vowel or consonant) when shown a number-letter pair. If a subject had to complete the number classification task for these stimuli – E1, 8Z, D3, 7U – the correct responses would be “odd”, “even”, “odd”, “odd”. But if they were cued to switch back and forth between the two tasks for the same stimuli in this example, the correct responses would be “odd”, “consonant”, “odd”, “vowel”. In order to successfully switch in a task-switching experiment, it is thought that some mental processes must happen, which can include switching attention between stimuli or concepts, retrieving different goal states and task-set rules, and activating or adjusting responses (Monsell, 2003). Many different task-switching paradigms have been used in psychology and cognitive neuroscience to understand how we switch between doing different tasks. One reliable behavioral finding is that participants are slower and less accurate in their responding on switch trials compared to non-switch trials. This suggests that switching is cognitively demanding and comes at a cost. Multitasking In spite of the evidence of our limited capacity, many of us like to think that we can do several things at once. Some people claim to be able to multitask without any problem: reading a textbook while watching television or talking with friends, talking on the phone while playing video games, even texting while driving. The fact is that sometimes we seem to juggle several things at once, particularly when some or all of the tasks are “easy.” For example, as we talk to someone while walking down the street, we don’t need to think consciously about what muscle to contract to take our next step. In fact, paying attention to automated skills can lead to a breakdown in performance, or “choking” (e.g., Beilock &amp; Carr, 2001). But what about higher level, more mentally demanding tasks? Is it possible to perform multiple complex tasks at the same time? In one study, two participants were trained to take dictation for spoken words while reading unrelated material for comprehension (Spelke et al., 1976). To establish a baseline performance and determine the amount of cognitive resources needed for each task, the participants first performed each task separately. Then they performed both tasks simultaneously. Next, they completed extensive practice of one hour per day, five days a week, for 17 weeks. Remarkably, the participants were able to learn dictation for lists of words and read for comprehension without any decline in performance for either task. The authors suggested that this may indicate there are no fixed limits on our attentional capacity. However, when the participants were asked to switch to different tasks, such as reading aloud instead of silently, performance was initially impaired. Therefore, the ability to multitask appears to be specific to well-learned tasks. Unless a task is fully automated, many researchers suggest that “multitasking” doesn’t exist – even when you think you are multitasking, you are really just rapidly switching your attention back and forth. A body of research using dual-task paradigms suggests that earlier estimates of our capacity for doing two or more things at the same time were overly optimistic; some cognitive operations cause “bottlenecks” that require exclusive use of a cognitive resource and therefore cannot be done concurrently (Pashler, 1993). This has been supported in experimental psychology and applied to real-world situations, including distracted driving (see next section) and emergency medicine. One review paper argued that it is impossible to multitask unless behaviors are completely automatic, and that in emergency medicine, physicians are instead rapidly switching between small tasks, which can come at a cost (Skaugset et al., 2016). Distracted Driving In today’s technology-driven society, questions regarding multitasking while using electronic devices have become increasingly relevant. Specifically, research investigating the effects of multitasking while driving—under controlled conditions—has produced some surprising results. While distractions such as applying makeup, tending to children in the back seat, fiddling a CD player, or eating a bowl of cereal while driving can impair performance, we often overestimate our ability to multitask behind the wheel. Despite this, cars are being built with ever more advanced technological capabilities that further encourage multitasking. Given these factors, it is important to ask how effective we truly are at dividing our attention in such situations. Figure 3.7: If you look at your phone for just 5 seconds while driving at 55mph, that means you have driven the length of a football field without looking at the road. Photo by Alexandre Boucher on Unsplash Most people acknowledge the distraction caused by texting while driving and the reason seems obvious: Your eyes are off the road and your hands and at least one hand (often both) are engaged while texting. However, the problem is not simply one of occupied hands or eyes, but rather that the cognitive demands on our limited capacity systems can seriously impair driving performance (Strayer et al., 2011). The effect of a cell phone conversation on performance (such as not noticing someone’s brake lights or responding more slowly to them) is just as significant when the individual is having a conversation with a hands-free device as with a handheld phone; the same impairments do not occur when listening to the radio or a book on tape (Strayer &amp; Johnston, 2001). Moreover, studies using eye-tracking devices have shown that drivers are less likely to later recognize objects that they did look at when using a cell phone while driving (Strayer &amp; Drews, 2007). These findings demonstrate that cognitive distractions such as cell phone conversations can produce inattentional blindness, or a lack of awareness of what is right before your eyes (see also Simons &amp; Chabris, 1999). Sadly, although we may think that we can multitask while driving, in fact the percentage of people who can truly perform cognitive tasks without impairing their driving performance is estimated to be only about 2% (Watson &amp; Strayer, 2010). Attention-Deficit/Hyperactivity Disorder (ADHD) By the end of their first decade of life, typically developing children have mastered the complex cognitive operations required to comply with the rules, such as stopping themselves from acting impulsively, paying attention to parents and teachers in the face of distraction, and sitting still despite boredom. For children with Attention-Deficit/Hyperactivity Disorder (ADHD), exercising self-control is a unique challenge. Some people used to believe that children with ADHD were willfully noncompliant due to moral or motivational deficits (Still, 1902). However, scientists now know that noncompliance observed in ADHD can be explained by many factors, including neurological dysfunction. ADHD is the most commonly diagnosed childhood behavior disorder, affecting 3% to 7% of children in the United States, according to the American Psychiatric Association (2000). The core symptoms of ADHD are organized into two clusters: hyperactivity/impulsivity and inattention. Hyperactive and impulsive symptoms are closely related, the former involving moving perpetually (even when stillness is expected) and the latter including acting without considering repercussions. Inattentive symptoms describe difficulty with organization and task follow-through, as well as a tendency to be distracted by external stimuli. Broadly speaking, boys are more likely than girls to experience symptoms from the hyperactive and impulsive cluster (Hartung &amp; Widiger, 1998), while girls are more likely to experience symptoms from the inattentive cluster (Quinn &amp; Madhoo, 2014). Gender differences in how ADHD presents, combined with other factors such as how parents, teachers, or clinicians notice and interpret ADHD symptoms, have contributed to many women and girls with ADHD not being diagnosed or treated (Quinn &amp; Madhoo, 2014). Key Takeaways It may be useful to think of attention as a mental resource, one that is needed to focus on and fully process important information, especially when there is a lot of distracting “noise” threatening to obscure the message. Our selective attention system allows us to find or track an object or conversation in the midst of distractions. Whether the selection process occurs early or late in the analysis of those events has been the focus of considerable research, and in fact how selection occurs may very well depend on the specific conditions. With respect to controlling our attention, in general we can only perform one cognitively demanding task at a time, and even then, we can be distracted from performing that task if the conditions are right. Switching back and forth between different tasks also comes at a cost. When we focus our attention on one task or source of information, we may not even be aware of unattended events even though they might seem too obvious to miss. This type of inattention blindness can occur even in well-learned tasks, such as driving while talking on a cell phone. Exercises Discuss: Discuss the implications of the different models of selective attention for everyday life. For instance, what advantages and disadvantages would be associated with being able to filter out all unwanted information at a very early stage in processing? What are the implications of processing all ignored information fully, even if you aren’t consciously aware of that information? Practice: Think of examples of when you feel you can successfully multitask and when you can’t. What aspects of the tasks or the situation seem to influence divided attention performance? How accurate do you think you are in judging your own multitasking ability? Apply: What are the public policy implications of current evidence of inattentional blindness as a result of distracted driving? Should this evidence influence traffic safety laws? What additional studies of distracted driving would you propose? 3.4 Glossary attentional control Our ability to choose what we pay attention to cognitive control The ability to regulate how we deploy our cognitive resources to achieve our goals cognitive flexibility How we adapt our cognition to new or changing environments or goals dichotic listening An experimental task in which two messages are presented to different ears. inattentional blindness The failure to notice a fully visible object when attention is devoted to something else. inhibitory control The suppression of goal-irrelevant stimuli limited capacity The notion that humans have limited mental resources that can be used at a given time. selective attention The ability to select certain stimuli in the environment to process, while ignoring distracting information. shadowing A task in which the individual is asked to repeat an auditory message as it is presented. subliminal perception The ability to process information for meaning when the individual is not consciously aware of that information. References Association, A. P. (2000). Diagnostic and statistical manual of mental disorders (4eme ed.) washington. Bargh, J. A. (2014). Our unconscious mind. Sci. Am., 310(1), 30–37. Beilock, S. L., &amp; Carr, T. H. (2001). On the fragility of skilled performance: What governs choking under pressure? J. Exp. Psychol. Gen., 130(4), 701–725. Broadbent, D. E., &amp; Dal Martello, M. F. (1958). Perception and communication. Psychological Science. Cheesman, J., &amp; Merikle, P. M. (1984). Priming with and without awareness. Percept. Psychophys., 36(4), 387–395. Cheesman, J., &amp; Merikle, P. M. (1986). Distinguishing conscious from unconscious perceptual processes. Can. J. Psychol., 40(4), 343–367. Cherry, E. C. (1953). Some experiments on the recognition of speech, with one and with two ears. Journal of the Acoustical Society of America, 25, 975–979. Deutsch, J. A., &amp; Deutsch, D. (1963). Attention: Some theoretical considerations. Psychol. Rev., 70(1), 80–90. Greenwald, A. G. (1992). New look 3: Unconscious cognition reclaimed. Am. Psychol., 47(6), 766–779. Harris, C. R., Coburn, N., Rohrer, D., &amp; Pashler, H. (2013). Two failures to replicate high-performance-goal priming effects. PloS One, 8(8), e72467. Hartung, C. M., &amp; Widiger, T. A. (1998). Gender differences in the diagnosis of mental disorders: Conclusions and controversies of the DSM–IV. Psychological Bulletin, 123(3), 260. James, W. (1890). Principles of psychology. Johnston, W. A., &amp; Heinz, S. P. (1978). Flexibility and capacity demands of attention. J. Exp. Psychol. Gen., 107(4), 420–435. Merikle, P. (2004). Subliminal perception. In Encyclopedia of psychology, vol. 7 (pp. 497–499). American Psychological Association. Monsell, S. (2003). Task switching. Trends in Cognitive Sciences, 7(3), 134–140. Moray, N. (1959). Attention in dichotic listening: Affective cues and the influence of instructions. Q. J. Exp. Psychol., 11(1), 56–60. Neisser, U. (1979). The control of information pickup in selective looking. In Perception and its development (pp. 201–219). Psychology Press. Pashler, H. (1993). Doing two things at the same time. American Scientist, 81(1), 48–55. Quinn, P. O., &amp; Madhoo, M. (2014). A review of attention-deficit/hyperactivity disorder in women and girls: Uncovering this hidden diagnosis. The Primary Care Companion for CNS Disorders, 16(3), 27250. Simons, D. J., &amp; Chabris, C. F. (1999). Gorillas in our midst: Sustained inattentional blindness for dynamic events. Perception, 28(9), 1059–1074. Skaugset, L. M., Farrell, S., Carney, M., Wolff, M., Santen, S. A., Perry, M., &amp; Cico, S. J. (2016). Can you multitask? Evidence and limitations of task switching and multitasking in emergency medicine. Annals of Emergency Medicine, 68(2), 189–195. Spelke, E., Hirst, W., &amp; Neisser, U. (1976). Skills of divided attention. Cognition, 4(3), 215–230. Still, G. F. (1902). The goulstonian lectures. Some Abnormal Psychical Conditions in Children, 1008–1012. Strayer, D. L., &amp; Drews, F. A. (2007). Cell-phone induced inattention blindness. Current Directions in Psychological Science, 16, 128–131. Strayer, D. L., &amp; Johnston, W. A. (2001). Driven to distraction: Dual-task studies of simulated driving and conversing on a cellular telephone. Psychol. Sci., 12(6), 462–466. Strayer, D. L., Watson, J. M., &amp; Drews, F. A. (2011). Cognitive distraction while multitasking in the automobile. In Advances in research and theory (pp. 29–58). Elsevier. Stroop, J. R. (1935). Studies of interference in serial verbal reactions. Journal of Experimental Psychology, 18(6), 643. Tiego, J., Testa, R., Bellgrove, M. A., Pantelis, C., &amp; Whittle, S. (2018). A hierarchical model of inhibitory control. Frontiers in Psychology, 9, 1339. Treisman, A. M. (1960). Contextual cues in selective listening. Quarterly Journal of Experimental Psychology, 12(4), 242–248. Watson, J. M., &amp; Strayer, D. L. (2010). Supertaskers: Profiles in extraordinary multitasking ability. Psychon. Bull. Rev., 17(4), 479–485. "],["working-memory-chapter.html", "Chapter 4 Short-term and Working Memory 4.1 Short-Term Memory 4.2 Working Memory 4.3 Glossary", " Chapter 4 Short-term and Working Memory Working memory is like your mind’s workspace: a limited capacity system for storage and processing of information. Have you ever noticed that it’s hard to understand someone if they throw too much information at you at once? That experience of confusion when receiving more information than we can process is called working memory overload. But working memory does more than simply hold information, it also processes and manipulates whatever information you’re currently thinking about. In fact, the name scientists use for the system changed over time from “short-term memory” to “working memory” to emphasize the processing function of this stage of memory. LEARNING OBJECTIVES Distinguish between the concepts of short-term memory and working memory. Explain the roles of knowledge and distinctiveness in working memory capacity. Explain the evidence for the separate components of working memory. When people talk about memory, they are describing the mind’s ability to encode, store, and retrieve information. Our ability to remember is what allows us to learn from our experiences. How does our brain store and later retrieve information? Many different models of memory have evolved in an attempt to answer this question. Distinctions are drawn between working memory and long-term memory based on the period of time information is accessible after it is first encountered. Sensory memory has the smallest time span for accessibility of information. With short-term and working memory, information is accessible seconds to minutes after it is first encountered. Long-term memory has an accessibility period from minutes to years to decades. In more recent research, the distinctions between working and long-term memory focus more so on whether attention is being used to actively hold things in mind. Under that interpretation, working memory lasts as long as attention is involved. While the focus of this chapter is on working memory, we will start describing what working memory used to be called: short-term memory. 4.1 Short-Term Memory Figure 4.1: Atkinson and Shiffrin’s Modal Model of Memory. In the middle of the 20th century, many scientists were interested in short-term memory (STM), or how humans can hold small amounts of information actively in their minds for a short period of time. Atkinson &amp; Shiffrin (1968) wrote a landmark paper that synthesized existing evidence on how memory functions and proposed a model of memory referred to as the Modal Model of Memory (4.1). In this model, information first enters sensory memory, which is a highly transient storage space for information that recently entered your sensory system. This information is high quality but fades away very quickly. Have you ever had the experience of hearing someone say something when you weren’t really paying attention, then repeating it immediately in your head, and then being able to understand it? For about three seconds, you can play back the auditory information that you just heard. You can even hear it in the speaker’s original tone of voice! You use your sensory memory to do this. The next stage the Modal Model is short-term memory. Information that you pay attention to from sensory memory enters the short-term memory store. As the name suggests, information is retained in the Short Term Memory for a rather short period of time (15–30 seconds). In order to keep information in short-term memory, the model suggests you must rehearse it. How much information can be held in short-term memory? According to George Miller (1959), the capacity of short-term memory is five to nine pieces of information (The magical number seven, plus or minus two). That’s why I could read a 7-digit phone number to you and have you repeat it back, but if I read you my 16-digit credit card number and asked you to repeat it to me, it would probably feel impossible. What counts as a “piece of information?” A piece of information is called a chunk, which is a meaningful unit of information. All of the following can be chunks: single digits or letters, whole words, or even sentences. An example of chunking information is the following. Try to remember the following digits: 1 2 2 5 1 9 8 4 Now try to remember the same digits, but group them differently: 12 - 25 - 1984 With this strategy you chunked eight pieces of information (eight digits) to three pieces to remember them as a date on the calendar. You could chunk the information even more efficiently if you recognize 12-25 as a single unit, the date of Christmas. The process of chunking is the process of combining smaller units of information into larger, meaningful units of information. The term “meaningful” is subjective— a meaningful chunk for you might not be a meaningful chunk for me. For example, 3 5 3 1 might not make a meaningful chunk for everyone, but if you’re a student of football history, you might chunk it as 35–31: the final score of the Super Bowl XIII, when Jackie Smith dropped a touchdown pass in the end zone. A famous experiment concerned with chunking was conducted by Chase &amp; Simon (1973) with novices and experts in chess playing. When asked to remember certain arrangements of chess pieces on a board, the experts performed significantly better that the novices. However, if the pieces were arranged randomly, i.e. not corresponding to possible game situations, both the experts and the novices performed equally poorly. The reason is that expert chess players spend hours studying chess games and memorizing board configurations. When trying to remember the layout of a chess board, the experienced chess players do not try to remember single positions of the figures in the correct game situation, but whole chunks of figures from their memory. In random board configurations this strategy cannot work, which shows that chunking (as done by experienced chess players) enhances the performance only in specific memory tasks. Figure 4.2: Left: A chessboard in the middle of a real game. Right: A random arrangement of chess pieces. Chess experts have significantly better memory for real boards, but perform more like novices for random board configurations. The third stage of the Modal Model is long-term memory. According to the Modal Model, the important processes for long-term memory are storage (i.e., transferring information from short-term to long-term memory), search (i.e., locating information in the long-term store) and retrieval (i.e., recovering the information from the long-term store). Long-term memory will be the focus of chapters 5 and 6. From the Modal Model to Baddeley’s Working Memory Model Atkinson &amp; Shiffrin (1968) formulated the Modal Model as a foundational framework to guide researchers in developing memory models (Wixted, 2024). Beyond the simple model, they depicted the short-term store as the central hub of memory activity, one that involved both verbal and visual stores (Atkinson &amp; Shiffrin, 1968, 1971). Building on these ideas, Baddeley &amp; Hitch (1974) proposed their Working Memory Model. While the Modal Model was relatively focused on the memory function of the short-term store, Baddeley and Hitch described a working memory as chiefly an information processing system. Notice the difference in terminology: Atkinson and Shiffrin called the temporary memory space “short-term memory” while Baddeley and Hitch use the term “working memory.” Atkinson and Shiffrin would not dispute this terminology – indeed, the first page of their introduction says “the short-term store is the subject’s working memory” (Atkinson &amp; Shiffrin, 1968, p. 90) – though it has become a common misconception that their 1968 paper asserts that short-term memory is limited to simple storage (Wixted, 2024). Today, scientists may still use the term “short-term memory” when discussing the temporary storage of simple items, but “working memory” has become the preferred term, emphasizing the role of the system in complex cognition. 4.2 Working Memory According to Baddeley, working memory is capable of both storage and manipulation of incoming information. Baddeley and Hitch’s 1974 model consists of three parts: two storage spaces called the phonological loop and the visuospatial sketch pad, and a control unit called the central executive. We will consider each part in turn: The phonological loop is responsible for auditory and verbal information, such as phone numbers, people’s names, or general conversation. One source of evidence that we have a special storage space for auditory information is the phonological similarity effect. Read the following list of words, then look away and try to repeat it to yourself: car rig seam bar rose pop gear And now try this one: leak feed beak deep heat peek beat If you are like the participants in a study by Conrad (1964), your performance was worse on the second list than it was in the first. The reason the second list was harder is because when you read the words on the page, you translate them into an acoustic form. Because the words in the second list sound alike, you are more likely to confuse them as you repeat them in your phonological loop. How much information can the phonological loop hold? Researchers have found that the magical number seven plus or minus two does not explain all of the available data. While Miller’s magical number is approximately accurate when English-speaking participants remember digits or letters, it doesn’t hold when the length of the words is manipulated. To demonstrate this to yourself, try to remember the following list of words: lip base rain duck bib fall gate And now try this one: carpenter radiate thermostat honesty photograph dinosaur horizon Both lists are seven words long, and yet people are much worse at a list like the second one (Baddeley et al., 1975). This is called the word-length effect: lists of short words are recalled better than lists of long words. If you think back to early research on short-term memory, this result is surprising! According to Miller’s magical number, these lists should be remembered equally well because they contain the same number of items. Findings like the word length effect led researchers to conclude that the capacity of the phonological loop should not be measured in number of items, but in amount of time instead: the phonological loop can hold about two seconds of auditory information, which it can replay over and again through an active articulatory process. Imagine you have two seconds of tape— you could fit a lot more short words on it than long words! The next component of working memory is the visuospatial sketch pad, which handles visual and spatial information. Like the phonological loop, the visuospatial sketch pad is primarily a storage space. What evidence do we have that visual and spatial information is stored independently from auditory and verbal information? Look at the block letter F to the right. In an example experiment, you would be instructed to memorize the letter, and then, starting with the starred corner and then traveling up and around the letter, indicate whether each corner is an outside corner (like the starred corner) or an inside corner (like the fifth corner as you travel around the shape). In one condition, participants would verbally say “outside” or “inside” for each corner. In the other condition, participants would point at the words “outside” and “inside” displayed in front of them for each corner. Brooks (1968) conducted an experiment similar to this one, and found that participants were much better at the task when they could verbally indicate the type of corner than when they had to point. Why? We have two storage spaces in working memory and each of them is limited in capacity. Mentally traveling around the block letter and judging the corners is a spatial task, and so it puts a load on the visuospatial sketch pad. If you add pointing on top of that, participants’ visuospatial sketch pads get overloaded and they struggle to do both simultaneously. If instead you allow participants to respond verbally, they are distributing the response aspect of the task onto the phonological loop. That way, neither storage system becomes overloaded. We have seen that the phonological loop and the visuospatial sketch pad deal with different kinds of information, which nonetheless have to interact in order to do certain tasks. The component that connects these two systems is the central executive. The central executive coordinates the activity of both the phonological loop and the visuospatial sketch pad. In fact, most of the “working” part of working memory is done by the central executive. The functions of the central executive can be broken down into three categories: shifting, updating, and inhibition (Miyake et al., 2000). Shifting refers to engaging and disengaging from tasks, such as switching your attention back and forth between watching television and doing the dishes. Updating refers to monitoring information that is incoming into working memory, and making room for it by replacing old information in working memory. Inhibition refers to the deliberate inhibition of responses, such as when the ticket taker says “enjoy your movie!” and you stop yourself from saying, “you too!” The episodic buffer Science is an ongoing process, and so despite the usefulness of Baddeley and Hitch’s working memory model, it was updated in 2000 to add another component: the episodic buffer (Baddeley, 2000). The episodic buffer is a limited capacity, temporary storage system that is controlled by the central executive and integrates information from a variety of sources including long-term memory. The episodic buffer was added to help account for human performance in complex cognition— for example, we can remember many more words when we remember a meaningful sentence than when we remember a random word list. The episodic buffer was added to help account for this and other phenomena in which working memory performance seemingly requires additional storage as well as interfacing with long-term memory. The addition of the Episodic Buffer paralleled broader movement in the field towards thinking about how knowledge impacts working memory capacity. Working Memory, Meaning, and Distinctiveness People not only have better memory for items that are more meaningful to them (e.g., you can remember 7 real-world objects better than 7 colors), but also for items that are distinct (Brady et al., 2016). The idea that we have better memory for more distinct items in a set (i.e., remembering blue vs. red is easier than remembering blue vs. teal) has been around for almost a century. Von Restorff (1933) showed that certain things were more or less memorable, not in and of themselves, but in relation to how distinct they were from other stimuli in the set. This idea has recently re-emerged in more recent research on working memory as a way to explain capacity limitations as well as memory errors. The finding that distinctive items can be remembered better than similar items has been reproduced many times, with many different kinds of stimuli. For instance, expert radiologists have the best memory for mammograms that are the most distinct from other mammograms (Schill et al., 2021). Research into memory and distinctiveness has also led to recent advances in understanding memory errors. For example, researchers have found that the similarity between two items can almost perfectly predict the memory errors we make. The more similar two items are, the more likely we are to think that the similar item was the thing we were trying to remember. (Schurgin et al., 2020). This has many real world implications such as in the field of eyewitness testimony, which will be touched on in later chapters. Key Takeaways We are very limited in how much information we can hold actively in our minds. Early research showed we could hold 7 plus or minus two chunks of information. More recent research focuses more on characterizing working memory dynamics. Working memory has three helper systems: the phonological loop, the visuospatial sketchpad, and the episodic buffer. The central executive controls attention and coordinates these helper systems. Our prior knowledge has a large influence on how much information we can hold in working memory; for example we can hold more information if we can group it into meaningful chunks. Distinctiveness also plays an important role. Exercises Discuss. While regular people can hold around 7 digits in working memory at once, competitive memorizers can hold over a thousand. Incredibly, these people don’t have enhanced working memory capacity – they just spend a lot of time practicing memory strategies! Based on what you’ve learned about chunking, what memory strategies might these competitors use? Practice. Make lists of short and long words, and quiz your friends to see which they can remember more of. 4.3 Glossary central executive A component of working memory that controls attention and coordinates the activity of the helper systems chunk A meaningful unit of information held in short-term memory episodic buffer A helper system in working memory for storing integrated chunks of information long-term memory Large-capacity storage for enduring memories. phonological loop A helper system in working memory for storing information encoded as sound phonological similarity effect Reduction in working memory span for similar-sounding words sensory memory Highly transient storage space for information that recently entered your sensory system short-term memory Small amounts of information actively held in the minds for a short period of time visuospatial sketch pad A helper system in working memory for storing information encoded spatially or visually word-length effect A reduction in working memory span for longer words compared to shorter words working memory A function of the brain that allows us to actively hold and manipulate a limited amount of information. References Atkinson, R. C., &amp; Shiffrin, R. M. (1968). Human memory: A proposed system and its control processes. In Psychology of learning and motivation (Vol. 2, pp. 89–195). Elsevier. Atkinson, R. C., &amp; Shiffrin, R. M. (1971). The control of short-term memory. Scientific American, 225(2), 82–91. Baddeley, A. D. (2000). The episodic buffer: A new component of working memory? Trends Cogn. Sci., 4(11), 417–423. Baddeley, A. D., &amp; Hitch, G. (1974). Working memory (G. H. Bower, Ed.; Vol. 8, pp. 47–89). Academic Press. Baddeley, A. D., Thomson, N., &amp; Buchanan, M. (1975). Word length and the structure of short-term memory. Journal of Verbal Learning and Verbal Behavior, 14(6), 575–589. Brady, T. F., Störmer, V. S., &amp; Alvarez, G. A. (2016). Working memory is not fixed-capacity: More active storage capacity for real-world objects than for simple stimuli. Proceedings of the National Academy of Sciences, 113(27), 7459–7464. Brooks, L. R. (1968). Spatial and verbal components of the act of recall. Can. J. Psychol., 22(5), 349–368. Chase, W. G., &amp; Simon, H. A. (1973). Perception in chess. Cogn. Psychol., 4(1), 55–81. Conrad, R. (1964). Acoustic confusions in immediate memory. British Journal of Psychology, 55(1), 75–84. Miyake, A., Friedman, N. P., Emerson, M. J., Witzki, A. H., Howerter, A., &amp; Wager, T. D. (2000). The unity and diversity of executive functions and their contributions to complex “frontal lobe” tasks: A latent variable analysis. Cogn. Psychol., 41(1), 49–100. Schill, H. M., Wolfe, J. M., &amp; Brady, T. F. (2021). Relationships between expertise and distinctiveness: Abnormal medical images lead to enhanced memory performance only in experts. Memory &amp; Cognition, 49(6), 1067–1081. Schurgin, M. W., Wixted, J. T., &amp; Brady, T. F. (2020). Psychophysical scaling reveals a unified theory of visual memory strength. Nature Human Behaviour, 4(11), 1156–1172. Von Restorff, H. (1933). Über die wirkung von bereichsbildungen im spurenfeld. Psychologische Forschung, 18(1), 299–342. Wixted, J. T. (2024). Atkinson and shiffrin’s (1968) influential model overshadowed their contemporary theory of human memory. Journal of Memory and Language, 136, 104471. "],["long-term-memory-chapter.html", "Chapter 5 Long-term Memory 5.1 Working Memory Vs. Long-Term Memory 5.2 Structure 5.3 Encoding, Retrieval, and Consolidation 5.4 Glossary", " Chapter 5 Long-term Memory Our memories allow us to do relatively simple things, such as remembering where we parked our car or the name of the current governor of California, but also allow us to form complex memories, such as how to ride a bicycle or to write a computer program. Moreover, our memories define us as individuals — they are our experiences, our relationships, our successes, and our failures. Without your memories, would you still be you? LEARNING OBJECTIVES Compare and contrast working memory and long-term memory, identifying the features that define each. Compare and contrast explicit and implicit memory, identifying the features that define each. Label and review the principles of encoding, storage, and retrieval. Describe how the context in which we learn information can influence our memory of that information. 5.1 Working Memory Vs. Long-Term Memory As we discussed in the last chapter, working memory is a temporary storage space for information that is being actively stored and manipulated in consciousness. Information that is not rehearsed will be forgotten within 18 to 30 seconds. Long-term memory, on the other hand, is where we store everything from a few moments to the earliest thing we can remember. There is theoretically no upper limit to the amount of information we can store in long-term memory. The Serial Position Curve Figure 5.1: The serial position curve is the result of both primacy effects and recency effects. The distinction between working memory and long-term memory can be demonstrated with the serial position curve. When we give people a long list of words one at a time (e.g., on flashcards) and then ask them to recall them, the results look something like those in 5.1. People are able to retrieve more words that were presented to them at the beginning and the end of the list than they are words that were presented in the middle of the list. This pattern, known as the serial position curve, is caused by two retrieval phenomenon: The primacy effect refers to a tendency to better remember stimuli that are presented early in a list. The recency effect refers to the tendency to better remember stimuli that are presented at the end of a list. Primacy and recency effects can be explanied in terms of the effects of rehearsal on short-term and long-term memory (Baddeley, 1990). Because we can keep the last words that we learned in the presented list in short-term memory by rehearsing them before the memory test begins, they are relatively easily remembered. So the recency effect can be explained in terms of maintenance rehearsal in short-term memory— the most recent words are still available in short-term memory at the time of recall. And the primacy effect can also be explained by rehearsal—when we hear the first word in the list we start to rehearse it, making it more likely that it will be moved from short-term to long-term memory. And the same is true for the other words that come early in the list. But for the words in the middle of the list, this rehearsal becomes much harder, making them less likely to be moved to long-term memory Brains Vs. Computers The distinction between short-term memory and long-term memory might remind you of the distinction between RAM and ROM in a computer. Are brains just computers that live in our heads? Not exactly. Here are some key differences: In computers, information can be accessed only if one knows the exact location of the memory. In the brain, information can be accessed through spreading activation from closely related concepts. Further, there is no exact location of a stored memory in the brain. Computers differentiate memory (e.g. the hard drive) from processing (the central processing unit), but in brains there is no such distinction. In the brain (but not in computers) existing memory is used to interpret and store incoming information, and retrieving information from memory changes the memory itself. The brain is self-organizing and self-reparing, but computers are not. If a person suffers a stroke, neural plasticity will help them recover. The brain is significantly more complex than any current computer. The brain is estimated to have 25,000,000,000,000,000 (25 million billion) interactions among axons, dendrites, neurons, and neurotransmitters, and that doesn’t include the approximately 1 trillion glial cells that may also be important for information processing and memory. 5.2 Structure According to Baddeley’s model, working memory includes a central executive, phonological loop, visuospatial sketchpad, and episodic buffer. What is the structure of long-term memory? As you can see in Figure 5.2, long-term memory can be divided into two major categories of memory types: explicit memory and implicit memory, which can be further divided into multiple sub-types: semantic, episodic, procedural, priming, and conditioning memory. Figure 5.2: Structure of long-term memory. Explicit Memory The first form of long-term memory we will discuss is explicit memory. We are measuring explicit memory when we assess memory by asking a person to consciously remember things. Explicit memory refers to knowledge or experiences that can be consciously remembered. There are two types of explicit memory: episodic and semantic. Episodic memory refers to the firsthand experiences that we have had (e.g., recollections of our high school graduation day or of the fantastic dinner we had in New York last year). [Semantic memory] refers to our knowledge of facts and concepts about the world (e.g., that the absolute value of −90 is greater than the absolute value of 9 and that one definition of the word “affect” is “the experience of feeling or emotion”). Explicit memory is assessed using measures in which the individual being tested must consciously attempt to remember the information. A recall memory test is a measure of explicit memory that involves bringing from memory information that has previously been remembered. We rely on our recall memory when we take an essay test, because the test requires us to generate previously remembered information. A multiple-choice test is an example of a recognition memory test, a measure of explicit memory that involves determining whether information has been seen or learned before. Your own experiences taking tests will probably lead you to agree with the scientific research finding that recall is more difficult than recognition. Recall, such as required on essay tests, involves two steps: first generating an answer and then determining whether it seems to be the correct one. Recognition, as on multiple-choice test, only involves determining which item from a list seems most correct (Haist et al., 1992). Although they involve different processes, recall and recognition memory measures tend to be correlated. Students who do better on a multiple-choice exam will also, by and large, do better on an essay exam (Bridgeman &amp; Morgan, 1996). A third way of measuring memory is known as relearning (Nelson, 1985). Measures of relearning (or savings) assess how much more quickly information is processed or learned when it is studied again after it has already been learned but then forgotten. If you have taken some French courses in the past, for instance, you might have forgotten most of the vocabulary you learned. But if you were to work on your French again, you’d learn the vocabulary much faster the second time around. Relearning can be a more sensitive measure of memory than either recall or recognition because it allows assessing memory in terms of “how much” or “how fast” rather than simply “correct” versus “incorrect” responses. Relearning also allows us to measure memory for procedures like driving a car or playing a piano piece, as well as memory for facts and figures. Implicit Memory While explicit memory consists of the things that we can consciously report that we know, implicit memory refers to knowledge that we cannot consciously access. However, implicit memory is nevertheless exceedingly important to us because it has a direct effect on our behavior. Implicit memory refers to the influence of experience on behavior, even if the individual is not aware of those influences. As you can see in Figure 5.2, there are three general types of implicit memory: procedural memory, classical conditioning effects, and priming. Procedural memory refers to our often unexplainable knowledge of how to do things. When we walk from one place to another, speak to another person in English, dial a cell phone, or play a video game, we are using procedural memory. Procedural memory allows us to perform complex tasks, even though we may not be able to explain to others how we do them. There is no way to tell someone how to ride a bicycle; a person has to learn by doing it. The idea of implicit memory helps explain how infants are able to learn. The ability to crawl, walk, and talk are procedures, and these skills are easily and efficiently developed while we are children despite the fact that as adults we have no conscious memory of having learned them. Also note that within procedural memory, it is often divided into three sub-types: cognitive, motor, and perceptual. A second type of implicit memory is classical conditioning effects, in which we learn, often without effort or awareness, to associate neutral stimuli (such as a sound or a light) with another stimulus (such as food), which creates a naturally occurring response, such as enjoyment or salivation. The memory for the association is demonstrated when the conditioned stimulus (the sound) begins to create the same response as the unconditioned stimulus (the food) did before the learning. The final type of implicit memory is known as priming, or changes in behavior as a result of experiences that have happened frequently or recently. Priming refers both to the activation of knowledge (e.g., we can prime the concept of kindness by presenting people with words related to kindness) and to the influence of that activation on behavior (people who are primed with the concept of kindness may act more kindly). One measure of the influence of priming on implicit memory is the word fragment test, in which a person is asked to fill in missing letters to make words. You can try this yourself: First, try to complete the following word fragments, but work on each one for only three or four seconds. Do any words pop into mind quickly? _ i b _ a _ y _ h _ s _ _ i _ n _ o _ k _ h _ i s _ Now read the following sentence carefully: “He got his materials from the shelves, checked them out, and then left the building.” Then try again to make words out of the word fragments. I think you might find that it is easier to complete fragments 1 and 3 as “library” and “book,” respectively, after you read the sentence than it was before you read it. However, reading the sentence didn’t really help you to complete fragments 2 and 4 as “physician” and “chaise.” This difference in implicit memory probably occurred because as you read the sentence, the concept of “library” (and perhaps “book”) was primed, even though they were never mentioned explicitly. Once a concept is primed it influences our behaviors, for instance, on word fragment tests. Our everyday behaviors are influenced by priming in a wide variety of situations. Seeing an advertisement for cigarettes may make us start smoking, seeing the flag of our home country may arouse our patriotism, and seeing a student from a rival school may arouse our competitive spirit. And these influences on our behaviors may occur without our being aware of them. 5.3 Encoding, Retrieval, and Consolidation Imagine you are able to perfectly study for an exam. You take notes in lecture and read the textbook as the quarter moves along. As you approach the exam, you develop study materials, test yourself on the information, and go to the professor’s office hours to ask about the parts you find the most difficult. The day before the exam, you explain all of the important concepts from class to your best friend. You get a good night’s sleep, and the next morning you find that remembering the important concepts from class feels relatively easy. The questions on the exam include bits of information that help you retrieve the concepts that you studied so hard to understand. You leave feeling like your exam performance was a good reflection of the hard work you put in to studying. Figure 5.3: Photo by Robert Bye on Uplash. In this situation, you were able to successfully encode, retrieve, and consolidate the information you sought to learn. Encoding refers to storing new information in long-term memory. This is the process you engaged in during lecture and studying. Retrieval refers to remembering information from long-term memory. This is what you did when you tested yourself on information and when you took the exam. Consolidation is the stabilization of long-term memories after initial encoding. Consolidation insulates your memories from interference from new memories that are formed. Interference causes a retreival failure of the target memory. Consolidation is aided by sleep, which is why you felt even more confident in your knowledge by getting a good night of sleep before the exam. The following sections will discuss the factors that affect encoding, retrieval, and consolidation. Encoding Maintenance rehearsal Maintenance rehearsal is a type of memory rehearsal that is useful in maintaining information in working memory. Because this usually involves repeating information without thinking about its meaning or connecting it to other information, the information is not usually transferred to long term memory. That is, maintenance rehearsal does not usually lead to encoding new long-term memories. An example of maintenance rehearsal would be repeating a phone number mentally, or aloud until the number is entered into the phone to make the call. The number is held in working memory long enough to make the call, but never transferred to long term memory. An hour, or even five minutes after the call, the phone number will no longer be remembered. Depth of processing The levels-of-processing effect, identified by Craik &amp; Tulving (1975), describes memory recall of stimuli as a function of the depth of mental processing at encoding. Deeper levels of analysis produce more elaborate, longer-lasting, and stronger memory traces than shallow levels of analysis. Depth of processing falls on a shallow to deep continuum. Shallow processing (e.g., processing based on phonemic and orthographic components) leads to a fragile memory trace that is susceptible to rapid decay. Conversely, deep processing (e.g., semantic processing) results in a more durable memory trace. This theory contradicts the notion that merely repeating something in your head (i.e., maintenance rehearsal) will improve long-term memory. Though the idea that type of rehearsal is important for memory, rather than mere duration of rehearsal, had been proposed previously (e.g., the Atkinson &amp; Shiffrin (1968) paper discussed in Chapter 4 proposed that some coding processes used during rehearsal are more effective than others), this theory was formalized by Craik &amp; Lockhart (1972) and tested experimentally by Craik &amp; Tulving (1975). Figure 5.4: Levels of processing based on evidence from Craik and Tulving. In a study from Craik &amp; Tulving (1975) participants were given a list of 60 words. Each word was presented along with three questions. The participant had to answer one of them. Those three questions were in one of three categories. One category of questions was about how the word was presented visually (“Is the word shown in italics?”). This category of questions was meant to promote orthographic processing, or processing related to how the word was written. The second category of questions was about the phonemic qualities of the word (“Does the word begin with the sound ‘bee’?”). This category was meant to promote phonological processing, or processing related to how the words sound. The third category of questions was presented so that the reader was forced to think about the word within a certain context (“Can you meet one in the street [a friend]”?). This category of questions was meant to promote semantic processing, or processing related to the words’ meaning. The result of this study showed that the more deeply words were processed at encoding, the more likely they were to be remembered later. Later work by Rogers et al. (1977) expanded the levels-of-processing effect by demonstrating an even deeper level of processing than semantic processing: self-referential processing (e.g., “Does this word describe you?”). This is referred to as the self-reference effect: processing words in terms of their relation to yourself promotes an even higher rate of recall than normal semantic processing. The Testing Effect The testing effect is that retrieval from memory rather than simple restudy is a more effective way to cement information into memory. For example, if you are trying to learn that the mitochondria is the powerhouse of the cell, you could either reread this fact or create a flashcard asking “what is the powerhouse of the cell?” and retrieving from memory that it is the “mitochondria”. The latter way is a more effective study strategy. Particularly in the case when correct answer feedback is provided. The testing effect has been ubiquitously found in a variety of declarative learning context, so regardless of what you are trying to learn, it is an effective method (Kromann et al., 2009; Lantz, 2010; Larsen et al., 2009; McDaniel et al., 2007; Nungester &amp; Duchastel, 1982; Pan et al., 2015; Rickard &amp; Pan, 2018). Although the testing effect was first noted by Abbot (1909), the past decade has seen a resurgence in research. With this resurgence is also a better understanding of potential mechanisms of the testing effect. Our current understanding has two leading categories of mechanisms. One theory assumes, active retrieval of a memory reinstates the previous contexts, which strengthens said memory and that memory search is restricted through episodic cues, making it quicker to find the memory Karpicke et al. (2014). A newer theory proposes that the testing effect is caused by the simple fact that testing causes two memories to be created, a study memory initially created during the initial acquisition of information and a test memory that is created during testing. Further, testing with correct answer feedback will always strengthen the study memory. Thus, the testing effect is the result of the fact that there are two potential memories that could be retrieved for the same bit of information Gupta et al. (2022). The Spacing Effect The spacing effect, which isthe benefit of temporally spaced practice relative to massed practice, is one of the most robust phenomena that improves declarative memory (A. S. Benjamin &amp; Tullis, 2010; Cepeda et al., 2008; Lohnas &amp; Kahana, 2014; Mozer et al., 2009; Polyn et al., 2009). Or in other words, spacing out your learning versus cramming for an exam the night before will yield better retention over a longer period of time. Note that this is the spaced repetition of the same information and not different information. In general, models propose three potential mechanisms of the spacing effect. contextual drift, study-phase retrieval, and multiple learning time scales. Contextual drift refers to the diversity of contexts at each study time point. The more diverse the contexts over encounters, the stronger the resulting memory. Study-phase retrieval is the reactivation of a prior memory, including its original context, upon restudy. Thus, upon each encounter of an item, the prior memory of that item will be reactivated, increasing its strength. Information can be represented at different time scales. Some information can only be represented at fast time scales and other information can only be represented at slower time scales. For example, observed contexts of information is most likely to be represented at slower time scales. Used together, the testing effect and the spacing effect are two of the most robust and effective strategies for learning. Retrieval Information stored in the memory is retrieved by way of association with other memories. Some memories can not be recalled by simply thinking about them. Rather, one must think about something associated with it. For example, if someone tries and fails to recollect the memories he had about a vacation he went on, and someone mentions the fact that he hired a classic car during this vacation, this may make him remember all sorts of things from that trip, such as what he ate there, where he went and what books he read. Figure 5.5: Photo albums can be effective sources of retrieval cues. Photo by BBH Singapore on Uplash. Encoding Specificity Principle The encoding specificity principle is the general principle that memory is best when the conditions at encoding match the conditions at retrieval. For example, take the song on the radio: perhaps you heard it while you were at a terrific party, having a great, philosophical conversation with a friend. Thus, the song became part of that whole complex experience. Years later, even though you haven’t thought about that party in ages, when you hear the song on the radio, the whole experience rushes back to you. In general, the encoding specificity principle states that, to the extent a retrieval cue (the song) matches or overlaps the memory trace of an experience (the party, the conversation), it will be effective in evoking the memory. One example of the encoding specificity principle is transfer-appropriate processing, in which memory is best when the type of cognitive processing at recall matches the type of cognitive processing at encoding. This was empirically shown in a study by Morris et al. (1977) using semantic and rhyme tasks. In a standard recognition test, memory was better following semantic processing compared to rhyme processing, as predicted by the levels-of-processing effect. However, in a rhyming recognition test, memory was better for those who engaged in rhyme processing compared to semantic processing. This adds a level of complexity to the levels-of-processing theory: while the levels-of-processing framework generally holds for a normal recognition test, performance on rhyming tests is actually better with phonological than semantic processing at encoding. Figure 5.6: Results from Morris and colleagues’ Transfer-Appropriate Processing experiment (Morris et al., 1977, Experiment 1). Other facets of the encoding specificity principle include context-dependent memory. Context-dependent learning refers to an increase in retrieval when the external situation in which information is learned matches the situation in which it is remembered. Godden &amp; Baddeley (1975) conducted a study to test this idea using scuba divers. They asked the divers to learn a list of words either when they were on land or when they were underwater. Then they tested the divers on their memory, either in the same or the opposite situation. The divers’ memory was better when they were tested in the same context in which they had learned the words than when they were tested in the other context. In this instance, the physical context itself provided cues for retrieval. Whereas context-dependent memory refers to a match in the external situation between learning and remembering, state-dependent memory refers to superior retrieval of memories when the individual is in the same physiological or psychological state as during encoding. Research has found, for instance, that animals that learn a maze while under the influence of one drug tend to remember their learning better when they are tested under the influence of the same drug than when they are tested without the drug (Jackson et al., 1992). Research with humans finds that bilinguals remember better when tested in the same language in which they learned the material (Marian &amp; Kaushanskaya, 2007). Mood states may also produce state-dependent learning. People who learn information when they are in a bad (rather than a good) mood find it easier to recall these memories when they are tested while they are in a bad mood, and vice versa. It is easier to recall unpleasant memories than pleasant ones when we’re sad, and easier to recall pleasant memories than unpleasant ones when we’re happy (Bower, 1981). Consolidation Memory consolidation is a category of processes that stabilize a memory after its initial acquisition. Reduction of Interference It is sometimes incorrectly stated that consolidation causes more learning. While this would be great if it were true (imagine learning more in our sleep!) this is a mischaracterization of what is actually occurring. It is more accurate to say that consolidation reduces the amount of interference from other memories when we go to retrieve a memory. Imagine a scenario where you study for class A at the beginning of the day and then go to class B and C that same day. All of the information from classes B and C have the potential to interfere with the retrieval of the information that you studied for class A. Consolidation reduces this problem. This can be illustrated from a study done by Gais et al. (2006). Participants in this study first studied English-German word pairs. Participants in one condition (the sleep condition) went to sleep within three hours of learning the pairs. Participants in the other condition (the awake condition) stayed awake for about 10 hours before sleeping. The next day, all groups were tested on their memory for those words. The researchers found that the sleep group forgot less material than the awake group. According to the researchers, this difference can be explained by a reduction in interference: the awake group learned a whole bunch of new information during the day, which had the potential to interfere with the previously learned information. The sleep group on the other hand, had little new information before sleep and also benefited from more immediate consolidation. Sleep Consolidation Deep sleep, or slow wave sleep (SWS), is associated with the consolidation of memories. Rasch et al. (2007) showed evidence for this by pairing items participants learned with a rose odor. During sleep, the researchers re-exposed participants to the same rose odor during different stages of sleep in order to reactivated the memories associated with the odor. They found that only re-exposure to the rose odor during deep sleep increased retention for declarative learning. This research approach is called targeted memory reactivation (see box). Importantly, the same result was not found for procedural memories. Researchers following this line of work have come to assume that dreams are a byproduct of the reactivation of brain areas during sleep. The dream experience itself is not what enhances memory performance but rather it is the reactivation of the neural circuits that causes this. Targeted Memory Reactivation Although sleep by itself does not cause more learning to occur, researchers can cause memories to be reactivated during sleep by exposing sleeping participants to odors or sounds associated with specific memories. This has been found in a variety of types of learning (Hu et al., 2020), though for some memory types, including some types of procedural learning, this reactivation does not seem to promote reduction in interference (Rasch et al., 2007). In general, targeted-memory-reactivation (TMR) has two steps. First participants learn information, which the researchers pair with another stimulus (for example, pairing a word pair with a certain smell). The participants then sleep while wearing sensors that detect their brain activity. When the researchers detect that the participant is in the desired stage of sleep, the paired stimulus is presented. This reactivates the memory that was originally learned while they were awake. This approach has largely been used to study possible mechanisms of consolidation. Some evidence currently supports the theory that memories are replayed while one sleeps. This promotes memory consolidation and leads to memory stabilization. With TMR, we can artificially induce this replay and examine possible effects that consolidation can have on memories. Further, researchers are also now beginning to examine sleep quality and its relation to TMR. Whitmore et al. (2022) used TMR to enhance face-name pairs. They replicated previous findings that TMR only works during deep sleep and that this effect is moderated by the amount of time spent in deep sleep Key Takeaways Memory refers to the ability to store and retrieve information over time. For some things our memory is very good, but our active cognitive processing of information ensures that memory is never an exact replica of what we have experienced. Explicit memory refers to experiences that can be intentionally and consciously remembered, and it is measured using recall, recognition, and relearning. Explicit memory includes episodic and semantic memories. Implicit memory refers to the influence of experience on behaviour, even if the individual is not aware of those influences. The three types of implicit memory are procedural memory, classical conditioning, and priming. The capacity of long-term memory is large, and there is no known limit to what we can remember. Information is better remembered when it is meaningfully elaborated. Context- and state-dependent learning, as well as primacy and recency effects, influence long-term memory. Exercises Plan a course of action to help you study for your next exam, incorporating as many of the techniques mentioned in this section as possible. Try to implement the plan. In the film Eternal Sunshine of the Spotless Mind, the characters undergo a medical procedure designed to erase their memories of a painful romantic relationship. Would you engage in such a procedure if it were safely offered to you? 5.4 Glossary classical conditioning effects A type of implicit memory in which we learn, often without effort or awareness, to associate neutral stimuli with another stimulus, which creates a naturally occurring response context-dependent learning an increase in retrieval when the external situation in which information is learned matches the situation in which it is remembered episodic memory explicit recollection of firsthand experiences levels-of-processing effect memory recall of stimuli as a function of the depth of mental processing at encoding orthographic processing processing related to how a word is written phonological processing processing related to how a word sounds primacy effect a tendency to better remember stimuli that are presented early in a list priming changes in behavior as a result of experiences that have happened frequently or recently procedural memory implicit knowledge of how to do things recall memory test a measure of explicit memory that involves bringing from memory information that has previously been remembered recency effect the tendency to better remember stimuli that are presented at the end of a list recognition memory test a measure of explicit memory that involves determining whether information has been seen or learned before relearning a memory measurement that assesses how much more quickly information is processed or learned when it is studied again after it has already been learned but then forgotten self-reference effect processing words in terms of their relation to yourself promotes an even higher rate of recall than normal semantic processing semantic processing processing related to a words’ meaning serial position curve a graphic depiction of the likelihood of remembering items from a list based on the order in which they were presented state-dependent memory superior retrieval of memories when the individual is in the same physiological or psychological state as during encoding transfer-appropriate processing memory is best when the type of cognitive processing at recall matches the type of cognitive processing at encoding References Abbot, E. E. (1909). On the analysis of the memory consciousness in orthography. Psychol. Monogr., 11(1), 127–158. Atkinson, R. C., &amp; Shiffrin, R. M. (1968). Human memory: A proposed system and its control processes. In Psychology of learning and motivation (Vol. 2, pp. 89–195). Elsevier. Baddeley, A. D. (1990). The development of the concept of working memory: Implications and contributions of neuropsychology. In G. Vallar &amp; T. Shallice (Eds.), Neuropsychological impairments of Short-Term memory (pp. 54–73). Cambridge University Press. Benjamin, A. S., &amp; Tullis, J. (2010). What makes distributed practice effective? Cognitive Psychology, 61, 228–247. https://doi.org/10.1016/j.cogpsych.2010.05.004.What Bower, G. H. (1981). Mood and memory. American Psychologist, 36(2), 129. Bridgeman, B., &amp; Morgan, R. (1996). Success in college for students with discrepancies between performance on multiple-choice and essay tests. J. Educ. Psychol., 88(2), 333–340. Cepeda, N. J., Vul, E., Rohrer, D., Wixted, J. T., &amp; Pashler, H. (2008). Spacing effects in learning: A temporal ridgeline of optimal retention. Psychological Science, 19, 1095–1102. https://doi.org/10.1111/j.1467-9280.2008.02209.x Craik, F. I. M., &amp; Lockhart, R. S. (1972). Levels of processing: A framework for memory research. Journal of Verbal Learning and Verbal Behavior, 11(6), 671–684. Craik, F. I. M., &amp; Tulving, E. (1975). Depth of processing and the retention of words in episodic memory. J. Exp. Psychol. Gen., 104(3), 268–294. Gais, S., Lucas, B., &amp; Born, J. (2006). Sleep after learning aids memory recall. Learning &amp; Memory, 13(3), 259–262. Godden, D. R., &amp; Baddeley, A. D. (1975). Context-dependent memory in two natural environments: On land and underwater. Br. J. Psychol., 66(3), 325–331. Gupta, M. W., Pan, S. C., &amp; Rickard, T. C. (2022). Prior episodic learning and the efficacy of retrieval practice. Memory &amp; Cognition, 50(4), 722–735. Haist, F., Shimamura, A. P., &amp; Squire, L. R. (1992). On the relationship between recall and recognition memory. J. Exp. Psychol. Learn. Mem. Cogn., 18(4), 691–702. Hu, X., Cheng, L. Y., Chiu, M. H., &amp; Paller, K. A. (2020). Promoting memory consolidation during sleep: A meta-analysis of targeted memory reactivation. Psychol. Bull., 146(3), 218–244. Jackson, A., Koek, W., &amp; Colpaert, F. C. (1992). NMDA antagonists make learning and recall state-dependent. Behav. Pharmacol., 3(4), 415–421. Karpicke, J. D., Lehman, M., &amp; Aue, W. R. (2014). Retrieval-based learning. An episodic context account. In Psychology of Learning and Motivation - Advances in Research and Theory (Vol. 61, pp. 237–284). https://doi.org/10.1016/B978-0-12-800283-4.00007-1 Kromann, C. B., Jensen, M. L., &amp; Ringsted, C. (2009). The effect of testing on skills learning. Medical Education, 43, 21–27. https://doi.org/10.1111/j.1365-2923.2008.03245.x Lantz, M. E. (2010). The use of ’clickers’ in the classroom: Teaching innovation or merely an amusing novelty? Computers in Human Behavior, 26, 556–561. https://doi.org/10.1016/j.chb.2010.02.014 Larsen, D. P., Butler, A. C., &amp; Roediger, H. L. (2009). Repeated testing improves long-term retention relative to repeated study: A randomised controlled trial. Medical Education, 43, 1174–1181. https://doi.org/10.1111/j.1365-2923.2009.03518.x Lohnas, L. J., &amp; Kahana, M. J. (2014). A retrieved context account of spacing and repetition effects in free recall. J Exp Psychol Learn Mem Cogn, 40, 755–764. https://doi.org/10.1038/jid.2014.371 Marian, V., &amp; Kaushanskaya, M. (2007). Language context guides memory content. Psychon. Bull. Rev., 14(5), 925–933. McDaniel, M. A., Anderson, J. L., Derbish, M. H., &amp; Morrisette, N. (2007). Testing the testing effect in the classroom. European Journal of Cognitive Psychology, 19, 494–513. https://doi.org/10.1080/09541440701326154 Morris, C. D., Bransford, J. D., &amp; Franks, J. J. (1977). Levels of processing versus transfer appropriate processing. J. Verbal Learning Verbal Behav., 16(5), 519–533. Mozer, M. C., Pashler, H., Cepeda, N., Lindsey, R., &amp; Vul, E. (2009). Predicting the optimal spacing of study: A multiscale context model of memory. Advances in Neural Information Processing Systems 22 - Proceedings of the 2009 Conference, 1321–1330. Nungester, R. J., &amp; Duchastel, P. C. (1982). Testing versus review: Effects on retention. Journal of Educational Psychology, 74, 18–22. https://doi.org/10.1037/0022-0663.74.1.18 Pan, S. C., Rubin, B. R., &amp; Rickard, T. C. (2015). Does testing with feedback improve adult spelling skills relative to copying and reading? Journal of Experimental Psychology: Applied, 21, 356–369. https://doi.org/10.1037/xap0000062 Polyn, S. M., Norman, K. A., &amp; Kahana, M. J. (2009). A context maintenance and retrieval model of organization. Psychology Review, 116, 129–156. https://doi.org/10.1037/a0014420.A Rasch, B., Büchel, C., Gais, S., &amp; Born, J. (2007). Odor cues during slow-wave sleep prompt declarative memory consolidation. Science, 315(5817), 1426–1429. Rickard, T. C., &amp; Pan, S. C. (2018). A dual memory theory of the testing effect. Psychonomic Bulletin and Review, 25, 847–869. https://doi.org/10.3758/s13423-017-1298-4 Rogers, T. B., Kuiper, N. A., &amp; Kirker, W. S. (1977). Self-reference and the encoding of personal information. J. Pers. Soc. Psychol., 35(9), 677–688. Whitmore, N. W., Bassard, A. M., &amp; Paller, K. A. (2022). Targeted memory reactivation of face-name learning depends on ample and undisturbed slow-wave sleep. NPJ Sci. Learn., 7(1), 1. "],["memory-in-context.html", "Chapter 6 Memory in Context 6.1 Kinds of Memory Biases 6.2 Memory in the Eyewitness Domain 6.3 Forgetting 6.4 Glossary", " Chapter 6 Memory in Context As we have seen, our memories are not perfect. They fail in part due to our inadequate encoding and storage, and in part due to our inability to accurately retrieve stored information. But memory is also influenced by the setting in which it occurs, by the events that occur to us after we have experienced an event, and by the cognitive processes that we use to help us remember. Although our cognition allows us to attend to, rehearse, and organize information, cognition may also lead to distortions and errors in our judgments and our behaviors. LEARNING OBJECTIVES Explain some of the errors that are common in human memory. Describe some of the important research that has demonstrated human memory errors and their consequences. Explain how schemas can distort our memories. Describe the conditions that affect accuracy and inaccuracy in eyewitness memory. 6.1 Kinds of Memory Biases Memory is susceptible to a wide variety of biases and errors. People can forget events that happened to them and people they once knew. They can mix up details across time and place. They can even remember whole complex events that never happened at all. Importantly, these errors, once made, can be very hard to unmake. A memory is no less “memorable” just because it is wrong. Some small memory errors are commonplace, and you have no doubt experienced many of them. You set down your keys without paying attention, and then cannot find them later when you go to look for them. You try to come up with a person’s name but cannot find it, even though you have the sense that it is right at the tip of your tongue (psychologists actually call this the tip-of-the-tongue effect, or TOT) (A. S. Brown, 1991). Other sorts of memory biases are more complicated and longer lasting. For example, it turns out that our expectations and beliefs about how the world works can have huge influences on our memories. Because many aspects of our everyday lives are full of redundancies, our memory systems take advantage of the recurring patterns by forming and using schemas, or memory templates (Alba &amp; Hasher, 1983; Brewer &amp; Treyens, 1981). Thus, we know to expect that a library will have shelves and tables and librarians, and so we don’t have to spend energy noticing these at the time. The result of this lack of attention, however, is that one is likely to remember schema-consistent information (such as tables), and to remember them in a rather generic way, whether or not they were actually present. Figure 6.1: For most of our experiences schematas are a benefit and help with information overload. However, they may make it difficult or impossible to recall certain details of a situation later. Do you recall the library as it actually was or the library as approximated by your library schemata? Dan Kleinman, CC BY 2.0 Schematic Processing: Distortions Based on Expectations We have seen that schemas help us remember information by organizing material into coherent representations. However, although schemas can improve our memories by helping us make sense of the world, they may also lead to cognitive biases. Using schemas may lead us to falsely remember things that never happened to us and to distort or misremember things that did. For one, schemas can lead us to pay attention to and search for information that fits our schemas—leading to something called confirmation bias. Confirmation bias is the tendency to verify and confirm our existing memories rather than to challenge and disconfirm them. The confirmation bias occurs because once we have schemas, they influence how we seek out and interpret new information. People tend to remember information that fits our schemas better than we remember information that disconfirms them (Stangor &amp; McMillan, 1992), a process that makes our views of the world very difficult to change. Furthermore, we ask questions in ways that confirm our schemas (Trope &amp; Thompson, 1997). If we think that a person is an extrovert, we might ask her about ways that she likes to have fun, thereby making it more likely that we will confirm our beliefs. In short, once we begin to believe in something — for instance, a stereotype about a group of people — it becomes very difficult to later convince us that these beliefs are not true; the beliefs become self-confirming. Darley &amp; Gross (1983b) demonstrated how schemas about social class could influence memory. In their research they gave participants a picture and some information about a Grade 4 girl named Hannah. To activate a schema about her social class, Hannah was pictured sitting in front of a nice suburban house for one-half of the participants and pictured in front of an impoverished house in an urban area for the other half. Then the participants watched a video that showed Hannah taking an intelligence test. As the test went on, Hannah got some of the questions right and some of them wrong, but the number of correct and incorrect answers was the same in both conditions. Then the participants were asked to remember how many questions Hannah got right and wrong. Demonstrating that stereotypes had influenced memory, the participants who thought that Hannah had come from an upper-class background remembered that she had gotten more correct answers than those who thought she was from a lower-class background. Our reliance on schemas can also make it more difficult for us to “think outside the box.” Wason (1960) asked undergraduate students to determine the rule that was used to generate the numbers 2-4-6 by asking them to generate possible sequences and then telling them if those numbers followed the rule. The first guess that students made was usually “consecutive ascending even numbers,” and they then asked questions designed to confirm their hypothesis (“Does 102-104-106 fit?” “What about 404-406-408?”). Upon receiving information that those guesses did fit the rule, the students stated that the rule was “consecutive ascending even numbers.” But the students’ use of the confirmation bias led them to ask only about instances that confirmed their hypothesis, and not about those that would disconfirm it. They never bothered to ask whether 1-2-3 or 3-11-200 would fit, and if they had they would have learned that the rule was not “consecutive ascending even numbers,” but simply “any three ascending numbers.” Again, you can see that once we have a schema (in this case a hypothesis), we continually retrieve that schema from memory rather than other relevant ones, leading us to act in ways that tend to confirm our beliefs. Source Monitoring One potential error in memory involves mistakes in differentiating the sources of information. Source monitoring refers to the ability to accurately identify the source of a memory. Perhaps you’ve had the experience of wondering whether you really experienced an event or only dreamed or imagined it. If so, you wouldn’t be alone. Rassin et al. (2001) reported that up to 25% of undergraduate students reported being confused about real versus dreamed events. Studies suggest that people who are fantasy-prone are more likely to experience source monitoring errors (Winograd et al., 1998), and such errors also occur more often for both children and the elderly than for adolescents and younger adults (Jacoby &amp; Rhodes, 2006). In other cases we may be sure that we remembered the information from real life but be uncertain about exactly where we heard it. Imagine that you read a news story in a tabloid magazine such as the National Enquirer. You would probably discounted the information within that story because you know that its source is unreliable. But what if later you were to remember the story but forget the source of the information? If this happens, you might become convinced that the news story is true because you forget to discount it. The sleeper effect refers to attitude change that occurs over time when we forget the source of information (Pratkanis et al., 1988). In still other cases we may forget where we learned information and mistakenly assume that we created the memory ourselves. Canadian authors Wayson Choy, Sky Lee, and Paul Yee launched a $6 million copyright infringement lawsuit against the parent company of Penguin Group Canada, claiming that the novel Gold Mountain Blues contained “substantial elements” of certain works by the plaintiffs (Cbc.ca, 2011). The suit was filed against Pearson Canada Inc., author Ling Zhang, and the novel’s U.K.-based translator Nicky Harman. Zhang claimed that the book shared a few general plot similarities with the other works but that those similarities reflect common events and experiences in the Chinese immigrant community. She argued that the novel was “the result of years of research and several field trips to China and Western Canada,” and that she had not read the other works. Nothing was proven in court. Finally, the musician George Harrison claimed that he was unaware that the melody of his song My Sweet Lord was almost identical to an earlier song by another composer. The judge in the copyright suit that followed ruled that Harrison did not intentionally commit the plagiarism. (Please use this knowledge to become extra vigilant about source attributions in your written work, not to try to excuse yourself if you are accused of plagiarism.) Memory Contamination Misinformation effect We know that schemas help us make sense of the world by filling in the gaps in our memory with information that makes sense given a particular context. This knowledge lead researchers to wonder whether these blanks could be filled in incorrectly when presented with other information. In an early study, undergraduate subjects first watched a slideshow depicting a small red car driving and then hitting a pedestrian (Loftus et al., 1978). Some subjects were then asked leading questions about what had happened in the slides. For example, subjects were asked, “How fast was the car traveling when it passed the yield sign?” But this question was actually designed to be misleading, because the original slide included a stop sign rather than a yield sign. Later, subjects were shown pairs of slides. One of the pair was the original slide containing the stop sign; the other was a replacement slide containing a yield sign. Subjects were asked which of the pair they had previously seen. Subjects who had been asked about the yield sign were likely to pick the slide showing the yield sign, even though they had originally seen the slide with the stop sign. In other words, the misinformation in the leading question led to inaccurate memory. This phenomenon is called the misinformation effect, because the misinformation that subjects were exposed to after the event (here in the form of a misleading question) apparently contaminates subjects’ memories of what they witnessed. Hundreds of subsequent studies have demonstrated that memory can be contaminated by erroneous information that people are exposed to after they witness an event (see Frenda et al., 2011; Loftus, 2005). The misinformation in these studies has led people to incorrectly remember everything from small but crucial details of a perpetrator’s appearance to objects as large as a barn that wasn’t there at all. Figure 6.2: Misinformation can be introduced into the memory of a witness between the time of seeing an event and reporting it later. Something as straightforward as which sort of traffic sign was in place at an intersection can be confused if subjects are exposed to erroneous information after the initial incident. Original photo by Kevin Lee on Unsplash; edited by C. Pilegard. These studies have demonstrated that young adults (the typical research subjects in psychology) are often susceptible to misinformation, but that children and older adults can be even more susceptible (Bartlett &amp; Memon, 2007; Ceci &amp; Bruck, 1995). In addition, misinformation effects can occur easily, and without any intention to deceive (Allan &amp; Gabbert, 2008). Even slight differences in the wording of a question can lead to misinformation effects. Subjects in one study were more likely to say yes when asked “Did you see the broken headlight?” than when asked “Did you see a broken headlight?” (Loftus, 1975). Other studies have shown that misinformation can corrupt memory even more easily when it is encountered in social situations (Gabbert et al., 2004). This is a problem particularly in cases where more than one person witnesses a crime. In these cases, witnesses tend to talk to one another in the immediate aftermath of the crime, including as they wait for police to arrive. But because different witnesses are different people with different perspectives, they are likely to see or notice different things, and thus remember different things, even when they witness the same event. So when they communicate about the crime later, they not only reinforce common memories for the event, they also contaminate each other’s memories for the event (Gabbert et al., 2003; Paterson &amp; Kemp, 2006; Takarangi et al., 2006). The misinformation effect has been modeled in the laboratory. Researchers had subjects watch a video in pairs. Both subjects sat in front of the same screen, but because they wore differently polarized glasses, they saw two different versions of a video, projected onto a screen. So, although they were both watching the same screen, and believed (quite reasonably) that they were watching the same video, they were actually watching two different versions of the video (Garry et al., 2008). In the video, Eric the electrician is seen wandering through an unoccupied house and helping himself to the contents thereof. A total of eight details were different between the two videos. After watching the videos, the “co-witnesses” worked together on 12 memory test questions. Four of these questions dealt with details that were different in the two versions of the video, so subjects had the chance to influence one another. Then subjects worked individually on 20 additional memory test questions. Eight of these were for details that were different in the two videos. Subjects’ accuracy was highly dependent on whether they had discussed the details previously. Their accuracy for items they had not previously discussed with their co-witness was 79%. But for items that they had discussed, their accuracy dropped markedly, to 34%. That is, subjects allowed their co-witnesses to corrupt their memories for what they had seen. False Memories Some memory errors are so “large” that they almost belong in a class of their own: false memories. Back in the early 1990s, a pattern emerged whereby people would go into therapy for depression and other everyday problems, but over the course of the therapy develop memories for violent and horrible victimhood, typically of sexual abuse (Loftus &amp; Ketcham, 1996). The widespread phenomena later became known as the “repressed memory epidemic.” During this time, patients’ therapists claimed that the patients were recovering genuine memories of real childhood abuse, buried deep in their minds for years or even decades. However, some experimental psychologists believed that methods being used in therapy were creating the perfect recipe for memory contamination. These researchers suspected that many of these memories were likely to be false—unintentionally created in therapy—and that these traumatic memories of victimization were for events that never happened. These researchers then set out to see whether it would indeed be possible for wholly false memories to be created by procedures similar to those used in these patients’ therapy. In early research into false memory, a typical study design was to recruit the families of undergraduates or other lab members. In one hallmark false memory study, undergraduate subjects’ family members were recruited to provide events from the students’ lives (Loftus &amp; Pickrell, 1995). The student subjects were told that the researchers had talked to their family members and learned about four different events from their childhoods. The researchers asked if the now undergraduate students remembered each of these four events—introduced via short hints. The subjects were asked to write about each of the four events in a booklet and then were interviewed two separate times. The trick was that one of the events came from the researchers rather than the family (and the family had actually assured the researchers that this event had not happened to the subject). In the first such study, this researcher-introduced event was a story about being lost in a shopping mall and rescued by an older adult. In this study, after just being asked whether they remembered these events occurring on three separate occasions, a quarter of subjects came to believe that they had indeed been lost in the mall. Similar procedures were used to get subjects to believe that they nearly drowned and had been rescued by a lifeguard, or that they had spilled punch on the bride’s parents at a family wedding, or that they had been attacked by a vicious animal as a child, among other events (Heaps &amp; Nash, 1999; Hyman et al., 1995; Porter et al., 1999). More recent false memory studies have used a variety of different manipulations to produce false memories in substantial minorities and even occasional majorities of manipulated subjects (Braun et al., 2002; Lindsay et al., 2004; Mazzoni et al., 1999; Seamon et al., 2006; Wade et al., 2002). For example, one group of researchers used a mock-advertising study, wherein subjects were asked to review (fake) advertisements for Disney vacations, to convince subjects that they had once met the character Bugs Bunny at Disneyland—an impossible false memory because Bugs is a Warner Brothers character (Braun et al., 2002). Other researchers gave subjects unmanipulated class photographs from their childhoods along with a fake story about a class prank, and thus enhanced the likelihood that subjects would falsely remember the prank (Lindsay et al., 2004). Using a false feedback manipulation, we have been able to persuade subjects to falsely remember having a variety of childhood experiences. In these studies, subjects are told (falsely) that a powerful computer system has analyzed questionnaires that they completed previously and has concluded that they had a particular experience years earlier. Subjects apparently believe what the computer says about them and adjust their memories to match this new information. A variety of different false memories have been implanted in this way. In some studies, subjects are told they once got sick on a particular food (Bernstein et al., 2005). These memories can then spill out into other aspects of subjects’ lives, such that they often become less interested in eating that food in the future (Bernstein &amp; Loftus, 2009b). Other false memories implanted with this methodology include having an unpleasant experience with the character Pluto at Disneyland and witnessing physical violence between one’s parents (Berkowitz et al., 2008; Laney &amp; Loftus, 2008). False Memories and Confidence One important caveat to these studies is that this research often doesn’t report (or didn’t collect) the confidence of the participants over time in regard to their degree of certainty about the fake events having taken place. As a result, we do not have much data reflecting how belief in these false memories grew with time. Did participants believe in these false memories immediately, or did it take a long time? Did these memory implant suddenly or were bits of information implanted gradually? When participants did report these false memories, were their reports qualified with statements like, “I’m not sure, but it is possible that happened,” or were they immediately confident that the false memory was true? The available evidence suggests that participants were not immediately highly confident while their memory was in the earlier stages of contamination. In fact, the opposite seems to be true: Participants start with reporting that they don’t remember certain events, and then over time and through repeated pushing by researchers during testing, their memories change. For example, one group of researchers altered childhood photographs of their subjects to appear that they rode in a hot air balloon and then asked the subjects to try to remember and describe their hot air balloon experience (Wade et al., 2002). After the memory test, researchers would prompt them to imagine events as if they were true, even though the participants had no memories for the events themselves. Then, participants would have another memory test. Transcripts from this study suggest that confidence in these false memories grew over time as more contamination occurred and that, at the beginning, participants did not remember these events at all. Here is a sample transcript of the first memory test between researcher and subject in Wade et al. (2002): Interviewer: “And again, if you want to tell me as much as you can recall about this event without leaving anything out.” Subject: “Mm … no, never actually thought I’d been in a hot air balloon, so there we go.” Interviewer: “You can’t remember anything about this event?” Subject: “Nah. Though [this photo] is me … no memory whatsoever.” Interviewer: “If you want to take the next few minutes and concentrate on getting a memory back, something about the event.” Subject: “No, yeah I honestly … no I can’t. That’s really annoying.” (p. 600) After that exchange, researchers would then employ the visualization techniques that were used by therapists. By the third session, confidence seems to have grown: Interviewer: “Same again, tell me everything you can remember about Event 3 without leaving anything out.” Subject: “Um, just trying to work out how old my sister was; trying to get the exact … when it happened. But I’m still pretty certain it occurred when I was in form one (6th grade) at um the local school there … Um basically for $10 or something you could go up in a hot air balloon and go up about 20 odd meters … it would have been a Saturday and I think we went with, yeah, my parents and, no it wasn’t, not my grandmother … not certain who any of the other people are there. Um, and I’m pretty certain that mum is down on the ground taking a photo.” (p. 600) Some researchers speculate that this confidence effect should be considered regarding the repressed memory phenomenon of the 1990s as well; for example, patients’ false memories may not have appeared suddenly with high confidence, but rather could have been the results of interviewers pursuing low-certainty ideas until they became false memories. One patient from that time retracted the accusations of sexual abuse she made against her family. In her memoir, she implied that she was never confident about the instances of abuse having took place. Specifically, she wrote, “I drove back across San Francisco Bay [in 1989], back to Planet Incest, where the question was always incest and the answer was always incest and the explanation was always incest, and no one ever asked, ‘Are you sure?’” (Maran, 2010, p. 126). Importantly, once these false memories are implanted—whether through complex methods or simple ones—it is extremely difficult to tell them apart from true memories (Bernstein &amp; Loftus, 2009a; Laney &amp; Loftus, 2008). 6.2 Memory in the Eyewitness Domain An eyewitness is a person who has witnessed a crime either as a victim or as a bystander. After a crime takes place, police will interview eyewitnesses to gather information. Eyewitnesses are asked to remember details of the crime (e.g., what actions took place, the sequence of those actions, etc.) as well as remember the face of the perpetrator along with any other identifying features the perpetrator may possess. Law enforcement uses this information to come up with a suspect and, once they have a suspect, they often administer a lineup to the witness and ask them to make an identification. In most jurisdictions in the United States, lineups are typically conducted with pictures, called photo spreads, rather than with actual people standing behind one-way glass (Wells et al., 2006). The eyewitness is given a set of small pictures of (typically) six or eight individuals who are dressed similarly, photographed in similar circumstances, and all match the original description that the eyewitness gave police soon-after the crime took place. If the lineup is constructed properly, one of these individuals in the lineup will be the police suspect, and the remainder are “foils” or “fillers.” If the eyewitness identifies the suspect, then the investigation of that suspect is likely to progress. However, suspects can be guilty or innocent. A witness selecting an innocent suspect can lead to a wrongful conviction. Selecting a filler photo will have no negative repercussions as fillers are people known to be innocent of the particular crime that is under investigation. Figure 6.3: Most lineups in the United States are conducted with photo spreads. With the proliferation of DNA testing, many past criminal cases were reexamined with the people who were convicted now being found to be innocent instead. About 70% of these 375 cases in which convictions were overturned involved eyewitness misidentification (Project, 2023). These statistics paired with laboratory-based lineup studies and the knowledge coming from false-memory research painted a very convincing picture that eyewitnesses not only get it wrong frequently, but they don’t even know when they’re getting wrong; namely, that a witness’s confidence is not related to their accuracy (Caputo &amp; Dunning, 2007; Cutler &amp; Penrod, 1995). This view proliferated to the degree that it made it into court rulings at the federal level and at the level of state supreme courts (State v. Guilbert, 2012; United States v. Greene, 2013). In the decades-long quest to better understand eyewitness reliability, it was discovered that past research did not use the optimal method of analysis when empirically testing the relationship between eyewitness confidence and their accuracy. When the past studies that found little relationship between the two were reanalyzed, they now showed that confidence is strongly predictive of accuracy on the first lineup test given to an eyewitness (Wixted &amp; Wells, 2017). A strong confidence-accuracy relationship indicates that witnesses who are highly confident on the first lineup test show high degrees of accuracy (i.e., the person they selected is highly likely to be the guilty perpetrator), and witnesses who have low confidence in their selection have lower degrees of accuracy (i.e., the person they selected is less likely to be the guilty perpetrator). Witnesses tend to know when they are more likely to make an error, and that knowledge is generally reflected in their confidence scores. It’s worth noting that most of the conversation about eyewitness reliability has been focused on facial recognition because misidentifications (i.e., recognizing an innocent suspect as the perpetrator) are largely what lead to the incarceration of innocent people. However, there is another type of memory test that is different from recognition, called recall. Recall refers to a witness searching their memory to retrieve relevant information. Most of the studies investigating false memories are experiments that used recall tasks to investigate memory. When police ask witnesses to describe the crime or the perpetrator’s appearance, that is also recall. Similar to studies on recognition memory, recall studies also show a strong confidence-accuracy relationship as well (Wixted et al., 2018). See Table 6.1 for the key distinctions between recall and recognition. Table 6.1: Recall vs. Recognition Recall Recognition Description Calling to mind something that is not present in front of them. Deciding whether a currently-presented item was previously encountered Example tasks “What did you see?” “Tell me about what happened.” “Did the perpetrator have a weapon?” “Have you seen this face before?” “Is this the car that drove away?” “Do you recognize this person?” “Is this the voice you heard?” Automaticity Recall is a search process; it requires you to actively search your memory. Recognition—especially face recognition—is typically an automatic process. Because the item you are considering is physically present (e.g., a photo of a suspect’s face), there is no need to search memory. Instead, your brain yields a familiarity signal quickly and automatically. Effect of time Generally, the passage of time makes it harder to recall information as more forgetting occurs. However, it is possible to initially fail at remembering something and then successfully remember it later. (Imagine all the times that you’ve seen someone and thought, “I wanted to tell you something, but I forgot what it was.” Then, as you’re going to bed, you suddenly remember what you wanted to say.) On a typical recognition test, after a few seconds of evaluating a stimulus, if it does not seem familiar, spending more time searching memory will not cause it to become more familiar. Because familiarity is an automatic process, you either experience the familiarity signal in a matter of seconds or you don’t. There are a lot of situational factors that can affect a witness’ memory. These factors include poor vision or viewing conditions during the crime, particularly stressful witnessing experiences, too little time to view the perpetrator or perpetrators, too much delay between witnessing and identifying, and being asked to identify a perpetrator from a race other than one’s own (Bornstein et al., 2012; Brigham et al., 2007; Burton et al., 1999; Deffenbacher et al., 2004). However, the presence of these factors does not necessarily make eyewitnesses more likely to misidentify an innocent person as a perpetrator. In other words, these factors do not implant false memories. Instead, they make it harder for the face of the perpetrator to enter the brain in the first place, increasing the likelihood that an eyewitness will fail to recognize a guilty person even if they are in the lineup (Wixted &amp; Wells, 2017). There is a substantial body of research demonstrating that eyewitnesses can make serious, but often understandable and even predictable, errors. However, there is an increasingly influential body of research indicating that, while those errors happen, they generally occur long after memory has already been contaminated. In fact, high-confidence errors on the first lineup test are scarce (Yilmaz et al., 2023), and a reanalysis of past research shows that eyewitnesses are reliable on the first, properly conducted, memory test (Wixted et al., 2018; Wixted &amp; Wells, 2017). People will often hear the words “eyewitness memory” and “eyewitness testimony” used interchangeably, but they are different. Eyewitness testimony refers to statements made in the courtroom weeks, months or years after an event occurred. And, as shown by decades of past research, memory becomes increasingly contaminated with the passage of time. The memory of a witness in court has long-been contaminated, so their testimony may not be accurate no matter how high their confidence might be. However, the reports that an eyewitness provides soon after a crime takes place are much more likely to be reliable, and law enforcement can use a witness’ confidence at that time as a gauge of their accuracy. There are many things that the justice system can do to help lineup identifications “go right.” For example, investigators can put together high-quality, fair lineups (Wells et al., 1998, 2020). A fair lineup is one in which the suspect and each of the fillers is equally likely to be chosen by someone who has read an eyewitness description of the perpetrator but who did not actually witness the crime (Brigham et al., 1990). This means that no one in the lineup should “stand out” from the others for reasons other than they being the person from the witness’ memory, and everyone in the lineup should match the description given by the eyewitness at the very start of the investigation. Other important recommendations that have come out of this research include better ways to conduct lineups, “double blind” lineups, unbiased instructions for witnesses, video-recording the lineup procedure, and avoiding the use of show-up procedures (see Technical Working Group for Eyewitness Evidence, 1999; Wells et al., 1998; Wells &amp; Olson, 2003). The most important recommendation is that a witness’ memory should only be tested once using the same suspect with the same witness (Wells et al., 2020; Wixted et al., 2021). Everything after the first lineup test is a test of contaminated memory. 6.3 Forgetting As you’ve come to see, memory is fragile, and forgetting can be frustrating and even embarrassing. But why do we forget? To answer this question, we will look at several perspectives on forgetting. Encoding Failure Sometimes memory errors happen before the actual memory process begins, which is called an encoding failure. Encoding failures refer to the times that the information never entered our brain to later become a memory. In these cases, we don’t say that we’ve “forgotten” a particular piece of information. Instead, we say that the information is “not available.” We can’t remember or forget something if we never stored it in our memory in the first place. This would be like trying to find a book on your e-reader that you never actually purchased and downloaded. Often, in order to remember something, we must pay attention to the details and actively work to process the information (effortful encoding). Lots of times we don’t do this. For instance, think of how many times in your life you’ve seen a penny. Can you accurately recall what the front of a U.S. penny looks like? When researchers Nickerson &amp; Adams (1979) asked this question, they found that most Americans don’t know which one it is. The reason is most likely encoding failure. Most of us never encode the details of the penny. We only encode enough information to be able to distinguish it from other coins. If we don’t encode the information, then it’s not in our long-term memory, so we will not be able to remember it. Figure 6.4: Can you tell which coin, (a), (b), (c), or (d) is the accurate depiction of a US nickel? The correct answer is (c). Interference Sometimes information is stored in our memory, but for some reason it is inaccessible. This is known as interference, and there are two types: proactive interference and retroactive interference. Have you ever gotten a new phone number or moved to a new address, but right after you tell people the old (and wrong) phone number or address? When the new year starts, do you find you accidentally write the previous year? These are examples of proactive interference: when old information hinders the recall of newly learned information. Retroactive interference happens when information learned more recently hinders the recall of older information. For example, this week you are studying about Freud’s Psychoanalytic Theory. Next week you study the humanistic perspective of Maslow and Rogers. Thereafter, you have trouble remembering Freud’s Psychosexual Stages of Development because you can only remember Maslow’s Hierarchy of Needs. Cue-Overload Principle Quick! Try to remember every experience you’ve ever had! That was a pretty hard task, right? That’s because the entirety of all our memories is not immediately accessible to us on command. Instead, memory is “cue-dependent,” which means we need something to trigger our ability to remember something (Tulving &amp; Thomson, 1973). The piece of information that helps us access a specific memory is called a “retrieval cue.” When others ask for more information to help “jog their memory,” in memory terms, it means they’re asking someone to provide them with a retrieval cue. Sometimes, however, that cue isn’t effective. One way the retrieval cue may fail is that it’s been overloaded—there are too many memories attached to that cue (Watkins &amp; Watkins, 1975). This weakens the cue (i.e., makes it less effective at pulling up a specific memory). For example, if I were to ask, “Do you remember your brother at Christmas,” the cue “brother at Christmas” may be ineffective at pulling up a specific memory because you (presumably) have many memories with siblings at Christmas. If I were to provide you a cue that wasn’t tied to a lot of memories (“Do you remember your brother during Christmas two years ago? When he started laughing while eating green beans?”), you would be better at immediately retrieving the memory from your brain (“Yes, I do remember! That was so funny! He laughed so hard that one green bean popped out of his nose!”). Flashbulb Memories You may have a clear memory of when you first heard about the 9/11 attacks in the United States in 2001, when the Chicago Cubs broke their 108-year-long championship title drought with their World Series win over Cleveland in 2016, or where you were when you learned of the January 6 insurrection on the United States Capitol in 2021. This type of memory, which we experience along with a great deal of emotion, is known as a flashbulb memory — a vivid and emotional memory of an unusual event that people believe they remember very well (R. Brown &amp; Kulik, 1977). People are very certain of their memories of these important events, and frequently overconfident. Talarico &amp; Rubin (2003) tested the accuracy of flashbulb memories by asking students to write down their memory of how they had heard the news about either the September 11, 2001, terrorist attacks or about an everyday event that had occurred to them during the same time frame. These recordings were made on September 12, 2001. Then the participants were asked again, either one, six, or 32 weeks later, to recall their memories. The participants became less accurate in their recollections of both the emotional event and the everyday events over time. Although the participants’ confidence in the accuracy of their memory of everyday events appropriately declined over time, confidence in the accuracy of their memory of learning about the attacks did not decline. The conclusion was that flashbulb memories are special only in that they are always confidently held despite their declining accuracy. Key Takeaways The human brain is wired to develop and make use of categories and schemas. Schemas help us remember new information but may also lead us to falsely remember things that never happened to us and to distort or misremember things that did. Eyewitness identification errors can lead to people being falsely accused and even convicted. Likewise, eyewitness memory can be corrupted by leading questions, misinterpretations of events, conversations with co-witnesses, and their own expectations for what should have happened. There are several reasons why forgetting occurs. One reason we forget is due to encoding failure: we can’t remember something if we never stored it in our memory in the first place. Other times, information is actually stored in our memory, but we cannot access it due to interference. Proactive interference happens when old information hinders the recall of newly learned information. Retroactive interference happens when information learned more recently hinders the recall of older information. Exercises Imagine that you are a juror in a murder case where an eyewitness testifies. In what ways might your knowledge of memory errors affect your use of this testimony? Consider some of the cognitive schemas that you hold in your memory. How do these knowledge structures bias your information processing and behaviour, and how might you prevent them from doing so? Describe a flashbulb memory of a significant event in your life. Compare your memory with someone else who was with you at the time. Are there differences? Why? 6.4 Glossary confirmation bias The tendency to verify and confirm our existing memories rather than to challenge and disconfirm them flashbulb memory Exceptionally vivid recollection of an important event forgetting loss of information from long-term memory misinformation effect A memory error caused by exposure to incorrect information between the original event (e.g., a crime) and later memory test (e.g., an interview, lineup, or day in court) proactive interference old information hinders the recall of newly learned information retroactive interference information learned more recently hinders the recall of older information schema A memory template, created through repeated exposure to a particular class of objects or events source monitoring The ability to accurately identify the source of a memory sleeper effect Attitude change that occurs over time when we forget the source of information References Alba, J. W., &amp; Hasher, L. (1983). Is memory schematic? Psychol. Bull., 93(2), 203–231. Allan, K., &amp; Gabbert, F. (2008). I still think it was a banana: Memorable “lies” and forgettable “truths.” Acta Psychologica, 127(2), 299–308. Bartlett, J. C., &amp; Memon, A. (2007). Eyewitness memory in young and older adults. Lawrence Erlbaum Mahwah, NJ. Berkowitz, S. R., Laney, C., Morris, E. K., Garry, M., &amp; Loftus, E. F. (2008). Pluto behaving badly: False beliefs and their consequences. Am. J. Psychol., 121(4), 643–660. Bernstein, D. M., Laney, C., Morris, E. K., &amp; Loftus, E. F. (2005). False memories about food can lead to food avoidance. Soc. Cogn., 23(1), 11–34. Bernstein, D. M., &amp; Loftus, E. F. (2009a). The consequences of false memories for food preferences and choices. Perspect. Psychol. Sci., 4(2), 135–139. Bernstein, D. M., &amp; Loftus, E. F. (2009b). How to tell if a particular memory is true or false. Perspect. Psychol. Sci., 4(4), 370–374. Bornstein, B. H., Deffenbacher, K. A., Penrod, S. D., &amp; McGorty, E. K. (2012). Effects of exposure time and cognitive operations on facial identification accuracy: A meta-analysis of two variables associated with initial memory strength. Psychology, Crime &amp; Law, 18(5), 473–490. Braun, K. A., Ellis, R., &amp; Loftus, E. F. (2002). Make my memory: How advertising can change our memories of the past. Psychol. Mark., 19(1), 1–23. Brewer, W. F., &amp; Treyens, J. C. (1981). Role of schemata in memory for places. Cognitive Psychology, 13(2), 207–230. Brigham, J. C., Bennett, L. B., Meissner, C. A., &amp; Mitchell, T. L. (2007). The influence of race on eyewitness memory. Brigham, J. C., Ready, D. J., &amp; Spier, S. A. (1990). Standards for evaluating the fairness of photograph lineups. Basic and Applied Social Psychology, 11(2), 149–163. Brown, A. S. (1991). A review of tip of the tongue experience. Psychological Bulletin, 109, 79–91. Brown, R., &amp; Kulik, J. (1977). Flashbulb memories. Cognition, 5(1), 73–99. Burton, A. M., Wilson, S., Cowan, M., &amp; Bruce, V. (1999). Face recognition in poor-quality video: Evidence from security surveillance. Psychological Science, 10(3), 243–248. Caputo, D., &amp; Dunning, D. (2007). Distinguishing accurate identifications from erroneous ones: Post-dictive indicators of eyewitness accuracy. In The handbook of eyewitness psychology: Volume II (pp. 441–464). Psychology Press. Ceci, S. J., &amp; Bruck, M. (1995). Jeopardy in the courtroom: A scientific analysis of children’s testimony. American Psychological Association. Cutler, B. L., &amp; Penrod, S. D. (1995). Mistaken identification: The eyewitness, psychology and the law. Cambridge University Press. Darley, J. M., &amp; Gross, P. H. (1983b). A hypothesis-confirming bias in labeling effects. Journal of Personality and Social Psychology, 44(1), 20. Deffenbacher, K. A., Bornstein, B. H., Penrod, S. D., &amp; McGorty, E. K. (2004). A meta-analytic review of the effects of high stress on eyewitness memory. Law and Human Behavior, 28(6), 687. Frenda, S. J., Nichols, R. M., &amp; Loftus, E. F. (2011). Current issues and advances in misinformation research. Current Directions in Psychological Science, 20(1), 20–23. Gabbert, F., Memon, A., &amp; Allan, K. (2003). Memory conformity: Can eyewitnesses influence each other’s memories for an event? Applied Cognitive Psychology: The Official Journal of the Society for Applied Research in Memory and Cognition, 17(5), 533–543. Gabbert, F., Memon, A., Allan, K., &amp; Wright, D. B. (2004). Say it to my face: Examining the effects of socially encountered misinformation. Legal and Criminological Psychology, 9(2), 215–227. Garry, M., French, L., Kinzett, T., &amp; Mori, K. (2008). Eyewitness memory following discussion: Using the MORI technique with a western sample. Applied Cognitive Psychology: The Official Journal of the Society for Applied Research in Memory and Cognition, 22(4), 431–439. Heaps, C., &amp; Nash, M. (1999). Individual differences in imagination inflation. Psychon. Bull. Rev., 6(2), 313–318. Hyman, I. E., Husband, T. H., &amp; Billings, F. J. (1995). False memories of childhood experiences. Appl. Cogn. Psychol., 9(3), 181–197. Jacoby, L. L., &amp; Rhodes, M. G. (2006). False remembering in the aged. Current Directions in Psychological Science, 15(2), 49–53. Laney, C., &amp; Loftus, E. F. (2008). Emotional content of true and false memories. Memory, 16(5), 500–516. Lindsay, D. S., Hagen, L., Read, J. D., Wade, K. A., &amp; Garry, M. (2004). True photographs and false memories. Psychol. Sci., 15(3), 149–154. Loftus, E. F. (1975). Leading questions and the eyewitness report. Cognitive Psychology, 7(4), 560–572. Loftus, E. F. (2005). Planting misinformation in the human mind: A 30-year investigation of the malleability of memory. Learning &amp; Memory, 12(4), 361–366. Loftus, E. F., &amp; Ketcham, K. (1996). The myth of repressed memory: False memories and allegations of sexual abuse. Macmillan. Loftus, E. F., Miller, D. G., &amp; Burns, H. J. (1978). Semantic integration of verbal information into a visual memory. Journal of Experimental Psychology: Human Learning and Memory, 4(1), 19. Loftus, E. F., &amp; Pickrell, J. E. (1995). The formation of false memories. Psychiatr. Ann., 25(12), 720–725. Maran, M. (2010). My lie: A true story of false memory. John Wiley &amp; Sons. Mazzoni, G. A. L., Loftus, E. F., Seitz, A., &amp; Lynn, S. J. (1999). Changing beliefs and memories through dream interpretation. Appl. Cogn. Psychol., 13(2), 125–144. Nickerson, R. S., &amp; Adams, M. J. (1979). Long-term memory for a common object. Cogn. Psychol., 11(3), 287–307. Paterson, H. M., &amp; Kemp, R. I. (2006). Co-witnesses talk: A survey of eyewitness discussion. Psychology, Crime &amp; Law, 12(2), 181–191. Porter, S., Yuille, J. C., &amp; Lehman, D. R. (1999). The nature of real, implanted, and fabricated memories for emotional childhood events: Implications for the recovered memory debate. Law Hum. Behav., 23(5), 517–537. Pratkanis, A. R., Greenwald, A. G., Leippe, M. R., &amp; Baumgardner, M. H. (1988). In search of reliable persuasion effects: III. The sleeper effect is dead: Long live the sleeper effect. J. Pers. Soc. Psychol., 54(2), 203–218. Project, I. (2023). Innocence project. Retrieved from https://www.innocenceproject.org/all-cases/. Rassin, E., Merckelbach, H., &amp; Spaan, V. (2001). When dreams become a royal road to confusion: Realistic dreams, dissociation, and fantasy proneness. J. Nerv. Ment. Dis., 189(7), 478–481. Seamon, J. G., Philbin, M. M., &amp; Harrison, L. G. (2006). Do you remember proposing marriage to the pepsi machine? False recollections from a campus walk. Psychon. Bull. Rev., 13(5), 752–756. Stangor, C., &amp; McMillan, D. (1992). Memory for expectancy-congruent and expectancy-incongruent information: A review of the social and social developmental literatures. Psychological Bulletin, 111(1), 42. State v. guilbert (Vol. 306, p. 705). (2012). (Vol. 306). Connecticut Supreme Court. Takarangi, M. K., Parker, S., &amp; Garry, M. (2006). Modernising the misinformation effect: The development of a new stimulus set. Applied Cognitive Psychology: The Official Journal of the Society for Applied Research in Memory and Cognition, 20(5), 583–590. Talarico, J. M., &amp; Rubin, D. C. (2003). Confidence, not consistency, characterizes flashbulb memories. Psychological Science, 14(5), 455–461. Technical Working Group for Eyewitness Evidence. (1999). Eyewitness evidence: A guide for law enforcement. U.S. Department of Justice, Office of Justice Programs. Trope, Y., &amp; Thompson, E. P. (1997). Looking for truth in all the wrong places? Asymmetric search of individuating information about stereotyped group members. Journal of Personality and Social Psychology, 73(2), 229. Tulving, E., &amp; Thomson, D. M. (1973). Encoding specificity and retrieval processes in episodic memory. Psychological Review, 80(5), 352. United states v. greene (Vol. 704, p. 298). (2013). (Vol. 704). United States Court of Appeals, Fourth Circuit. Wade, K. A., Garry, M., Don Read, J., &amp; Lindsay, D. S. (2002). A picture is worth a thousand lies: Using false photographs to create false childhood memories. Psychonomic Bulletin &amp; Review, 9(3), 597–603. Wason, P. C. (1960). On the failure to eliminate hypotheses in a conceptual task. Q. J. Exp. Psychol., 12(3), 129–140. Watkins, O. C., &amp; Watkins, M. J. (1975). Buildup of proactive inhibition as a cue-overload effect. Journal of Experimental Psychology: Human Learning and Memory, 1(4), 442. Wells, G. L., Kovera, M. B., Douglass, A. B., Brewer, N., Meissner, C. A., &amp; Wixted, J. T. (2020). Policy and procedure recommendations for the collection and preservation of eyewitness identification evidence. Law and Human Behavior, 44(1), 3–36. https://doi.org/10.1037/lhb0000359 Wells, G. L., Memon, A., &amp; Penrod, S. D. (2006). Eyewitness evidence: Improving its probative value. Psychological Science in the Public Interest, 7(2), 45–75. Wells, G. L., &amp; Olson, E. A. (2003). Eyewitness testimony. Annual Review of Psychology, 54(1), 277–295. Wells, G. L., Small, M., Penrod, S., Malpass, R. S., Fulero, S. M., &amp; Brimacombe, C. A. E. (1998). Eyewitness identification procedures: Recommendations for lineups and photospreads. Law and Human Behavior, 22(6), 603–647. https://doi.org/10.1023/A:1025750605807 Winograd, E., Peluso, J. P., &amp; Glover, T. A. (1998). Individual differences in susceptibility to memory illusions. Appl. Cogn. Psychol., 12(7), S5–S27. Wixted, J. T., Mickes, L., &amp; Fisher, R. P. (2018). Rethinking the reliability of eyewitness memory. Perspectives on Psychological Science, 13(3), 324–335. Wixted, J. T., &amp; Wells, G. L. (2017). The relationship between eyewitness confidence and identification accuracy: A new synthesis. Psychological Science in the Public Interest, 18(1), 10–65. Wixted, J. T., Wells, G. L., Loftus, E. F., &amp; Garrett, B. L. (2021). Test a witness’s memory of a suspect only once. Psychological Science in the Public Interest, 22(1_suppl), 1S–18S. https://doi.org/10.1177/15291006211026259 Yilmaz, A. S., Shen, K. J., &amp; Wixted, J. T. (2023). The principles of memory versus the federal rules of evidence. "],["knowledge.html", "Chapter 7 Knowledge 7.1 Nature of Categories 7.2 Theories of Concept Representation 7.3 Organization of Concepts 7.4 Glossary", " Chapter 7 Knowledge People form mental concepts of categories of objects, which permit them to respond appropriately to new objects they encounter. Most concepts cannot be strictly defined but are organized around the “best” examples or prototypes, which have the properties most common in the category. Objects fall into many different categories, but there is usually a most salient one, called the basic-level category, which is at an intermediate level of specificity (e.g., chairs, rather than furniture or desk chairs). Concepts are closely related to our knowledge of the world, and people can more easily learn concepts that are consistent with their knowledge. Theories of concepts argue either that people learn a summary description of a whole category or else that they learn exemplars of the category. LEARNING OBJECTIVES Understand the problems with attempting to define categories. Learn about theories of the mental representation of concepts. Learn how knowledge may influence concept learning. Figure 7.1: Although you’ve (probably) never seen this particular truck before, you know a lot about it because of the knowledge you’ve accumulated in the past about the features in the category of trucks. CC0 Public Domain Consider the following set of objects: some dust, papers, a computer monitor, two pens, a cup, and an orange. What do these things have in common? Only that they all happen to be on my desk as I write this. This set of things can be considered a category, a set of objects that can be treated as equivalent in some way. But, most of our categories seem much more informative—they share many properties. For example, consider the following categories: trucks, wireless devices, weddings, psychopaths, and trout. Although the objects in a given category are different from one another, they have many commonalities. When you know something is a truck, you know quite a bit about it. The psychology of categories concerns how people learn, remember, and use informative categories such as trucks or psychopaths. The mental representations we form of categories are called concepts. There is a category of trucks in the world, and I also have a concept of trucks in my head. We assume that people’s concepts correspond more or less closely to the actual category, but it can be useful to distinguish the two, as when someone’s concept is not really correct. Concepts are at the core of intelligent behavior. We expect people to be able to know what to do in new situations and when confronting new objects. If you go into a new classroom and see chairs, a blackboard, a projector, and a screen, you know what these things are and how they will be used. You’ll sit on one of the chairs and expect the instructor to write on the blackboard or project something onto the screen. You do this even if you have never seen any of these particular objects before, because you have concepts of classrooms, chairs, projectors, and so forth, that tell you what they are and what you’re supposed to do with them. Furthermore, if someone tells you a new fact about the projector—for example, that it has a halogen bulb—you are likely to extend this fact to other projectors you encounter. In short, concepts allow you to extend what you have learned about a limited number of objects to a potentially infinite set of entities. You know thousands of categories, most of which you have learned without careful study or instruction. Although this accomplishment may seem simple, we know that it isn’t, because it is difficult to program computers to solve such intellectual tasks. If you teach a learning program that a robin, a swallow, and a duck are all birds, it may not recognize a cardinal or peacock as a bird. As we’ll shortly see, the problem is that objects in categories are often surprisingly diverse. Simpler organisms, such as animals and human infants, also have concepts (Mareschal et al., 2010). Squirrels may have a concept of predators, for example, that is specific to their own lives and experiences. However, animals likely have many fewer concepts and cannot understand complex concepts such as mortgages or musical instruments. 7.1 Nature of Categories Figure 7.2: Here is a very good dog, but one that does not fit perfectly into a well-defined category where all dogs have four legs. Image: State Farm, CC BY 2.0 Traditionally, it has been assumed that categories are well-defined. This means that you can give a definition that specifies what is in and out of the category. Such a definition has two parts. First, it provides the necessary features for category membership: What must objects have in order to be in it? Second, those features must be jointly sufficient for membership: If an object has those features, then it is in the category. For example, if I defined a dog as a four-legged animal that barks, this would mean that every dog is four-legged, an animal, and barks, and also that anything that has all those properties is a dog. Unfortunately, it has not been possible to find definitions for many familiar categories. Definitions are neat and clear-cut; the world is messy and often unclear. For example, consider our definition of dogs. In reality, not all dogs have four legs; not all dogs bark. I knew a dog that lost her bark with age (this was an improvement); no one doubted that she was still a dog. It is often possible to find some necessary features (e.g., all dogs have blood and breathe), but these features are generally not sufficient to determine category membership (you also have blood and breathe but are not a dog). Even in domains where one might expect to find clear-cut definitions, such as science and law, there are often problems. For example, many people were upset when Pluto was downgraded from its status as a planet to a dwarf planet in 2006. Upset turned to outrage when they discovered that there was no hard-and-fast definition of planethood: “Aren’t these astronomers scientists? Can’t they make a simple definition?” In fact, they couldn’t. After an astronomical organization tried to make a definition for planets, a number of astronomers complained that it might not include accepted planets such as Neptune and refused to use it. If everything looked like our Earth, our moon, and our sun, it would be easy to give definitions of planets, moons, and stars, but the universe has sadly not conformed to this ideal. Table 7.1: Examples of two categories, with members ordered by typicality (from Rosch &amp; Mervis, 1975) Furniture Fruit chair orange table banana desk pear bookcase plum lamp strawberry cushion pineapple rug lemon stove honeydew picture date vase tomato Typicality Even among items that clearly are in a category, some seem to be “better” members than others (E. H. Rosch, 1973). Among birds, for example, robins and sparrows are very typical. In contrast, ostriches and penguins are very atypical (meaning not typical). If someone says, “There’s a bird in my yard,” the image you have will be of a smallish passerine bird such as a robin, not an eagle or hummingbird or turkey. You can find out which category members are typical merely by asking people. Table 7.1 shows a list of category members in order of their rated typicality. Typicality is perhaps the most important variable in predicting how people interact with categories. Table 7.2 is a partial list of what typicality influences. We can understand the two phenomena of borderline members and typicality as two sides of the same coin. Think of the most typical category member: This is often called the category prototype. Items that are less and less similar to the [prototype] become less and less typical. At some point, these less typical items become so atypical that you start to doubt whether they are in the category at all. Is a rug really an example of furniture? It’s in the home like chairs and tables, but it’s also different from most furniture in its structure and use. From day to day, you might change your mind as to whether this atypical example is in or out of the category. So, changes in typicality ultimately lead to borderline members. Table 7.2: Influences of typicality on cognition. Influence Source Typical items are judged category members more often Hampton (1979) Speed of categorization is faster for typical items Rips et al. (1973) Typical members are learned before atypical ones Rosch &amp; Mervis (1975) Learning a category is easier if typical examples are provided Mervis &amp; Pani (1980) In language comprehension, references to typical members are understood more easily Garrod &amp; Sanford (1977) In language production, people tend to say typical items before atypical ones (e.g., “apples and lemons” rather than “lemons and apples”) Onishi et al. (2008) Source of Typicality Intuitively, it is not surprising that robins are better examples of birds than penguins are, or that a table is a more typical kind of furniture than is a rug. But given that robins and penguins are known to be birds, why should one be more typical than the other? One possible answer is the frequency with which we encounter the object: We see a lot more robins than penguins, so they must be more typical. Frequency does have some effect, but it is actually not the most important variable (Rosch et al., 1976). For example, I see both rugs and tables every single day, but one of them is much more typical as furniture than the other. The best account of what makes something typical comes from Rosch &amp; Mervis (1975) family resemblance theory. They proposed that items are likely to be typical if they (a) have the features that are frequent in the category and (b) do not have features frequent in other categories. Let’s compare two extremes, robins and penguins. Robins are small flying birds that sing, live in nests in trees, migrate in winter, hop around on your lawn, and so on. Most of these properties are found in many other birds. In contrast, penguins do not fly, do not sing, do not live in nests or in trees, do not hop around on your lawn. Furthermore, they have properties that are common in other categories, such as swimming expertly and having wings that look and act like fins. These properties are more often found in fish than in birds. According to Rosch and Mervis, then, it is not because a robin is a very common bird that makes it typical. Rather, it is because the robin has the shape, size, body parts, and behaviors that are very common among birds—and not common among fish, mammals, bugs, and so forth. In a classic experiment, Rosch &amp; Mervis (1975) made up two new categories, with arbitrary features. Subjects viewed example after example and had to learn which example was in which category. Rosch and Mervis constructed some items that had features that were common in the category and other items that had features less common in the category. The subjects learned the first type of item before they learned the second type. Furthermore, they then rated the items with common features as more typical. In another experiment, Rosch and Mervis constructed items that differed in how many features were shared with a different category. The more features were shared, the longer it took subjects to learn which category the item was in. These experiments, and many later studies, support both parts of the family resemblance theory. Figure 7.3: When you think of “bird,” how closely does the robin resemble your general figure? CC0 Public Domain 7.2 Theories of Concept Representation Now that we know these facts about the psychology of concepts, the question arises of how concepts are mentally represented. There have been two main answers. The first, somewhat confusingly called the prototype theory suggests that people have a summary representation of the category, a mental description that is meant to apply to the category as a whole. (The significance of summary will become apparent when the next theory is described.) This description can be represented as a set of weighted features (Smith &amp; Medin, 1981). The features are weighted by their frequency in the category. For the category of birds, having wings and feathers would have a very high weight; eating worms would have a lower weight; living in Antarctica would have a lower weight still, but not zero, as some birds do live there. Figure 7.4: If you were asked, “What kind of animal is this?” according to prototype theory, you would consult your summary representations of different categories and then select the one that is most similar to this image—probably a lizard! Adhi Rachdian, CC BY 2.0 The idea behind prototype theory is that when you learn a category, you learn a general description that applies to the category as a whole: Birds have wings and usually fly; some eat worms; some swim underwater to catch fish. People can state these generalizations, and sometimes we learn about categories by reading or hearing such statements (“The kimodo dragon can grow to be 10 feet long”). When you try to classify an item, you see how well it matches that weighted list of features. For example, if you saw something with wings and feathers fly onto your front lawn and eat a worm, you could (unconsciously) consult your concepts and see which ones contained the features you observed. This example possesses many of the highly weighted bird features, and so it should be easy to identify as a bird. This theory readily explains the phenomena we discussed earlier. Typical category members have more, higher-weighted features. Therefore, it is easier to match them to your conceptual representation. Less typical items have fewer or lower-weighted features (and they may have features of other concepts). Therefore, they don’t match your representation as well. This makes people less certain in classifying such items. Borderline items may have features in common with multiple categories or not be very close to any of them. For example, edible seaweed does not have many of the common features of vegetables but also is not close to any other food concept (meat, fish, fruit, etc.), making it hard to know what kind of food it is. A very different account of concept representation is the exemplar theory (exemplar being a fancy name for an example; Medin &amp; Schaffer (1978)). This theory denies that there is a summary representation. Instead, the theory claims that your concept of vegetables is remembered examples of vegetables you have seen. This could of course be hundreds or thousands of exemplars over the course of your life, though we don’t know for sure how many exemplars you actually remember. How does this theory explain classification? When you see an object, you (unconsciously) compare it to the exemplars in your memory, and you judge how similar it is to exemplars in different categories. For example, if you see some object on your plate and want to identify it, it will probably activate memories of vegetables, meats, fruit, and so on. In order to categorize this object, you calculate how similar it is to each exemplar in your memory. These similarity scores are added up for each category. Perhaps the object is very similar to a large number of vegetable exemplars, moderately similar to a few fruit, and only minimally similar to some exemplars of meat you remember. These similarity scores are compared, and the category with the highest score is chosen. Why would someone propose such a theory of concepts? One answer is that in many experiments studying concepts, people learn concepts by seeing exemplars over and over again until they learn to classify them correctly. Under such conditions, it seems likely that people eventually memorize the exemplars (J. D. Smith &amp; Minda, 1998). There is also evidence that close similarity to well-remembered objects has a large effect on classification. Allen &amp; Brooks (1991) taught people to classify items by following a rule. However, they also had their subjects study the items, which were richly detailed. In a later test, the experimenters gave people new items that were very similar to one of the old items but were in a different category. That is, they changed one property so that the item no longer followed the rule. They discovered that people were often fooled by such items. Rather than following the category rule they had been taught, they seemed to recognize the new item as being very similar to an old one and so put it, incorrectly, into the same category. 7.3 Organization of Concepts Semantic Networks The Semantic Network approach proposes that concepts of the mind are arranged in a functional storage-system for the meanings of words. In a graphical illustration of such a semantic net, concepts of our mental dictionary are represented by nodes, which represent a piece of knowledge about our world. Links between the nodes indicate the relationship between concepts. The links can not only show that there is a relationship, they can also indicate the kind of relation by their length, for example. Every concept in the net is in a dynamical correlation with other concepts, which may have protoypically similar characteristics or functions. Collins and Quillian’s Model One of the first scientists who thought about structural models of human memory that could be run on a computer was Quillian (1967). Together with Allan Collins, he developed the Semantic Network with related categories and with a hierarchical organization. In the Figure 7.5, Collins and Quillian’s network with added properties at each node is shown. As already mentioned, the skeleton-nodes are interconnected by links. At the nodes, concept names are added. General concepts are on the top and more particular ones at the bottom. By looking at the concept “car”, one gets the information that a car has 4 wheels, has an engine, has windows, and furthermore moves around, needs fuel, is manmade. Figure 7.5: Semantic Network according to Collins and Quillian with nodes, links, concept names and properties. These pieces of information must be stored somewhere. It would take too much space if every detail must be stored at every level. So the information of a car is stored at the base level and further information about specific cars, e.g. BMW, is stored at the lower level, where you do not need the fact that the BMW also has four wheels, if you already know that it is a car. This way of storing shared properties at a higher-level node is called cognitive economy. Information that is shared by several concepts is stored in the highest parent node. All child nodes below the information bearer also contain the properties of the parent node. However, there are exceptions. Sometimes a special car has not four wheels, but three. This specific property is stored in the child node. Evidence for this structure can be found by the sentence verification technique. In experiments participants had to answer statements about concepts with “yes” or “no”. It took longer to say “yes” if the concept-bearing nodes were further apart. The phenomenon that adjacent concepts are activated is called spreading activation. These concepts are far more easily accessed by memory, they are “primed”. This was studied and backed by Meyer &amp; Schvaneveldt (1971) with a lexical-decision task. Participants had to decide if word pairs were words or non-words. They were faster at finding real word pairs if the concepts of the two words were near each other in a semantic network. Key Takeaways Concepts are central to our everyday thought. When we are planning for the future or thinking about our past, we think about specific events and objects in terms of their categories. If you’re visiting a friend with a new baby, you have some expectations about what the baby will do, what gifts would be appropriate, how you should behave toward it, and so on. Knowing about the category of babies helps you to effectively plan and behave when you encounter this child you’ve never seen before. Learning about those categories is a complex process that involves seeing exemplars (babies), hearing or reading general descriptions (“Babies like black-and-white pictures”), general knowledge (babies have kidneys), and learning the occasional rule (all babies have a rooting reflex). Exercises Practice: Pick a couple of familiar categories and try to come up with definitions for them. When you evaluate each proposal (a) is it in fact accurate as a definition, and (b) is it a definition that people might actually use in identifying category members? Practice: For the same categories, can you identify members that seem to be “better” and “worse” members? What about these items makes them typical and atypical? Discussion: Choose three common categories: a natural kind, a human artifact, and a social event. Are the corresponding categories likely to differ across countries and cultures? Can you make a hypothesis about when such categories are likely to differ and when they are not? 7.4 Glossary Basic-level category The neutral, preferred category for a given object, at an intermediate level of specificity. Category A set of entities that are equivalent in some way. Usually the items are similar to one another. Cognitive economy A principle of semantic organization that properties of a category that are shared by many members of a category are stored at a higher-level node in the network. Concept The mental representation of a category. Exemplar An example in memory that is labeled as being in a particular category. Protoype The most typical category member. Prototype theory A theory of concept representation that people have a summary representation of a category that is meant to apply to the category as a whole. Spreading activation When a concept is activated in memory, related concepts also increase in activation. Typicality The difference in “goodness” of category members, ranging from the most typical (the prototype) to borderline members. References Allen, S. W., &amp; Brooks, L. R. (1991). Specializing the operation of an explicit rule. J. Exp. Psychol. Gen., 120(1), 3–19. Garrod, S., &amp; Sanford, A. (1977). Interpreting anaphoric relations: The integration of semantic information while reading. Journal of Verbal Learning and Verbal Behavior, 16(1), 77–90. Hampton, J. A. (1979). Polymorphous concepts in semantic memory. J. Verbal Learning Verbal Behav., 18(4), 441–461. Mareschal, D., Quinn, P. C., &amp; Lea, S. E. G. (2010). The making of human concepts: A final look. In The making of human concepts (pp. 387–394). Oxford University Press. Medin, D. L., &amp; Schaffer, M. M. (1978). Context theory of classification learning. Psychol. Rev., 85(3), 207–238. Mervis, C. B., &amp; Pani, J. R. (1980). Acquisition of basic object categories. Cognitive Psychology, 12(4), 496–522. Meyer, D. E., &amp; Schvaneveldt, R. W. (1971). Facilitation in recognizing pairs of words: Evidence of a dependence between retrieval operations. Journal of Experimental Psychology, 90(2), 227. Onishi, K. H., Murphy, G. L., &amp; Bock, K. (2008). Prototypicality in sentence production. Cognitive Psychology, 56(2), 103–141. Quillian, M. R. (1967). Word concepts: A theory and simulation of some basic semantic capabilities. Behavioral Science, 12(5), 410–430. Rips, L. J., Shoben, E. J., &amp; Smith, E. E. (1973). Semantic distance and the verification of semantic relations. Journal of Verbal Learning and Verbal Behavior, 12(1), 1–20. Rosch, E. H. (1973). On the internal structure of perceptual and semantic categories. In Cognitive development and acquisition of language (pp. 111–144). Elsevier. Rosch, E., &amp; Mervis, C. B. (1975). Family resemblance: Studies in the internal structure of categories. Cognitive Psychology, 7, 573–605. Rosch, E., Simpson, C., &amp; Miller, R. S. (1976). Structural bases of typicality effects. J. Exp. Psychol. Hum. Percept. Perform., 2(4), 491–502. Smith, J. D., &amp; Minda, J. P. (1998). Prototypes in the mist: The early epochs of category learning. J. Exp. Psychol. Learn. Mem. Cogn., 24(6), 1411–1436. "],["language.html", "Chapter 8 Language 8.1 What is Language? 8.2 Mechanisms of Language 8.3 Language Acquisition 8.4 Language and Thought 8.5 Glossary", " Chapter 8 Language Although language is often used for the transmission of information (“turn right at the next light and then go straight,” “Place tab A into slot B”), this is only its most mundane function. Language also allows us to access existing knowledge, to draw conclusions, to set and accomplish goals, and to understand and communicate complex social relationships. Human language is the most complex behavior on the planet and, at least as far as we know, in the universe. Some languages are oral - or spoken - which involves the operation of the vocal cords, and the coordination of breath with movements of the throat, mouth, and tongue. Other languages are signed, where the language is expressed by movements of the hands. You may be familiar with American Sign Language (ASL), but there are at least 300 sign languages around the world. All human languages involve a variety of complex cognitive, social, and biological processes. Language involves both the ability to comprehend spoken, signed, or written words and to create communication in real time when we speak, sign, or write. LEARNING OBJECTIVES Recognize the complexity and diversity of human language. Distinguish between the components of language. Explain important processes in language processing and production. Describe how language is learned in infancy and adulthood. Explain evidence on the relationship between language and thought. 8.1 What is Language? Did you know that you can read minds? That’s pretty much what you do every time you use human language to communicate with the people around you. Language allows us to convert our thoughts into a code, which others can decode, essentially transmitting our thoughts into another person’s mind. Imagine talking to a friend. Before you speak, you decide what to want to say. If you use spoken languages, you then squeeze the air out of your lungs, vibrating your vocal folds, and manipulate parts of your mouth to produce sounds. Those sounds then vibrate the air particles and are picked up by your eardrums and interpreted as auditory information by your brain. If you use signed languages, you use a variety of manual and facial gestures to produce meaningful signs. The person you’re signing with sees these gestures and interprets this as visual information. In both cases, if the communication was successful, the other person ends up with an idea in their mind that’s similar to the idea in yours. All humans have this shared system that allows us to understand each other’s ideas through language. We are not the only creatures that can communicate, however. Nonhuman animals have a wide variety of systems of communication. Some species communicate using scents; others use visual displays, such as baring the teeth, puffing up the fur, or flapping the wings; and still others use vocal sounds. Male songbirds, such as canaries and finches, sing songs both to attract mates and defend their territory; similarly, chimpanzees employ a variety of facial expressions, sounds, and actions, including ground-slapping, to communicate aggression (De Waal, 1989). Honeybees use a waggle dance to direct other bees to the location of food sources (Von Frisch, 1950). The communication of vervet monkeys is relatively advanced in the sense that they use specific sounds to convey specific meanings. Vervets make different calls to signify that they have seen either a leopard, a snake, or a hawk (Seyfarth &amp; Cheney, 1997). Despite their wide abilities to communicate, efforts to teach animals to use language have had only limited success. The most proficient nonhuman language speaker is Kanzi, a bonobo who lives at the Language Learning Center at Georgia State University (Savage-Rumbaugh &amp; Lewin, 1994). As you can see in the video linked in Figure 8.1, Kanzi has a propensity for language that is in many ways similar to humans. He learned faster when he was younger than when he got older, he learns by observation, and he can use symbols to comment on social interactions, rather than simply for food treats. Kanzi can also create elementary syntax and understand relatively complex commands. Kanzi can make tools and can even play the video game Pac-Man. And yet even Kanzi does not have a true language in the same way that humans do. Human babies learn words faster and faster as they get older, but Kanzi does not. Each new word he learns is almost as difficult as the one before. Kanzi usually requires many trials to learn a new sign, whereas human babies can speak words after only one exposure. Kanzi’s language is focused primarily on food and pleasure and only rarely on social relationships. Although he can combine words, he generates few new phrases and cannot master syntactic rules beyond the level of about a two-year-old human child (Greenfield &amp; Savage-Rumbaugh, 1991). Figure 8.1: Watch: “Kanzi and Novel Sentences” on YouTube; Image: Wikimedia Commons In sum, although many animals communicate, none of them has a true language. With some exceptions, the information that can be communicated in nonhuman species is limited primarily to displays of liking or disliking, and related to basic motivations of aggression and mating. Humans also use this more primitive type of communication, in the form of nonverbal behaviors such as eye contact, touch, hand signs, and interpersonal distance, to communicate their like or dislike for others, but they (unlike nonhuman animals) also supplement this more primitive communication with language. Although other animal brains share similarities to ours, only the human brain is complex enough to create language. What is perhaps most remarkable is that although language never appears in non-humans, language is universal in humans. All humans, unless they have a profound brain abnormality or are completely isolated from other humans, learn language. Linguistic Diversity We’ve introduced language, but we can also talk about individual languages. There are thousands of human languages that are currently being spoken around the world. We don’t know the exact number because it’s hard to define what the difference between a language and a dialect is. Colloquially, the term dialect is used to refer to ways of speaking that people perceive to be substandard, low status, associated with working class, non-prestigious, geographically-isolated, or some derivation or aberration from a ‘standard’ version of the language. The linguistic fact though is that everyone has a dialect. Rather than think about languages and dialects in a hierarchical way, linguists think about dialects as subdivisions of a language. What you may think of as ‘English’ is Mainstream US English (MUSE), which is a dialect of English. If you’re from California, you may speak Californian English, yet another dialect. No one of these dialects is more correct or legitimate than another. The way you speak may also be affected by the kinds of groups you associate with. Speaking differently with different social groups is called a sociolect. There are many words, phrases, and pronunciation differences associated with the LGBT+ community. Using these features could constitute a sociolect. Last, there are also just some things that we each do a little bit differently. Think of particular saying that you might use with your family, inside jokes with your friends, memes, and features from different dialects and sociolects. All of these come together to create the unique version of a language that you speak. We call that an idiolect. The Components of Language Language can be conceptualized in terms of sounds, meaning, and the environmental factors that help us understand it. Phonemes are the elementary sounds of our language, morphemes are the smallest units of meaning in a language, syntax is the set of grammatical rules that control how words are put together, and contextual information is the elements of communication that are not part of the content of language but that help us understand its meaning. A phoneme is the smallest unit of sound that makes a meaningful difference in a language. The word “bit” has three phonemes, /b/, /i/, and /t/ (in transcription, phonemes are placed between slashes), and the word “pit” also has three: /p/, /i/, and /t/. In spoken languages, phonemes are produced by the positions and movements of the vocal tract, including our lips, teeth, tongue, vocal cords, and throat, whereas in sign languages phonemes are defined by the shapes and movement of the hands. There are hundreds of unique phonemes that can be made by human speakers, but most languages only use a small subset of the possibilities. English contains about 45 phonemes, whereas other languages have as few as 15 and others more than 60. The Hawaiian language contains only about a dozen phonemes, including five vowels (a, e, i, o, and u) and seven consonants (h, k, l, m, n, p, and w). Whereas phonemes are the smallest units of sound in language, a morpheme is a string of one or more phonemes that makes up the smallest units of meaning in a language. Some morphemes, such as one-letter words like “I” and “a,” are also phonemes, but most morphemes are made up of combinations of phonemes. Some morphemes are prefixes and suffixes used to modify other words. For example, the syllable “re-” as in “rewrite” or “repay” means “to do again,” and the suffix “-est” as in “happiest” or “coolest” means “to the maximum.” Syntax is the set of rules of a language by which we construct sentences. Each language has a different syntax. The syntax of the English language requires that each sentence have a noun and a verb, each of which may be modified by adjectives and adverbs. For example, in African American English (AAE), which has different syntactic rules that MUSE, to describe a habitual action, one could say, “They be reading in the library.” This sentence doesn’t follow the rules of MUSE syntax, in which one would more likely describe the same event by saying something like, “They are usually reading in the library” (Green, 2002). This also shows us how many systematic grammatical differences there might be between two very similar dialects of the same language, like AAE and MUSE. Semantics is the meaning of words and sentences. Languages denote, refer to, and represent things. Words do not possess fixed meanings but change their interpretation as a function of the context in which they are spoken. We use contextual information — the information surrounding language—to help us interpret it. The study of this kind of contextual information is called pragmatics. Examples of contextual information include the knowledge that we have and that we know that other people have, and nonverbal expressions such as facial expressions, postures, gestures, and tone of voice. Misunderstandings can easily arise if people aren’t attentive to contextual information or if some of it is missing, such as in newspaper headlines or in text messages (see box). An example that can help differentiate semantics and pragmatics is the sentence, “It’s cold in here.” The denotational meaning (i.e. semantics) of the sentence refers to the temperature in the room. But perhaps someone says this situation to someone sitting next to an open window. Now, this sentence means something like “please close the window.” That is the pragmatic meaning. Examples in Which Syntax Is Correct but the Interpretation Can Be Ambiguous Grandmother of Eight Makes Hole in One Milk Drinkers Turn to Powder Farmer Bill Dies in House Old School Pillars Are Replaced by Alumni Two Convicts Evade Noose, Jury Hung Include Your Children When Baking Cookies 8.2 Mechanisms of Language The two major components of language that are studied in cognitive psychology are language production and language comprehension. Language production encompasses the process of converting thoughts into language, while language comprehension, or language processing, is the process of decoding language back into thoughts (Figure 8.2). Successful communication happens when the listener correctly decodes the speech signal produced by the speaker, understanding the intended message. Figure 8.2: A model of communication with language exchange. Language Processing Language processing is an incredibly complex process. In this section, we introduce some of the topics that fall under language processing. Speech Perception Above, we introduced phonemes as a concept. But how do we know these are relevant units of language to study? Speech is a continuous signal and part of language comprehension is picking out the sounds and telling them apart. These tasks comprise speech perception. The first of these tasks, [speech segmentation], requires the listener to tell where one word stops and another begins. The place where one word stops and another begins is called a word boundary. Children as young as 8 months are sensitive to the statistical regularities in the language that they hear and use them to segment words. A study found that after being familiarized with sequences of words generated by a speech synthesizer, infants were able to tell where word boundaries were (Saffran et al., 1996). Part of telling words apart requires listeners to tell two sounds apart. For example, to tell the words ‘pat’ and ‘bat’ apart, you have to be able to tell the difference between the two sounds /p/ and /b/. English speakers can differentiate the /r/ phoneme from the /l/ phoneme, and thus “rake” and “lake” are heard as different words. In Japanese, however, /r/ and /l/ are the same phoneme, and thus speakers of that language cannot tell the difference between the word “rake” and the word “lake.” Try saying the words “cool” and “keep” out loud. Can you hear the difference between the two /k/ sounds? To English speakers they both sound the same, but to speakers of Arabic these represent two different phonemes. We know that infants are born able to understand all phonemes, but as they get older, they lose their ability to do so. By 10 months of age a child’s ability to recognize phonemes becomes very similar to that of the adult speakers of the native language. Phonemes that were initially differentiated come to be treated as equivalent (Werker &amp; Tees, 2002). Follow the link in Figure 8.3 to hear an explanation of how psychologists study phoneme perception in infants. Figure 8.3: Watch Tuning Into Speech Sounds on YouTube Research, like that done by Saffran et al. (1996) and Werker &amp; Tees (2002), tells us that as children learn a particular language, they very quickly start to pick up on statistical patterns in the language and start to pay attention only to the relevant acoustic information that helps them tell sounds apart in their language. Sentence Processing Sentence processing takes place whenever a reader or listener processes a language utterance, either in isolation or in the context of a conversation or a text. To do this, you group words into phrases, based on the syntactic rules from your language, in order to determine what the sentence means. This process is called [parsing]. By and large, this happens so quickly and automatically, that it’s hard to notice yourself doing it. Let’s look at examples of when parsing is very hard. [Garden path sentences] are sentences that start out in a way that makes you think the sentence means one thing, but information at the end of the sentence forces you to reevaluate the meaning of the sentence. These are all over the place, if you’re looking out for them (see box for more examples). Can you understand these Garden Path Sentences the first time? The horse raced past the barn fell. The old man the boat. When Fred eats food gets thrown. The cotton clothing is made of grows in Mississippi. These sentences show us that as you read or hear the next word, you are continuously parsing the sentence and evaluating the meaning. You don’t wait until the end of the sentence to parse and understand it. Language Production Less is known about language production, because studying it can be more difficult. One way that people study it is by asking what affects the way people speak. One thing that affects language production is the characteristics of the language that the speaker hears before speaking. For example, if the speaker is listening to descriptions of an event that use passive constructions (e.g. ‘The election was won by the incumbent.’) as opposed to active constructions (e.g. ‘The incumbent won the election.’), the speaker is more likely to use the passive construction when they speak next (Pickering &amp; Ferreira, 2008). This is called structural priming. 8.3 Language Acquisition Language learning begins even before birth, because the fetus can hear muffled versions of speaking from outside the womb. Moon et al. (1993) found that infants only two days old sucked harder on a pacifier when they heard their mothers’ native language being spoken than when they heard a foreign language, even when strangers were speaking the languages. During the first year or so after birth, and long before they speak their first words, infants are already learning language. One aspect of this learning is practice in producing speech. By the time they are six to eight weeks old, babies start making vowel sounds (ooohh, aaahh, goo) as well as a variety of cries and squeals to help them practice. Figure 8.4: Babies often engage in vocal exchanges to help them practice language. At about seven months, infants begin babbling, engaging in intentional vocalizations that lack specific meaning. Children babble as practice in creating specific sounds, and by the time they are one year old, the babbling uses primarily the sounds of the language that they are learning (Boysson-Bardies et al., 1984). These vocalizations have a conversational tone that sounds meaningful even though it isn’t. Babbling also helps children understand the social, communicative function of language (Figure 8.4). Children who are exposed to sign language babble in sign by making hand movements that represent real language (Petitto &amp; Marentette, 1991). At the same time that infants are practicing their speaking skills by babbling, they are also learning to better understand sounds and eventually the words of language. One of the first words that children understand is their own name, usually by about six months, followed by commonly used words like “bottle,” “mama,” and “doggie” by 10 to 12 months (Mandel et al., 1995). The early utterances of children contain many errors, for instance, confusing /b/ and /d/, or /c/ and /z/. And the words that children create are often simplified, in part because they are not yet able to make the more complex sounds of the real language (Dobrich &amp; Scarborough, 1992). Children may say “keekee” for kitty, “nana” for banana, and “vesketti” for spaghetti in part because it is easier. Often these early words are accompanied by gestures that may also be easier to produce than the words themselves. Children’s pronunciations become increasingly accurate between one and three years, but some problems may persist until school age. Most of a child’s first words are nouns, and early sentences may include only the noun. “Ma” may mean “more milk please” and “da” may mean “look, there’s Fido.” Eventually the length of the utterances increases to two words (“mo ma” or “da bark”), and these primitive sentences begin to follow the appropriate syntax of the native language. Infants usually produces their first words at about one year of age. It is at this point that the child first understands that words are more than sounds — they refer to particular objects and ideas. By the time children are two years old, they have a vocabulary of several hundred words, and by kindergarten their vocabularies have increased to several thousand words. By Grade 5, most children know about 50,000 words. The average university student knows about 200,000 words. Because language involves the active categorization of sounds and words into higher level units, children make some mistakes in interpreting what words mean and how to use them. In particular, they often make overextensions of concepts, which means they use a given word in a broader context than appropriate. A child might at first call all adult men “daddy” or all animals “doggie.” Children learn that people are usually referring to things that they are looking at when they are speaking (Baldwin, 1993), and that that the speaker’s emotional expressions are related to the content of their speech. Children also use their knowledge of syntax to help them figure out what words mean. If a child hears an adult point to a strange object and say, “this is a dirb,” they will infer that a “dirb” is a thing, but if they hear them say, “this is a one of those dirb things” they will infer that it refers to the color or other characteristic of the object. And if they hear the word “dirbing,” they will infer that “dirbing” is something that we do (Waxman, 1990). Language Environment and Cognitive Development Not all children are given access to language at an early age. One of the best documented cases of intentional language deprivation is the case of Genie, who was locked in a small room from infancy until the age of 13, almost completely deprived of human contact. When psychologists were evaluating her condition and beginning to treat her, she was only able to say a few words and it wasn’t clear how much she understood. As she went through therapy, she followed some normal language acquisition milestones, like the one-word stage and using short phrases. But her language development was slower than average and she never acquired a full grammar (Pines, 1981). Despite the known damage of language deprivation, some children still grow up with limited or no access to a language environment. About 90-95% of babies who are born deaf or hard of hearing are born into families where the primary language modality is speech (Mitchell &amp; Karchmer, 2004). In some cases, families who learn that their child is deaf choose to start learning a sign language like ASL. If they use sign in their interactions with their child, then the baby has access to an ambient language in the visual modality and will develop a mental grammar from that input. However, some families of deaf children choose not to use signed language, instead waiting to expose the child to language until they can receive a cochlear implant. A cochlear implant (CI) is an electronic prosthetic device that is implanted surgically. It takes sounds from the environment and converts them to electrical signals. Those signals then get transmitted to the brain via the auditory nerve (NIDCD, 2021). In developed countries, it is very common for deaf children to receive a CI between ages one and three years. Even so, most children with CIs still score well below hearing children on standard tests of language proficiency, even when they’ve had the implant for several years – not because of any deficit in the children, but because the electrical signals from a cochlear implant are not a sufficient language environment for typical development (Mauldin, 2019). This language deprivation leads to long-term impairments in social and cognitive function and, ironically, to lower scores on tests of vocal language comprehension (M. L. Hall et al., 2019; W. C. Hall, 2017; Humphries et al., 2016; Lillo-Martin &amp; Henner, 2021). In contrast, when deaf children of hearing parents have access to ASL by age six months, their vocabulary develops at a comparable rate to deaf children of deaf signing parents (Caselli et al., 2021). This evidence from deaf children shows us just how vital the language environment is, and how important it is that children have access to an ambient language as early as possible. The neural connections that make up mental grammar can only form in response to language input from the environment. Without that input in the first year of life, it’s much harder for the brain to build a mental grammar. Bilingualism While a lot of work on children’s language acquisition focuses on kids who are acquiring just one language, monolingual kids are not the norm around the world. It is just as common for children to grow up with more than one language in the environment. Some children have one language in the home environment and another at daycare. In some families, one adult speaks one language and another adult speaks a different one. And some kids grow up in an environment where all the adults switch between two or three languages. Some early psychological research showed that, when compared with monolingual children, bilingual children performed more slowly when processing language and their verbal scores were lower. However, these tests were frequently given in English, even when this was not the child’s first language, and the children tested were often of lower socioeconomic status than the monolingual children (Andrews, 1982). More current research that has controlled for these factors has found that, although bilingual children may, in some cases, learn language somewhat slower than do monolingual children (Oller &amp; Pearson, 2002), bilingual and monolingual children do not significantly differ in the final depth of language learning, nor do they generally confuse the two languages (Nicoladis &amp; Genesee, 1997). In fact, participants who speak two languages have sometimes been found to have better cognitive functioning, cognitive flexibility, and analytic skills in comparison to monolinguals (Bialystok, 2009), though other studies have found no executive function advantage for bilinguals (Dick et al., 2019). Research has also found that learning a second language produces changes in the area of the brain in the left hemisphere that is involved in language, such that this area is denser and contains more neurons (Mechelli et al., 2004). Furthermore, the increased density is stronger in those individuals who are most proficient in their second language and who learned the second language earlier. Thus, research does not support any long-term harm in development when learning a second language, and research shows either a benefit or no difference in cognitive abilities between bilingual and monolingual speakers. Adult Language Acquisition As the previous sections showed, language learning is not usually difficult or effortful for young children. As long as they’re in an environment where they have access to language used by adults, they’ll learn it pretty quickly. One reason it’s so easy for little ones is because of their neural plasticity: their brains are super-keen to make new connections in response to their experiences. The older you get, the harder it is for your brain to grow new neural pathways, so the harder it is to learn new things. It’s not impossible by any means, it just takes more effort! On the other hand, adults have some advantages over children. We have metacognitive skills we can apply consciously to language learning, such as memorizing new vocabulary and morphology, choosing to study a little bit each day, or seeking out books, movies, podcasts or other media in our new language. We also have metalinguistic awareness that we can employ. For example, we can consciously practice placing our articulators in new positions, or we can compare and contrast the syntactic structures in our new language with those from our L1 (first language). Another factor that can make language learning harder for adults than for children is our self-consciousness. Learning a new language usually involves interacting with other people, including other people who are more proficient than us. If you’re adult who’s used to being seen as competent, it can be embarrassing to feel like a beginner and to make mistakes. Perhaps focusing on narrowly on things like grammar when discussing adult second language acquisition is a mistake. Some critics say this approach to teaching and learning treats language as something that can acquired in isolation from people, communities, and relationships, and reflects colonial ways of thinking (Czaykowska-Higgins et al., 2017; Lukaniec &amp; Palakurthy, 2022; MacKenzie et al., 2022). Importantly, adult language learners bring all kinds of cultural knowledge, traditions, expectations, and emotions to language learning, of which we may or may not be consciously aware. Research Focus: When can we best learn language? Testing the critical period hypothesis For many years psychologists assumed that there was a critical period (a time in which learning can easily occur) for language learning, lasting between infancy and puberty, and after which language learning was more difficult or impossible (J. R. Brown, 1967; Penfield &amp; Roberts, 1959). But later research provided a different interpretation. An important study by J. S. Johnson &amp; Newport (1989) using Chinese and Korean speakers who ahd learned English as a second language provided the first insight. The participants were all adults who had immigrated to the United States between three and 39 years of age and who were tested on their English skills by being asked to detect grammatical errors in sentences. Johnson and Newport found that the participants who had begun learning English before they were seven years old learned it as well as native English speakers but that the ability to learn English dropped off gradually for the participants who had started later. Newport and Johnson also found a correlation between the age of acquisition and the variance in the ultimate learning of the language. While early learners were almost all successful in acquiring their language to a high degree of proficiency, later learners showed much greater individual variation. Johnson and Newport’s finding that children who immigrated before they were seven years old learned English fluently seemed consistent with the idea of a critical period in language learning. But their finding of gradual decrease in proficiency for those immigrated between eight and 39 years of age was not - rather, it suggested that there might not be a single critical period of language learning that ended at puberty, as early theorists had expected, but that language learning at later ages is imply better when it occurs earlier. This idea was reinforced in research by Hakuta et al. (2003), who examined census records of language learning in millions of Chinese and Spanish immigrants. The census form asks respondents to describe their own English ability using one of five categories: not at all, not well, well, very well, and speak only English. The results of this research dealt another blow to the idea of the critical period, because it showed that regardless of what year was used as a cutoff point for the end of the critical period, there was no evidence for any discontinuity in language-learning potential. Rather, the results (Figure 8.5) showed that the degree of success in second-language acquisition declined steadily throughout the respondent’s life span. The difficulty of learning language as one gets older is probably due to the fact that, with age, the brain loses its [plasticity] - that is, its ability to develop new neural connections. Figure 8.5: English Proficiency in Native Chinese Speakers. Hakuta et al. found no evidence for critical periods in language learning. Regardless of level of education, self-reported second-language skills decreased consistently across age of immigration. 8.4 Language and Thought Researchers have been asking for decades whether the specific language(s) you speak affects the way you think. This hypothesis is often called the Sapir-Whorf Hypothesis (Sapir &amp; Whorf, 1956). There are two versions of this theory: a strong version and a weak version. The strong version, called linguistic determinism, is that the language you speak will determine, or limit, the way that you think. The weak version, called linguistic relativity, is the hypothesis that people who speak different languages think differently as a result, or that language influences your thought but does not limit it. While there is little evidence for linguistic determinism, there are a variety of studies that support the conclusion that language influences thought, though indirectly via mechanisms like attention. One of these studies is about the perception of color. Different languages have different color terms. Russian has two words for the range of colors we call ‘blue’ in English. One corresponds to light blues, and the other to dark blues. As a result, Russian speakers are better than English speakers at discriminating between shades of blue, when one fell into the light blue category, and the other the dark blue category (Winawer et al., 2007). Another piece of evidence comes from grammatical gender. If you have studied a language like French or Spanish, you know that some language categorize nouns into groups, called grammatical genders. So every noun will be either masculine or feminine. (Semenuks et al., 2017) found that speakers tended to use feminine-associated words to describe objects that had feminine grammatical gender. For example, if bridge is feminine in French, speakers would be more likely to describe it using an adjective with the gender associations as the gender of the noun. So, the grammatical groups that nouns fall into affects how you might describe those nouns. These examples show how language may - even in quite a limited way - influence the way we think about these things. Key Takeaways Language involves both the ability to comprehend spoken and written words and to speak and write. Some languages are sign languages, in which the communication is expressed by movements of the hands. Phonemes are the elementary sounds of our language, morphemes are the smallest units of meaningful language, syntax is the grammatical rules that control how words are put together, and contextual information is the elements of communication that help us understand its meaning. Recent research suggests that there is not a single critical period of language learning, but that language learning is simply better when it occurs earlier. Language learning begins even before birth. An infant usually produces his or her first words at about one year of age. Although other animals communicate and may be able to express ideas, only the human brain is complex enough to create real language. Our language may have some influence on our thinking, but it does not affect our underlying understanding of concepts. Exercises What languages do you speak? Did you ever try to learn a new one? What problems did you have when you did this? Would you consider trying to learn a new language? Some animals, such as Kanzi, display at least some language. Is it accurate to say that Kanzi knows language? 8.5 Glossary babbling Babbling consists of intentional vocalizations that lack specific meaning produced by young children when they are learning language. bilingualism Bilingualism refers to the state of being able to speak two languages. critical period A time at which learning can easily occur. dialect A dialect is a subdivision, or version, of a language that may be associated with a geographical location or socioeconomic group. idiolect An idiolect is the version of a language that a speaker speaks, which is unique to that speaker. language comprehension the process of decoding language back into thoughts language production the process of converting thoughts into language linguistic determinism The claim that the language you speak will determine, or limit, the way that you think. linguistic relativity The hypothesis that people who speak different languages think differently as a result, or that language influences your thought but does not limit it. morpheme A morpheme is the smallest unit of language that carries meaning. overextensions Overextentions are when a learning of a language - often a child - uses a word in a broader context than appropriate. phoneme A phoneme is a unit of sound that can distinguish one word from another in a particular language. pragmatics Pragmatics refers to the meaning of words and sentences in context, where the surrounding language may change the meaning of the word or sentence. semantics Semantics is the meaning of words and sentences. Usually this refers to the definitional meaning, like you might find in a dictionary. sociolect A sociolect is a version of a language that is associated with a particular social group. syntax Syntax is the list of rules that allows you to construct grammatical sentences in a given language. References Andrews, I. (1982). Bilinguals out of focus: A critical discussion. International Review of Applied Linguistics in Language Teaching, 20(4), 297–305. Baldwin, D. A. (1993). Early referential understanding: Infants’ ability to recognize referential acts for what they are. Dev. Psychol., 29(5), 832–843. Bialystok, E. (2009). Bilingualism: The good, the bad, and the indifferent. Biling. (Camb. Engl.), 12(1), 3–11. Boysson-Bardies, B. de, Sagart, L., &amp; Durand, C. (1984). Discernible differences in the babbling of infants according to target language. Journal of Child Language, 11(1), 1–15. Brown, J. R. (1967). Biological foundations of language. Neurology, 17(12), 1219–1219. Caselli, N., Pyers, J., &amp; Lieberman, A. M. (2021). Deaf children of hearing parents have age-level vocabulary growth when exposed to american sign language by 6 months of age. The Journal of Pediatrics, 232, 229–236. Czaykowska-Higgins, E., Burton, S., McIvor, O., &amp; Marinakis, A. (2017). Supporting indigenous language revitalisation through collaborative postsecondary proficiency-building curriculum. De Waal, F. B. (1989). Peacemaking among primates. Harvard University Press. Dick, A. S., Garcia, N. L., Pruden, S. M., Thompson, W. K., Hawes, S. W., Sutherland, M. T., Riedel, M. C., Laird, A. R., &amp; Gonzalez, R. (2019). No evidence for a bilingual executive function advantage in the ABCD study. Nature Human Behaviour, 3(7), 692–701. Dobrich, W., &amp; Scarborough, H. S. (1992). Phonological characteristics of words young children try to say. J. Child Lang., 19(3), 597–616. Green, L. (2002). A descriptive study of african american english: Research in linguistics and education. International Journal of Qualitative Studies in Education, 15(6), 673–690. Greenfield, P. M., &amp; Savage-Rumbaugh, E. S. (1991). Imitation, grammatical development, and the invention of protogrammar by an ape. In N. A. Krasnegor, D. M. Rumbaugh, R. L. Schiefelbusch, &amp; &amp;. M. Studdert-Kennedy (Eds.), Biological and behavioral determinants of language development (pp. 235–258). Lawrence Erlbaum Associates. Hakuta, K., Bialystok, E., &amp; Wiley, E. (2003). Critical evidence: A test of the critical-period hypothesis for second-language acquisition. Psychol. Sci., 14(1), 31–38. Hall, M. L., Hall, W. C., &amp; Caselli, N. K. (2019). Deaf children need language, not (just) speech. First Language, 39(4), 367–395. Hall, W. C. (2017). What you don’t know can hurt you: The risk of language deprivation by impairing sign language development in deaf children. Maternal and Child Health Journal, 21(5), 961–965. Humphries, T., Kushalnagar, P., Mathur, G., Napoli, D. J., Padden, C., Rathmann, C., &amp; Smith, S. (2016). Avoiding linguistic neglect of deaf children. Social Service Review, 90(4), 589–619. Johnson, J. S., &amp; Newport, E. L. (1989). Critical period effects in second language learning: The influence of maturational state on the acquisition of english as a second language. Cogn. Psychol., 21(1), 60–99. Lillo-Martin, D., &amp; Henner, J. (2021). Acquisition of sign languages. Annual Review of Linguistics, 7, 395. Lukaniec, M., &amp; Palakurthy, K. (2022). Additional language learning in the context of indigenous language reclamation. In The routledge handbook of second language acquisition and sociolinguistics (pp. 341–355). Routledge. MacKenzie, A., Engman, M., &amp; McGurk, O. (2022). Overt and symbolic linguistic violence: Plantation ideology and language reclamation in northern ireland. Teaching in Higher Education, 27(4), 489–501. Mandel, D. R., Jusczyk, P. W., &amp; Pisoni, D. B. (1995). Infants’ recognition of the sound patterns of their own names. Psychol. Sci., 6(5), 314–317. Mauldin, L. (2019). Don’t look at it as a miracle cure: Contested notions of success and failure in family narratives of pediatric cochlear implantation. Social Science &amp; Medicine, 228, 117–125. Mechelli, A., Crinion, J. T., Noppeney, U., O’doherty, J., Ashburner, J., Frackowiak, R. S., &amp; Price, C. J. (2004). Structural plasticity in the bilingual brain: Proficiency in a second language and age at acquisition affect grey-matter density. Nature. Mitchell, R. E., &amp; Karchmer, M. (2004). Chasing the mythical ten percent: Parental hearing status of deaf and hard of hearing students in the united states. Sign Language Studies, 4(2), 138–163. Moon, C., Cooper, R. P., &amp; Fifer, W. P. (1993). Two-day-olds prefer their native language. Infant Behav. Dev., 16(4), 495–500. Nicoladis, E., &amp; Genesee, F. (1997). Language development in preschool bilingual children. Journal of Speech-Language Pathology and Audiology, 21(4), 258–270. NIDCD, N. (2021). Cochlear implants. National Institute on Deafness; Other Communication Disorders. Oller, D. K., &amp; Pearson, B. Z. (2002). Chapter 1: Assessing the effects of bilingualism: A background. In K. Oller &amp; R. Eilers (Eds.), Language and literacy in bilingual children (pp. 3–21). Multilingual Matters. Penfield, W., &amp; Roberts, L. (1959). Speech &amp; brain mechanisms princeton, NJ princeton uni. Press. Petitto, L. A., &amp; Marentette, P. F. (1991). Babbling in the manual mode: Evidence for the ontogeny of language. Science, 251(5000), 1493–1496. Pickering, M. J., &amp; Ferreira, V. S. (2008). Structural priming: A critical review. Psychological Bulletin, 134(3), 427. Pines, M. (1981). The civilizing of genie. Psychology Today, 15(9), 28–34. Saffran, J. R., Aslin, R. N., &amp; Newport, E. L. (1996). Statistical learning by 8-month-old infants. Science, 274(5294), 1926–1928. Sapir, E., &amp; Whorf, B. (1956). Language, thought, and reality. Selected Writings. Savage-Rumbaugh, S., &amp; Lewin, R. (1994). Kanzi: The ape at the brink of the human mind. John Wiley &amp; Sons. Semenuks, A., Phillips, W., Dalca, I., Kim, C., &amp; Boroditsky, L. (2017). Effects of grammatical gender on object description. CogSci. Seyfarth, R. M., &amp; Cheney, D. L. (1997). Behavioral mechanisms underlying vocal communication in nonhuman primates. Anim. Learn. Behav., 25(3), 249–267. Von Frisch, K. (1950). Bees: Their vision, chemical senses, and language. Cornell University Press. Waxman, S. R. (1990). Linguistic biases and the establishment of conceptual hierarchies: Evidence from preschool children. Cogn. Dev., 5(2), 123–150. Werker, J. F., &amp; Tees, R. C. (2002). Cross-language speech perception: Evidence for perceptual reorganization during the first year of life. Infant Behav. Dev., 25(1), 121–133. Winawer, J., Witthoft, N., Frank, M. C., Wu, L., Wade, A. R., &amp; Boroditsky, L. (2007). Russian blues reveal effects of language on color discrimination. Proceedings of the National Academy of Sciences, 104(19), 7780–7785. "],["reasoning-and-decision-making.html", "Chapter 9 Reasoning and Decision Making 9.1 Deductive reasoning 9.2 Inductive reasoning 9.3 Decision making", " Chapter 9 Reasoning and Decision Making We like to think that we make important decisions rationally, logically, and without bias or error— but what if that’s not the case? Let us consider the following scene of Knut’s life: It is a rainy summer afternoon in Germany and Knut and his wife are tired of watching the black crows in their garden. They decide to escape from the dreary weather and take a vacation to Spain, as Knut and his wife have never been there before. They will leave the next day, and he is packing his bag. He packs the crucial things first: underwear, socks, pajamas, and a toiletry bag with a toothbrush, shampoo, soap, sun tan lotion, and bug spray. Knut cannot find the bug spray, and his wife volunteers to go buy a new bottle. He advises her to take an umbrella for the walk to the pharmacy as it is raining outside, and then he turns back to packing. But what did he already pack into his bag? Immediately, he remembers and continues, putting together outfits and packing his clothing. Since it is summer, Knut packs mostly shorts and t-shirts. After half an hour, he is finally convinced that he has done everything necessary for a nice vacation. With this story of Knut’s vacation preparation, we will explain the basic principles of reasoning and decision making. We will demonstrate how much cognitive work is necessary for even this fragment of everyday life. In reasoning, available information is taken into account in the form of premises. A conclusion is reached on the basis of these premises through a process of inference. The content of the conclusion goes beyond either one of the premises. To demonstrate, consider the following consideration Knut makes before planning his vacation: Premise 1: In all countries in southern Europe it is warm during summer. Premise 2: Spain is a country in southern Europe. Conclusion: Therefore, in Spain it is warm during summer. The conclusion in this example follows directly from the premises, but it includes information that is not explicitly stated in the premises. This is a typical feature of a process of reasoning. We will discuss the two major kinds of reasoning, inductive reasoning and deductive reasoning, which logically complement of one another. 9.1 Deductive reasoning LEARNING OBJECTIVES Understand deductive and inductive reasoning Understand the main theories of decision making, as well as examples of how people’s choice contexts affect the decisions they make Deductive reasoning is concerned with syllogisms in which the conclusion follows logically from the premises. The following example about Knut makes this process clear: Premise: Knut knows: If it is warm, one needs shorts and t-shirts. Premise: He also knows that it is warm in Spain during summer. Conclusion: Therefore, Knut reasons that he needs shorts and t-shirts in Spain. In this example it is obvious that the premises are about rather general information and the resulting conclusion is about a more special case which can be inferred from the two premises. We will now differentiate between the two major kinds of syllogisms: categorical and conditional syllogisms. Categorical syllogisms In categorical syllogisms, the statements of the premises typically begin with “all”, “none” or “some” and the conclusion starts with “therefore,” “thus,” or “hence.” These kinds of syllogisms describe a relationship between two categories. In the example given above in the introduction of deductive reasoning these categories are Spain and the need for shorts and T-Shirts. Two different approaches serve the study of categorical syllogisms: the “normative approach” and the “descriptive approach”. The normative approach The normative approach to categorical syllogisms is based on logic, and deals with the problem of categorizing conclusions as either valid or invalid. “Valid” means that the conclusion follows logically from the premises whereas “invalid” means the contrary. Two basic principles and a method called Euler Circles have been developed to help make validity judgments. The first principle was created by Aristotle, and states “If the two premises are true, the conclusion of a valid syllogism must be true” (Goldstein, 2005). The second principle states “The validity of a syllogism is determined only by its form, not its content.” These two principles explain why the following syllogism is (surprisingly) valid: Premise 1: All flowers are animals. Premise 2: All animals can jump. Conclusion: Therefore, all flowers can jump. Even though it is quite obvious that the first premise is not true and further that the conclusion is not true, the whole syllogism is still valid. That is, when you apply formal logic to the syllogism in the example, the conclusion is valid. It is possible to display a syllogism formally with symbols or letters and explain its relationship graphically with the help of diagrams. One way to demonstrate a premise graphically is to use Euler circles (pronounced “oyler”). Starting with a circle to represent the first premise and adding one or more circles for the second one (Figure 9.1), one can compare the constructed diagrams with the conclusion. The displayed syllogism in Figure 9.1 is obviously valid. The conclusion shows that everything that can jump contains animals which again contains flowers. This aligns with the two premises which point out that flowers are animals and thus are able to jump. Euler circles help represent such logic. Figure 9.1: Euler Circles The descriptive approach The descriptive approach is concerned with estimating people’s ability to judge the validity of syllogisms and explaining errors people make. This psychological approach uses two methods in order to study people’s performance: Method of evaluation: People are given two premises and a conclusion. Their task is to judge whether the syllogism is valid. Method of production: Participants are given two premises. Their task is to develop a logically valid conclusion. In addition to the form of a syllogism, the content can influence a person’s decision and cause the person to neglect logical thinking. The belief bias states that people tend to judge syllogisms with believable conclusions as valid, while they tend to judge syllogisms with unbelievable conclusions as invalid. Given a conclusion as like “Some bananas are pink”, hardly any participants would judge the syllogism as valid, even though it might be logically valid according to its premises (e.g. Some bananas are fruits. All fruits are pink.) Conditional syllogisms Another type of syllogism is called “conditional syllogism.” Just like the categorical syllogisms, they also have two premises and a conclusion. The difference is that the first premise has the form “If … then”. Syllogisms like this one are common in everyday life. Consider the following example from the story about Knut: Premise 1: If it is raining, Knut’s wife needs an umbrella. Premise 2: It is raining. Conclusion: Therefore, Knut’s wife needs an umbrella. Conditional syllogisms are typically given in the abstract form: “If p then q”, where “p” is called the “antecedent” and “q” the “consequent”. Forms of conditional syllogisms There are four major forms of conditional syllogisms: “modus ponens”, “modus tollens”, “denying the antecedent”, and “affirming the consequent”. These are illustrated in the table below (9.1) by means of the conditional syllogism above (i.e. If it is raining, Knut’s wife needs an umbrella). The table indicates the premises, the resulting conclusions and whether the form is valid. The bottom row displays the how frequently people correctly identify the validity of the syllogisms. Table 9.1: Different kinds of conditional syllogisms. Modus Ponens Modus Tollens Denying the Antecedent Affirming the Consequent Description The antecedent of the first premise is affirmed in the second premise. The consequent of the first premise is negated in the second premise. The antecedent of the first premise is negates in the second premise. The antecedent of the first premise is affirmed in the second premise. Formal If P then Q. P Therefore Q. If P the Q. Not-Q Therefore Not-P. If P then Q. Not-P Therefore Not-Q. If P then Q. Q Therefore P. Example If it is raining, Knut’s wife needs an umbrella. It is raining. Therefore Knut’s wife needs an umbrella. If it is raining, Knut’s wife needs an umbrella. Knut’s wife does not need an umbrella. Therefore it is not raining. If it is raining, Knut’s wife needs an umbrella. It is not raining. Therefore Knut’s wife does not need an umbrella. If it is raining, Knut’s wife needs an umbrella. Knut’s wife needs an umbrella. Therefore it is raining. Validity Valid Valid Invalid Invalid Correct Judgements 97 % correctly identify as valid. 60% correctly identify as valid. 40% correctly identify as invalid. 40% correctly identify as invalid. As we can see, the validity of the syllogisms with valid conclusions is easier to judge correctly than the validity of the syllogisms with invalid conclusions. The conclusion in the instance of the modus ponens is apparently valid. In the example it is very clear that Knut’s wife needs an umbrella if it is raining. The validity of the modus tollens is more difficult to recognize. Referring to the example, if Knut’s wife does not need an umbrella it can’t be raining. The first premise says that if it is raining, she needs an umbrella. So the reason for Knut’s wife not needing an umbrella is that it is not raining. Consequently, the conclusion is valid. The validity of the remaining two kinds of conditional syllogisms is judged correctly by only 40% of people. If the method of denying the antecedent is applied, the second premise says that it is not raining. But from this fact it does not follow logically that Knut’s wife does not need an umbrella— she could need an umbrella for another reason, such as to shield from the sun. Affirming the consequent in the case of the given example means that the second premise says that Knut’s wife needs an umbrella, but again the reason for this can be circumstances apart from rain. So, it does not logically follow that it is raining. Therefore, the conclusion of this syllogism is invalid. The four kinds of syllogisms have shown that it is not always easy to make correct judgments concerning the validity of the conclusions. The following passages will deal with other errors people make during the process of conditional reasoning. The Wason Selection Task The Wason Selection Task is a famous experiment which shows that people make more reasoning errors in abstract situations than when the situation is taken from real life (Wason, 1960). In the abstract version of the Wason Selection Task, four cards are shown to the participants with a letter on one side and a number on the other (Figure 9.2). The task is to indicate the minimum number of cards that have to be turned over to test whether the following rule is observed: “If there is a vowel on one side then there is an even number on the other side.” 53% of participants selected the ‘E’ card which is correct, because turning this card over is necessary to test the truth of the rule. However another card still needs to be turned over. 64% indicated that the ‘4’ card has to be turned over which is not right. Only 4% of participants answered correctly that the ‘7’ card needs to be turned over in addition to the ‘E’. The correctness of turning over these two cards becomes more obvious if the same task is stated in terms of real-world items instead of vowels and numbers. One of the experiments for determining this was the beer/drinking-age problem used by Griggs &amp; Cox (1982). This experiment is identical to the Wason Selection Task except that instead of numbers and letters on the cards, everyday terms (beer, soda and ages) were used (Figure 9.3). Griggs and Cox gave the following rule to participants: “If a person is drinking beer then he or she must be older than 21.” In this case 73% of participants answered correctly, that the cards with “beer” and “14 years” have to be turned over to test whether the rule is kept. Figure 9.2: Original Wason Selection Task cards Figure 9.3: Drinking-age Wason Selection Task Why is the performance better in the case of real–world items? There are two different approaches to explain why participants’ performance is significantly better in the case of the beer/drinking-age problem than in the abstract version of the Wason Selection Task: the permission schemas approach and the evolutionary approach. The rule, “if a person is 21 years old or older then they are allowed to drink alcohol,” is well-known as an experience from everyday life. Based on a lifetime of learning rules in which one must satisfy some criteria for permission to perform a specific act, we have a “permission schema” already stored in long-term memory to think about such situations. Participants can apply this previously-learned permission schema to the Wason Selection Task for real–world items to improve participants’ performance. On the contrary such a permission schema from everyday life does not exist for the abstract version of the Wason Selection Task (Cheng &amp; Holyoak, 1985; Griggs &amp; Cox, 1983). The evolutionary approach concerns the human ability of “cheater detection”. This approach states that an important aspect of human behavior across our evolutionary history is the ability for people to cooperate in a way that is mutually beneficial. As long as a person who receives a benefit also pays the relevant cost, everything works well in a social exchange. If someone cheats, however, and receives a benefit from others without paying the cost, problems arise. It is assumed that the ability to detect cheaters became a part of the human cognitive makeup during evolution. This cognitive ability improves the performance in the beer/drinking-age version of the Wason Selection Task as it allows people to detect a cheating person who does not behave according to the rule. Cheater-detection does not work in the case of the abstract version of the Wason Selection Task as vowels and numbers cannot behave in any way, much less cheat, and so the cheater detection mechanism is not activated (Cosmides, 1989; Gigerenzer &amp; Hug, 1992). 9.2 Inductive reasoning So far we have discussed deductive reasoning, which is reaching conclusions based on logical rules applied to a set of premises. However, many problems cannot be represented in a way that would make it possible to use these rules to come to a conclusion. Inductive reasoning is the process of making observations and applying those observations via generalization to a different problem. Therefore one infers from a special case to the general principle, which is just the opposite of the procedure of deductive reasoning (9.4). A good example of inductive reasoning is the following: Premise: All crows Knut and his wife have ever seen are black. Conclusion: Therefore, they reason that all crows on earth are black. In this example it is obvious that Knut and his wife infer from the simple observation about the crows they have seen to the general principle about all crows. Considering Figure 9.4, this means that they infer from the subset (yellow circle) to the whole (blue circle). As with this example, it is typical in inductive reasoning that the premises are believed to support the conclusion, but do not ensure the conclusion. Figure 9.4: An example of inductive reasoning would be generalizing from the subset of crows you have seen to all crows. Forms of inductive reasoning The two different forms of inductive reasoning are “strong” and “weak” induction. The former indicates that the truth of the conclusion is very likely if the assumed premises are true. An example for this form of reasoning is the one given in the previous section. In this case it is obvious that the premise (“All crows Knut and his wife have ever seen are black”) gives good evidence for the conclusion (“All crows on earth are black”) to be true. Nevertheless it is still possible, although very unlikely, that not all crows are black. On the contrary, conclusions reached by “weak induction” are supported by the premises in a relatively weak manner. In this approach, the truth of the premises makes the truth of the conclusion possible, but not likely. An example for this kind of reasoning is the following: Premise: Knut always listens to music with his iPhone. Conclusion: Therefore, he reasons that all music is only heard with iPhones. In this instance the conclusion is obviously false. The information the premise contains is not very representative and although it is true, it does not give decisive evidence for the truth of the conclusion. To sum it up, strong inductive reasoning yields conclusions which are very probable whereas the conclusions reached through weak inductive reasoning are unlikely to be true. Reliability of conclusions If the strength of the conclusion of an inductive argument has to be determined, three factors concerning the premises play a decisive role. The of Knut and his wife’s observations about crows (see previous sections) displays these factors: When Knut and his wife observe, in addition to the black crows in Germany, the crows in Spain, the number of observations they make concerning the crows increases. Furthermore, the representativeness of these observations is supported if Knut and his wife observe the crows at different times and in different places and see that they are black every time. The quality of the evidence for all crows being black increases if Knut and his wife add scientific measurements that support the conclusion. For example, they could find out that the crows’ genes determine that the only color they can be is black. Conclusions reached through a process of inductive reasoning are never definitely true, as no one has seen all crows on earth. It is possible, although very unlikely, that there is a green or brown exemplar. The three above factors contribute to the strength of an inductive argument. The stronger these factors are, the more reliable the conclusions reached through induction. Processes and constraints In the process of inductive reasoning people often make use of heuristics. These heuristics help people make judgments, but sometimes cause errors when the cues they are sensitive to are misleading. In the following sections, two of these heuristics, the availability heuristic and the representativeness heuristic, are explained. Subsequently, confirmation bias is introduced, which refers to when people rely too heavily on their own prior beliefs when assessing new evidence. The availability heuristic The availability heuristic refers to when people judge more memorable events (or traits, classes of objects, etc.) to be more frequent than less memorable events. In Kahneman and Tversky’s paper introducing the heuristic, they describe that “Availability is an ecologically valid clue for the judgment of frequency because, in general, frequent events are easier to recall or imagine than infrequent ones. However, availability is also affected by various factors which are unrelated to actual frequency” (Tversky &amp; Kahneman, 1973). In other words, while an event’s memorability is often a good cue as to how frequent that event is, this cue is imperfect. For example, in an experiment done by Sarah Lichtenstein and colleagues, participants were asked to choose from a list which causes of death occur most often. Because of the availability heuristic, people judged more “spectacular” causes like homicide or tornadoes to cause more deaths than less dramatic causes, like asthma. They propose that this is because dramatic causes of death are disproportionately publicized in media, resulting in easier recall for participants and thus erroneously higher estimates of frequency (Lichtenstein et al., 1978). The representativeness heuristic Similarly to the availability heuristic, the representativeness heuristic is used when people judge frequency. However in this case, people rely on a judgment of similarity instead of using memorability as a cue. For example, Tversky &amp; Kahneman (1983) gave the following description to a group of participants: “Linda is 31 years old, single, outspoken and very bright. She majored in philosophy. As a student, she was deeply concerned with issues of discrimination and social justice, and also participated in anti-nuclear demonstrations.” They then asked participants to rank a series of descriptions in order of how likely they were to describe Linda. Most participants rated “Linda is a bank teller and is active in the feminist movement” as more likely than “Linda is a bank teller”. However, this is logically impossible, as the first description includes the second (Figure 9.5). Because the participants based their judgment on similarity between the description of Linda and being “active in the feminist movement”, rather than reasoning in line with the formal laws of logic, most committed this “conjunction fallacy”. Like the availability heuristic though, the representativeness heuristic is often accurate and useful. Tversky and Kahneman write: “Representativeness tends to covary with frequency: Common instances and frequent events are generally more representative than unusual instances and rare events” (Tversky &amp; Kahneman, 1983). Figure 9.5: Feminist bank tellers. Confirmation bias Confirmation bias refers to a set of related phenomena wherein people seek or interpret evidence in a way that aligns with beliefs they already hold. This can take the form of overweighting evidence that supports one’s belief, underweighting evidence that opposes one’s belief, using an information search strategy that is more likely to reveal supportive evidence, remembering only evidence that aligns with one’s beliefs, and more (Nickerson, 1998b). It is important to note that with most forms of confirmation bias, psychologists are not referring to intentional, goal-directed reasoning, such as what a lawyer defending a case might engage in. Rather, these phenomena are typically understood to be unintentional, at least at a conscious level. For example, if someone believes that black cats bring bad luck, they may be additionally inclined to notice or remember negative events that occurred after seeing a black cat. Induction vs. deduction Figure 9.6: Deductive and inductive reasoning. The table below (9.2) summarizes the most prevalent properties and differences between deductive and inductive reasoning which are important to keep in mind. Table 9.2: Induction vs. deduction Deductive Reasoning Inductive Reasoning Premises Stated as facts or general principles (“It is warm in the Summer of Spain.”) Based on observations of specific cases (“All crows Knut and his wife have seen are back.”) Conclusion Conclusion is more special than the information the premises provide. It is reached directly by applying logical rules to the premises. Conclusion is more general than the information the premises provide. It is reached by generalizing the premises` information. Validity If the premises are true, the conclusions must be true. If the premises are true, the conclusion is probably true. Usage More difficult to use (mainly in logical problems). One needs facts which are definitely true. Used often in everyday life (fast and easy). Evidence is used instead of proved facts. 9.3 Decision making The psychological processes of decision making are critical in everyday life. Imagine Knut deciding whether he should pack his heavy umbrella because of a forecasted “20% chance of rain” or selecting between two different types of travel insurance for his vacation. There are two main ways to describe and analyze decision making. The “normative” approach describes how decision makers ought to make decisions. That is, it describes how people should behave if they were perfectly rational and had well-defined preferences. Here, “rational” typically means that one adheres to the laws of probability and formal logic. The “descriptive” approach to studying decision making describes how real-life decision makers actually make decisions, rather than how they should. A common finding among decision making researchers is that people do not typically follow what a perfectly rational actor would do, though this claim is contentious and widely debated within the field. Theories of Decision Making Expected Value Theory: In the earliest days of probability theory, theorists used Expected Value Theory (EVT) as a normative account. Under EVT, the value of an option is calculated as the probability that option will occur, multiplied by the quantitative value of that option. For example, if a casino offers a game in which there as an 80% chance of winning $10, the “expected value” is .8 x 10 = $8. Because EVT is normative, people should be willing to pay up to $7.99 in order to play that game, because the expected value ($8) is higher than the cost (&lt; $8). However, EVT has several issues as a normative theory. One sizeable issue is that most decisions do not have quantitative outcomes. For example, when deciding whether to take an umbrella due to a 20% chance of rain, there is no clear number that expresses how good or bad it would be to get caught in the rain! A second sizeable issue is that some decisions can have infinitely large expected values but have infinitely small probabilities of occurring, and people are typically uninterested in taking those gambles, despite EVT suggesting they should. This was first demonstrated in a game called the St. Petersburg Paradox, described in box 9.4.1. St. Petersburg Paradox A classic paradox dating back to 1713, the St. Petersburg Paradox, describes a game in which a fair coin is flipped repeatedly until it lands on heads, at which point the game ends. The number of times the coin was flipped (including the final heads) is the variable k, and the player wins \\(2^k\\) dollars. How much would you be willing to pay to play this game? Most people are unwilling to pay more than ten dollars. However, you might notice that the expected value of this game is infinite, because hypothetically the coin could never land on heads, though it’s incredibly unlikely. Needless to say, a game with infinite expected value that most people would not pay ten dollars for presents a problem for Expected Value Theory. Expected Utility Theory: Due to these issues, most modern theorists believe that Expected Utility Theory (EUT) is a better normative account of decision making. Under EUT, “utility” instead of “value” is multiplied by a probability. Utility is a more general term that refers to how subjectively good we believe an outcome to be, and “disutility” refers to how subjectively bad we believe an outcome to be. For example, if Knut hates to be caught in the rain, he might multiply the chance of rain, 20%, by how much disutility the rain gives him, perhaps -60 “utils” (negative because it’s disutility here). Utils are best thought of as a hypothetical measure of satisfaction with some outcome. Here, Knut’s expected utility from the rain is .2 x -60 utils = -12 utils. If Knut knew that packing his heavy umbrella gave him -20 utils, he would decide to forgo packing the umbrella because the expected disutility of carrying the heavy umbrella is more negative than the expected disutility of getting caught in the rain. EUT also accounts for the fact that people typically experience diminishing marginal utility. This is the observation that the utility of a gain is proportional to how much of that good you already have. For example, if you have $100 dollars and somebody gives you $5,000, you will experience much more gain in utility than if you have $100,000 and somebody gives you that same $5,000. Diminishing marginal utility helps resolve the issue with EVT demonstrated by the St. Petersburg Paradox because the extremely high potential values cease to be meaningful (if you have $100 billion, the next dollar you earn is meaningless). Prospect Theory: By having participants in experiments report their willingness to play various gambles, Daniel Kahneman and Amos Tversky measured how people actually weigh value and probability, which they called Prospect Theory (Kahneman &amp; Tversky, 1979). The major findings of Prospect Theory (PT), called the probability weighting function and the value function, are shown in Figure 9.7. The value function shows that, on average, people show diminishing marginal (dis)utility for large gains or large losses, and that losses typically have greater disutility than comparably sized gains have utility (hence the “kink” at zero). The probability weighting function shows that, on average, people tend to overweight small probabilities (i.e., behave as though they have higher probabilities than they actually do), and underweight large probabilities (i.e., behave as though they have lower probabilities than they actually do). Figure 9.7: a) value weighting function and b) probability weighting function, both from Prospect Theory. v(.) refers to subjective utility, and w(.) refers to subjective probability. Constructed Preferences Contrary to theorists’ claim that rational decision makers always have well-defined preferences, real-life decision makers tend to make different selections depending on how choices are presented to them. Two common examples of this are default effects and framing effects. Default Effects: The default effect refers to the tendency for decision makers to choose a pre-selected option, rather than select non-pre-selected options in a particular choice context. A classic example of this effect concerns organ donor rates across different countries. In some countries, such as Austria and France, citizens are enrolled to be organ donors by default. That is, unless a citizen actively selects to not be an organ donor, they will be presumed to be one. In other countries, such as the United States, citizens are presumed not to be organ donors unless they actively select to be one. In countries like Austria and France, organ donor rates are typically as high as 99% (E. J. Johnson &amp; Goldstein, 2003). As of 2022, organ donor rates in the United States are approximately 54%. This large gap is partially due to the different default option in each country. Default effects are thought to occur for three main reasons (Dinner et al., 2011; Jachimowicz et al., 2019). First, people are sometimes inattentive when making selections (Choi et al., 2002). Second, because people often perceive losses as worse than comparable gains (as discussed in the Prospect Theory section), the losses associated with switching away from a particular option may feel more significant than the gains associated with a new option. Finally, people often perceive defaults as implicit recommendations (McKenzie et al., 2006). Framing Effects: Framing effects occur when logically equivalent descriptions of an option lead to different choices. There are a number of types of framing effects, with one of the simplest being attribute framing (Levin et al., 1998). For example, a doctor might describe that five years after undergoing a particular surgery, 90% of patients are still alive. A doctor could also describe that five years after that same surgery, 10% of patients had died, a logically equivalent description (one entails the other). In multiple experiments, people have been found to be much more willing to undergo riskier medical procedures when they are described in the “positive” frame (90%, here) than the “negative” frame (10%) (McNeil et al., 1982; Wilson et al., 1987). There is debate in the literature as to why framing effects occur, with varying perspectives on whether they are reasonable ways for decision makers to behave. Are more options better? People often have the intuition that more options are better, because there is a higher chance a decision maker can find something that suits their needs or preferences. Traditional economic theory agrees! However, psychological research reveals this is typically not the case. In one classic experiment, shoppers in a grocery store were asked to sample flavors of jam. Some shoppers were given 6 flavors to choose from, while others were given 24 flavors to choose from. Shoppers were far more likely to purchase a jar afterward if they had only 6 flavors to choose from. Subsequent studies suggest people are also more satisfied with their ultimate selections if they had fewer options to choose from (Iyengar &amp; Lepper, 2000). A recent meta-analysis found that this “choice overload” tends to be a function of decision task difficulty, choosers’ uncertainty in their preferences, choice set complexity, and the decision goal (Chernev et al., 2015). Key Takeaways Deductive reasoning describes when we draw a specific conclusion from general premises. Inductive reasoning describes the opposite process, where we draw general conclusions from specific premises. Normative and descriptive theories of decision making do not always align. People typically use heuristics (cognitive shortcuts) rather than formal logic and reasoning. These heuristics often help people arrive at sufficiently accurate conclusions, though they sometimes result in reasoning errors. Exercises Imagine a gamble with a 20% probability of winning $50. How much money does Expected Value Theory suggest you should be willing to pay to try this gamble? Describe default effects and framing effects. How are those psychological phenomena divergent from what a perfectly “rational” decision maker would do? References Cheng, P. W., &amp; Holyoak, K. J. (1985). Pragmatic reasoning schemas. Cognitive Psychology, 17(4), 391–416. Chernev, A., Böckenholt, U., &amp; Goodman, J. (2015). Choice overload: A conceptual review and meta-analysis. Journal of Consumer Psychology, 25(2), 333–358. Choi, J. J., Laibson, D., Madrian, B. C., &amp; Metrick, A. (2002). Defined contribution pensions: Plan rules, participant choices, and the path of least resistance. Tax Policy and the Economy, 16, 67–113. Cosmides, L. (1989). The logic of social exchange: Has natural selection shaped how humans reason? Studies with the wason selection task. Cognition, 31(3), 187–276. Dinner, I., Johnson, E. J., Goldstein, D. G., &amp; Liu, K. (2011). Partitioning default effects: Why people choose not to choose. Journal of Experimental Psychology: Applied, 17(4), 332. Gigerenzer, G., &amp; Hug, K. (1992). Domain-specific reasoning: Social contracts, cheating, and perspective change. Cognition, 43(2), 127–171. Goldstein, E. (2005). Cognitive psychology - connecting, mind research, and everyday experience. Thomson Wadsworth. Griggs, R. A., &amp; Cox, J. R. (1982). The elusive thematic-materials effect in wason’s selection task. British Journal of Psychology, 73(3), 407–420. Griggs, R. A., &amp; Cox, J. R. (1983). The effects of problem content and negation on wason’s selection task. Quarterly Journal of Experimental Psychology, 35(3), 519–533. Iyengar, S. S., &amp; Lepper, M. R. (2000). When choice is demotivating: Can one desire too much of a good thing? Journal of Personality and Social Psychology, 79(6), 995. Jachimowicz, J. M., Duncan, S., Weber, E. U., &amp; Johnson, E. J. (2019). When and why defaults influence decisions: A meta-analysis of default effects. Behavioural Public Policy, 3(2), 159–186. Johnson, E. J., &amp; Goldstein, D. (2003). Do defaults save lives? In Science (No. 5649; Vol. 302, pp. 1338–1339). American Association for the Advancement of Science. Kahneman, D., &amp; Tversky, A. (1979). Prospect theory: An analysis of decisions under risk. Econometrica, 47, 278. Levin, I. P., Schneider, S. L., &amp; Gaeth, G. J. (1998). All frames are not created equal: A typology and critical analysis of framing effects. Organizational Behavior and Human Decision Processes, 76(2), 149–188. Lichtenstein, S., Slovic, P., Fischhoff, B., Layman, M., &amp; Combs, B. (1978). Judged frequency of lethal events. Journal of Experimental Psychology: Human Learning and Memory, 4(6), 551. McKenzie, C. R., Liersch, M. J., &amp; Finkelstein, S. R. (2006). Recommendations implicit in policy defaults. Psychological Science, 17(5), 414–420. McNeil, B. J., Pauker, S. G., Sox Jr, H. C., &amp; Tversky, A. (1982). On the elicitation of preferences for alternative therapies. New England Journal of Medicine, 306(21), 1259–1262. Nickerson, R. S. (1998b). Confirmation bias: A ubiquitous phenomenon in many guises. Review of General Psychology, 2(2), 175–220. Tversky, A., &amp; Kahneman, D. (1973). Availability: A heuristic for judging frequency and probability. Cognitive Psychology, 5(2), 207–232. Tversky, A., &amp; Kahneman, D. (1983). Extensional versus intuitive reasoning: The conjunction fallacy in probability judgment. Psychological Review, 90(4), 293. Wason, P. C. (1960). On the failure to eliminate hypotheses in a conceptual task. Q. J. Exp. Psychol., 12(3), 129–140. Wilson, D. K., Kaplan, R. M., &amp; Schneiderman, L. J. (1987). Framing of decisions and selections of alternatives in health care. Social Behaviour, 2(1), 51–59. "],["problem-solving.html", "Chapter 10 Problem Solving 10.1 What is a problem? 10.2 Restructuring: The Gestalt Approach 10.3 Solving Problems by Analogy 10.4 Creativity 10.5 How do Experts Solve Problems? 10.6 Glossary", " Chapter 10 Problem Solving How do we achieve our goals when the solution is not immediately obvious? What mental blocks are likely to get in our way, and how can we leverage our prior knowledge to solve novel problems? LEARNING OBJECTIVES Distinguish between well-defined and ill-defined problems. Identify the importance of insight problems in understanding how humans solve problems. Describe impediments to problem solving such as functional fixedness. Distinguish between problem-solving processes for novices and experts. 10.1 What is a problem? The most basic definition of a problem is any given situation that differs from a desired goal. This definition is very useful for discussing problem solving in terms of evolutionary adaptation, as it allows us to understand every aspect of human or animal life as a problem. This includes issues like finding food in harsh winters, remembering where you left your provisions, making decisions about which way to go, learning, repeating and varying all kinds of complex movements, and so on. Though all of these problems were of crucial importance during the human evolutionary process, they are by no means solved exclusively by humans. We find an amazing variety of different solutions for these problems in nature; just consider, for example, the way a bat hunts its prey compared to a spider. We will mainly focus on problems that are not solved by animals or evolution; we will instead focus on abstract problems, such as playing chess. Furthermore, we will not consider problems that have an obvious solution. For example, imagine you to take a sip of coffee from a mug next to your right hand. You do not even have to think about how to do this. This is not because the situation itself is trivial (a robot capable of recognizing the mug, deciding whether it is full, then grabbing it and moving it to your mouth would be a highly complex machine) but because in the context of all possible situations it is so trivial that it no longer is a problem our consciousness needs to be bothered with. The problems we will discuss in the following all need some conscious effort, though some seem to be solved without us being able to say how exactly we got to the solution. We will often find that the strategies we use to solve these problems are applicable to more basic problems, too. Non-trivial, abstract problems can be divided into two groups: well-defined problems and ill-defined problems. Well-defined Problems For many abstract problems, it is possible to find an algorithmic solution. We call problems well-defined if they can be properly formalized, which involves the following properties: The problem has a clearly defined given state. This might be the line-up of a chess game, a given formula you have to solve, or the set-up of the towers of Hanoi game (which we will discuss later). There is a finite set of operators, that is, rules you may apply to the given state. For the chess game, e.g., these would be the rules that tell you which piece you may move to which position. Finally, the problem has a clear goal state: The equations is resolved to x, all discs are moved to the right stack, or the other player is in checkmate. A problem that fulfills these requirements can be solved using algorithms. Therefore many well-defined problems can be very effectively solved by computers, like playing chess. Ill-defined Problems Though many problems can be properly formalized, there are still others where this is not the case. Good examples for this are all kinds of tasks that involve creativity, and, generally speaking, all problems for which it is not possible to clearly define a given state and a goal state. Formalizing a problem such as “Please paint a beautiful picture” may be impossible. Still, this is a problem most people would be able to approach in one way or the other, even if the result may be totally different from person to person. And while one person might judge that picture X is gorgeous, you might completely disagree. The line between well-defined and ill-defined problems is not always neat: ill-defined problems often involve sub-problems that can be perfectly well-defined. On the other hand, many everyday problems that seem to be completely well-defined involve — when examined in detail — a great amount of creativity and ambiguity. Consider the fairly ill-defined task of writing an essay about something you read in class: you will not be able to complete this task without first understanding the text you have to write about. This step is the first subgoal you have to solve. In this example, an ill-defined problem involves a well-defined sub-problem. 10.2 Restructuring: The Gestalt Approach One dominant approach to problem solving originated from Gestalt psychologists in the 1920s. Their understanding of problem solving emphasizes behavior in situations requiring relatively novel means of attaining goals and suggests that problem solving involves a process called restructuring. With a Gestalt approach, two main questions have to be considered to understand the process of problem solving: (1) how is a problem represented in a person’s mind?, and (2) how does solving this problem involve a reorganization or restructuring of this representation? How is a problem represented in the mind? Generally speaking, problem representations are models of the situation as experienced by the solver. Representing a problem means to analyze it and split it into separate components, including objects, predicates, state space, operators, and selection criteria. Internal and external representations are distinguished: an internal representation is one held in memory, and which has to be retrieved by cognitive processes, while an external representation exists in the environment, such like physical objects or symbols whose information can be picked up and processed by the perceptual system. The efficiency of problem solving depends on the underlying representations in a person’s mind, which usually also involves personal aspects. Re-analyzing the problem along different dimensions, or changing from one representation to another, can result in arriving at a new understanding of a problem. This is called restructuring. The following example illustrates this: Two boys of different ages are playing badminton. The older one is a more skilled player, and therefore the outcome of matches between the two becomes predictable. After repeated defeats the younger boy finally loses interest in playing. The older boy now faces a problem, namely that he has no one to play with anymore. The usual options, according to Wertheimer (1945), range from “offering candy” and “playing a different game” to “not playing at full ability” and “shaming the younger boy into playing.” All of these strategies aim at making the younger boy stay. The older boy instead comes up with a different solution: He proposes that they should try to keep the birdie in play as long as possible. Thus they change from a game of competition to one of cooperation. The proposal is happily accepted and the game is on again. The key in this story is that the older boy restructured the problem, having found that his attitude toward the game made it difficult to keep the younger boy playing. With the new type of game the problem is solved: the older boy is not bored, and the younger boy is not frustrated. In some cases, new representations can make a problem more difficult or much easier to solve. In the latter case insight – the sudden realization of a problem’s solution – may be the key to finding a solution. Insight There are two very different ways of approaching a goal-oriented situation. In one case an organism readily reproduces the response to the given problem from past experience. This is called reproductive thinking. The second way requires something new and different to achieve the goal—prior learning is of little help here. Such productive thinking is sometimes argued to involve insight. Gestalt psychologists state that insight problems are a separate category of problems in their own right. Tasks that might involve insight usually have certain features: they require something new and non-obvious to be done, and in most cases they are difficult enough to predict that the initial solution attempt will be unsuccessful. When you solve a problem of this kind you often have a so called “aha” experience: the solution pops into mind all of a sudden. In one moment you have no idea how to answer the problem, and you feel you are not making any progress trying out different ideas, but in the next moment the problem is solved. Would you like to experience such an effect? Here is an example of an insight problem from Silveira (1971): you are given four pieces of a chain, each made up of three links (see Figure 10.1). The task is to link it all up to a closed loop. To open a link costs 2 cents, and to close a link costs 3 cents. You have 15 cents to spend. What should you do? If you want to know the correct solution, scroll down to Figure 10.7. Figure 10.1: The materials for the cheap necklace problem. To show that solving insight problems involves restructuring, psychologists have created a number of problems that are more difficult to solve for participants with previous experiences, since it is harder for them to change the representation of the given situation. For non-insight problems the opposite is the case. Solving arithmetical problems, for instance, requires schemas, through which one can get to the solution step by step. Fixation Sometimes, previous experience or familiarity can even make problem solving more difficult. This is the case whenever habitual directions get in the way of finding new directions – an effect called fixation. Functional fixedness Functional fixedness concerns the solution of object use problems. The basic idea is that when the usual function an object is emphasized, it will be far more difficult for a person to use that object in a novel manner. An example for this effect is the candle problem (Duncker, 1945): Imagine you are given a book of matches, a box of tacks, and a candle (Figure 10.2). On the wall of the room there is a corkboard. Your task is to fix the candle to the corkboard in such a way that no wax will drop on the floor when the candle is lit. Got an idea? Figure 10.2: The materials for the candle problem. If you’re having trouble thinking of a solution, try considering the same materials, but presented slightly differently, in Figure 10.3. Figure 10.3: The same materials, presented slightly differently. Here’s a clue: when people are confronted with a problem and given certain objects to solve it, it is difficult for them to figure out that they could use the objects in a different way. In Figure 10.4, the box is being used as a holder for the tacks. In Figure 10.3, the box is presented without a specific use. Presenting the materials in the latter fashion helps participants have the insight they need: the box has to be recognized as a support rather than as a container— tack the box to the wall, and place the candle upright in the box. The box will catch the falling wax (Figure 10.4). Figure 10.4: A solution to the candle problem. A further example is the two-string problem (Maier, 1931): You are left in a room with a pair of pliers and given the task to tie two strings together that are hanging from the ceiling (Figure 10.5. The problem you face is that you can never reach both strings at a time because they are just too far away from each other. What can you do? Figure 10.5: The two-string problem. Solution: You must recognize you can use the pliers in a novel function: as weight for a pendulum. You can tie them to one of the strings, push it away, hold the other string and wait for the first one to swing toward you (Figure 10.6). Figure 10.6: A solution to the two-string problem. Figure 10.7: The solution to the cheap necklace problem. Mental fixedness Functional fixedness as involved in the examples above illustrates a mental set: a person’s tendency to respond to a given task in a manner based on past experience. Because we map an object to a particular function we have difficulty thinking of an alternative use (i.e., pliers as pendulum’s weight). One approach to studying fixation was to study wrong-answer verbal insight problems. In these problems, people tend to give an incorrect answer when failing to solve a problem rather than to give no answer at all. A typical example: People are told that on a lake the area covered by water lilies doubles every 24 hours and that it takes 60 days to cover the whole lake. Then they are asked how many days it takes to cover half the lake. The typical response is “30 days” (whereas 59 days is correct). These wrong solutions are due to an inaccurate interpretation, or representation, of the problem. This can happen because of sloppiness (a quick shallow reading of the problem and/or weak monitoring of their efforts made to come to a solution). In this case error feedback should help people to reconsider the problem features, note the inadequacy of their first answer, and find the correct solution. If, however, people are truly fixated on their incorrect representation, being told the answer is wrong does not help. Dominowski &amp; Dallob (1995) investigated these two possibilities by giving participants error feedback. Error feedback only led to right answers in approximately one third of cases. The authors concluded that only approximately one third of the wrong answers were due to inadequate monitoring. Another approach is the study of examples with and without a preceding analogous task. In cases such like the water-jug task (Luchins, 1942), analogous thinking indeed leads to a correct solution, but to take a different way might make the case much simpler: Imagine you are given three jugs with different capacities and are asked to measure the required amount of water. You are not allowed to use anything except the jugs and as much water as you likes. In the first case the sizes are: 127 cups, 21 cups and 3 cups. Your goal is to measure 100 cups of water. In the second case you are asked to measure 18 cups from jugs of 39, 15 and 3 cups capacity. Participants who are given the 100 cup task first choose a complicated way to solve the second task. Participants who did not know about that complex task solved the 18 cup case by just adding three cups to 15. This fixation on prior problem-solving experience—persisting in using a suboptimal solution, rather than seeking a more efficient solution—is also known as the Einstellung effect. Fixation on Examples When trying to solve an ill-defined problem with many possible solutions, people often find it helpful to look at examples of solutions. You’ve probably done this as a student when you have been assigned to write an essay, or complete a creative final project—it is natural to want to see examples of what you are supposed to do. However, provision of examples is not always a good thing for promoting creative thinking, as examples can be a source of mental fixation. To illustrate, consider one study by S. M. Smith et al. (1993). In this study, participants were asked to draw pictures of alien creatures. Some participants were provided with examples such as those in Figure 10.8, and some were not. Do you notice that the examples have features in common? What do you think happened to the participants’ drawings when they saw these examples? Figure 10.8: S. M. Smith et al. (1993) showed their participants examples similar to these. Drawings by Michael Barlev and Annie Ditta. Participants were more likely to include the shared features of the examples (i.e., four legs, antennae, tail) in their pictures compared to when participants were not shown examples, demonstrating fixation. This effect persisted—and even increased!—when participants were instructed to be as different as possible from the examples in a follow-up study. However, recent work has shown that sometimes, people can generate more creative ideas when they are given common examples and are told to avoid using them (George &amp; Wiley, 2020). Overcoming Mental Fixation Clearly, mental fixation poses a challenge for solving problems and thinking of creative ideas. Yet, we are able to accomplish these tasks (albeit with some difficulty). So, how is it that we are able to overcome mental fixation and reach solutions? One strategy for overcoming mental fixation is quite simple—take a break from whatever you are working on, and do something else for a while! You’ve probably experienced this before. Perhaps you are working on an essay and have hit a mental road block about what to write next. In frustration, you get up to go take a walk. Then, either during that walk or when you get back to your computer, the idea suddenly hits you in a moment of insight—aha! And you are able to continue with your work. The phenomenon—that we are more likely to come to a solution when taking a break between problem-solving attempts compared to when we work continuously on a problem—is called incubation. There are many theories proposed for why incubation periods are effective. One theory is called forgetting fixation. Under this theory, attempts to solve problems result in the generation of mental fixation on unhelpful information that impedes progress. However, taking a break gets you out of that mental space, and sometimes even into a new physical space, too! The passage of time, along with this shift in mental and physical context, allows for that unhelpful information to be forgotten, so problem solvers are more able to access new information and apply it to the problem at hand. Often, this work happens unconsciously, which can result in solutions coming to mind all at once, fully formed, in what has been termed an “Aha!” moment (Kounios &amp; Beeman, 2009)—an insight experience. 10.3 Solving Problems by Analogy One special kind of restructuring is analogical problem solving. Here, to find a solution to one problem (i.e., the target problem) an analogous solution to another problem (i.e., the base problem) is presented. An example for this kind of strategy is the radiation problem posed by Duncker (1945): As a doctor you have to treat a patient with a malignant, inoperable tumor, buried deep inside the body. There exists a special kind of ray which is harmless at a low intensity, but at sufficiently high intensity is able to destroy the tumor. At such high intensity, however, the ray will also destroy the healthy tissue it passes through on the way to the tumor. What can be done to destroy the tumor while preserving the healthy tissue? When this question was asked to participants in an experiment, most of them couldn’t come up with the appropriate answer to the problem. Then they were told a story that went something like this: A general wanted to capture his enemy’s fortress. He gathered a large army to launch a full-scale direct attack, but then learned that all the roads leading directly towards the fortress were blocked by landmines. These roadblocks were designed in such a way that it was possible for small groups of the fortress-owner’s men to pass over them safely, but a large group of men would set them off. The general devised the following plan: He divided his troops into several smaller groups and ordered each of them to march down a different road, timed in such a way that the entire army would reunite exactly when reaching the fortress and could hit with full strength. Here, the story about the general is the base problem, and the radiation problem is the target problem. The fortress is analogous to the tumor and the big army corresponds to the highly intensive ray. Likewise, a small group of soldiers represents a ray at low intensity. The solution to the problem is to split the ray up, as the general did with his army, and send the now harmless rays towards the tumor from different angles in such a way that they all meet when reaching it. No healthy tissue is damaged but the tumor itself gets destroyed by the ray at its full intensity. Gick &amp; Holyoak (1980) presented Duncker’s radiation problem to a group of participants. Ten percent of participants were able to solve the problem right away, but thirty percent could solve it when they read the story of the general before. After being given an additional hint — to use the story as help — seventy-five percent of them solved the problem. Following these results, Gick and Holyoak concluded that analogical problem solving requires recognizing that an analogical connection exists between the target and the base problem and mapping corresponding parts of the two problems onto each other (fortress → tumor, army → ray, etc.). One reason that people may fail to solve a problem by analogy is that we tend to pay attention to the surface features of problems (e.g., the tumor problem is a medical problem, and the fortress problem is a military problem). However, analogical problem solving requires making connections based on a problems structural features (e.g., both the tumor problem and the fortress problem involve a large force that is too dangerous when applied from a single direction, so many smaller forces must converge simultaneously instead). Next, Gick and Holyoak started looking for factors that could help the recognizing and mapping processes without providing an explicit hint. Schemas The abstract concept that links the target problem with the base problem is called the problem schema. Gick &amp; Holyoak (1983) investigated how to achieve schema induction, or the creation of an useful problem-solving schema that can be applied from a base problem to a target problem. The experimenters had participants read stories that presented problems and their solutions. One story was the above story about the general, and other stories required the same problem schema (i.e., if a heavy force coming from one direction is not suitable, use multiple smaller forces that simultaneously converge on the target). The experimenters manipulated how many of these stories the participants read before the participants were asked to solve the radiation problem. The experiment showed that in order to solve the target problem, reading two stories with analogical problems is more helpful than reading only one story. This evidence suggests that schema induction can be achieved by exposing people to multiple problems with the same problem schema. 10.4 Creativity In order to solve problems—particularly ill-defined problems—humans need to think creatively and stretch beyond the boundaries of what they have accomplished previously. You may have heard of the phrase “thinking outside of the box” as related to creativity—let’s go through an example to illustrate what this means. In a classic problem-solving task, participants are presented with an array of nine dots and are instructed to connect all 9 dots with four lines, without picking up their writing instrument (Maier, 1930). See Figure 10.9 below. Figure 10.9: The nine-dot problem. Think you have a solution? The only way to solve this problem is to literally go “outside of the box” by drawing lines that extend beyond the boundaries of the box that the dots create. This example illustrates how, when we think creatively, we often need to think of novel and appropriate solutions to a task that we have not generated before (see Restructuring: The Gestalt Approach section for more information about how we do this). See Figure 10.10 for different possible solutions. Figure 10.10: Solutions to the nine-dot problem. Creativity can be studied in many different ways—from the personal (i.e., identifying what traits are associated with more creative people), to the societal (i.e., what makes something broadly recognized as creative?), to the cognitive (i.e., how is our cognitive system structured such that we are able to think creatively and solve problems when we encounter them?). We will focus on the last approach—often called the creative cognition approach (S. M. Smith et al., 1995). This approach to studying creativity assumes that all humans are capable of creative thought; it is not something special that only some humans have and others do not. Indeed, the creative cognition approach states that the human cognitive system—with all its strengths and its flaws—is built in order to facilitate our ability to think creatively. For example, why would it make sense that our memory system is fallible; that is, why do we forget things that are sometimes important to remember? Our ability to forget has been argued to be a byproduct of our ability to think flexibly and apply old knowledge to new scenarios in ways that are appropriate for the current situation (Ditta &amp; Storm, 2018). See The Sins of Human Memory and How They Benefit Creative Thinking for more information. The Sins of Human Memory and How They Benefit Creative Thinking In a classic paper, Schacter (1999) argued that the flaws in the human memory system—everyday issues we face like forgetting names, misremembering where we encountered a piece of information, and failure to access information we know we have stored in memory—are actually byproducts of an adaptive system. Ditta &amp; Storm (2018) took this argument a step further, and argued that these flaws in the memory system also support creative thinking. For example, losing access to information, like when we forget, helps us overcome the tendency to reproduce mundane solutions to problems that would not be creative or useful. Misremembering information—as when musicians mistakenly attribute a melody as the product of their own mind, rather than another song they heard in the past—is likely a byproduct of our memory system helping us to take old information from its context, and transfer and apply it to a new context where that information might be helpful—but not in its original form. Of course, sometimes these memory errors get us into trouble (as with the musician example—this is called cryptomnesia and is an example of accidental plagiarism!), but without a flexible memory system that occasionally produces such errors, we would likely be much less able to solve problems when we encounter them. The “seven sins” of human memory are listed below—see if you can think of why such sins might actually reflect a benefit of the cognitive system! Seven Sins of Human Memory Transience: The fact that memory for information declines over time. Absentmindedness: Without appropriate memory cues to trigger our memory, we may fail to complete a task in the future. Blocking: Related information impairs our ability to access the piece of information we want (e.g., tip-of-the-tongue phenomenon). Misattribution: Misremembering where you encountered a piece of information. Suggestibility: The fact that our memory can be changed with outside suggestion from others—typically through imagination inflation, where we imagine how an event might have happened and mistakenly attribute that imagination as an actual component of the event. Bias: The fact that we remember information in ways consistent with our own biases (e.g., remembering a political candidate’s speech as being better or worse than it was, based on whether we support or do not support them). Persistence: The fact that memories—particularly highly emotional ones—are resistant to forgetting, even when we want them to be (e.g., post-traumatic stress disorder, PTSD). Defining Creativity The term creativity can mean many things to different people. Thus, when researchers study creativity, they need to operationally define the term. Though scholars disagree about the best way to define creativity, most agree that in order for something to be creative, it must be both novel and appropriate for the situation (Runco &amp; Jaeger, 2012). Novelty is straightforward: an idea must be new (at least to the individual thinking it, if not to society as a whole) to be considered creative, as reproducing what came before is typically not something that most people would call creative. Appropriateness refers to the need for the idea to “make sense” in the context of the problem. For example, if we return to the nine-dot problem, a creative solution must involve drawing lines; folding the paper to make multiple dots touch would not be an appropriate solution (though it is certainly outside of the box!). Scholars disagree about “appropriateness,” and you might find yourself agreeing: folding the paper before the drawing the lines is certainly a novel way to approach the problem! Scholars also sometimes include additional characteristics in their definitions of creativity; for example, an idea instilling an element of “surprise” as necessary for something to be labeled as creative (Simonton, 2018). Creative Thinking Creative thinking is sometimes defined as consisting of both divergent and convergent thinking. Divergent thinking is when the goal is to explore and idea space and generate as many different ideas as possible. In the lab, divergent thinking is often measured through the Alternative Uses Task, in which participants are presented with everyday household objects and asked to generate as many uses for those objects as possible. Those ideas are then scored on a number of different dimensions, including originality and fluency (i.e., how many ideas they were able to generate). Interestingly, divergent thinking and producing creative ideas seems to be a process that unfolds over time, with some research demonstrating that the most creative ideas come later during the idea generation period, perhaps after the more common ideas that immediately come to mind (George &amp; Wiley, 2020). Convergent thinking, on the other hand, is when the goal is to unite many different concepts into a single idea or smaller subset of ideas. Convergent thinking is typically measured in the lab through the Remote Associates Task (RAT), where participants are presented with three seemingly unrelated words and are asked to find the fourth word that relates to all three words (e.g., WISE-WORK-TOWER; solution: CLOCK). Creativity is determined by counting the number of RAT problem solved correctly. Though convergent and divergent thinking are sometimes studied separately, it is likely that creative thinking involves elements of both types of thinking. 10.5 How do Experts Solve Problems? An expert is someone who devotes large amounts of their time and energy to one specific field of interest in which they, subsequently, reach a certain level of mastery. It should not be a surprise that experts tend to be better at solving problems in their field than novices (i.e., people who are beginners or not as well-trained in a field as experts) are. Experts are faster at coming up with solutions and have a higher rate of correct solutions. But what is the difference between the way experts and non-experts solve problems? Research on the nature of expertise has come up with the following conclusions: Experts know more about their field, their knowledge is organized differently, and they spend more time analyzing the problem. Expertise is domain-specific: when it comes to problems that are outside the experts’ domain of expertise, their performance often does not differ from that of novices. Knowledge An experiment by Chase &amp; Simon (1973) dealt with the question of how well experts and novices are able to reproduce positions of chess pieces on chess boards after a brief presentation. The results showed that experts were far better at reproducing actual game positions, but that their performance was comparable with that of novices when the chess pieces were arranged randomly on the board. Chase and Simon concluded that the superior performance on actual game positions was due to the ability to recognize familiar patterns: A chess expert has up to 50,000 patterns stored in his memory. In comparison, a good player might know about 1,000 patterns by heart and a novice only few to none at all. This very detailed knowledge is of crucial help when an expert is confronted with a new problem in his field. Still, it is not only the amount of knowledge that makes an expert more successful. Experts also organize their knowledge differently from novices. Organization Chi et al. (1981) took a set of 24 physics problems and presented them to a group of physics professors as well as to a group of students with only one semester of physics. The task was to group the problems based on their similarities. The students tended to group the problems based on their surface structure (i.e., similarities of objects used in the problem, such as sketches illustrating the problem), whereas the professors used their deep structure (i.e., the general physical principles that underlie the problems) as criteria. By recognizing the actual structure of a problem experts are able to connect the given task to the relevant knowledge they already have (e.g., another problem they solved earlier which required the same strategy). Analysis Experts often spend more time analyzing a problem before actually trying to solve it. This way of approaching a problem may often result in what appears to be a slow start, but in the long run this strategy is much more effective. A novice, on the other hand, might start working on the problem right away, but often reaches dead ends as they chose a wrong path in the very beginning. The Curse of Expertise Though experts approach problem solving differently and are typically more successful than novices, expertise is not always a good thing. There are some conditions under which knowing too much can lead to inhibition—especially in creative problem solving. For example, in a study using the Remote Associates Task discussed earlier, three types of problems were used (Wiley, 1998). First, problems that used terms related to baseball, but where the solution was unrelated to baseball (e.g., PLATE-BROKEN-SHOT; solution: GLASS). Second, problems using baseball terms where the solution was related to baseball (e.g., PLATE-BROKEN-REST; solution: HOME). Third, neutral problems unrelated to baseball at all (e.g., BLUE-KNFIE-COTTAGE; solution: CHEESE). Baseball experts and novices were asked to solve all of the problem types. In this study, experts ended up worse than novices at solving the problems with baseball-unrelated solutions! The experts were likely too fixated on trying to find a solution related to baseball when it was not appropriate to do so. Thus, having too much knowledge can sometimes reduce our capability to think outside the box. Key Takeaways The way we mentally represent problems affects our ability to solve them. In the case of insight problems, we may need to restructure the way we see the problem in order to arrive at a solution. Sometimes, our mental representation of a problem space can inhibit our ability to solve problems. For example, functional fixedness is a bias to only see an object for its given use, even when alternative uses for the object would help solve the problem. Experts don’t just know more than novices; their knowledge is also organized differently. Exercises A Google Image search for “life hacks” yields ideas for using common household objects in novel, useful ways. What do these life hacks tend to share in common? They are compelling examples of overcoming functional fixedness! Try to identify some relevant life hacks and identify how functional fixedness was overcome (e.g., what was the object’s intended use, and how is it being used in the life hack?). Do you have expert knowledge in an area? Maybe it’s an academic subject, an instrument, a video game, or something from your place of work (e.g., many restaurant servers have astounding expert knowledge of their menus, enough that they can take a whole table’s order without writing anything down!). What is the difference between how you might solve a problem in this domain versus how a novice might solve the same problem? 10.6 Glossary “aha” experience A moment of insight in which a solution pops into the problem solver’s head. appropriateness The extent to which a solution makes sense in the context of the problem. convergent thinking Creative thinking in which the goal is to unite many different concepts into a single idea or smaller subset of ideas. creativity The ability to generate novel and appropriate solutions to problems. divergent thinking Creative thinking in which the goal is to explore and idea space and generate as many different ideas as possible. expert A person who reaches mastery in a field following extensive practice. incubation Individuals become more likely to generate a solution when taking a break between problem-solving attempts compared to when working continuously on a problem. mental set A person’s tendency to respond to a given task in a manner based on past experience. novelty The extent to which a solution to a problem is new, as opposed to reproducing what has come before. novice A person who is a beginner in a field. problem Any situation that differs from a desired goal. productive thinking Producing a response to a problem that is new or different from previous problem-solving experience. reproductive thinking Producing a response to a problem based on previous problem-solving experience. restructuring Re-analyzing the problem along different dimensions, or changing from one representation to another, resulting in a new understanding of a problem. schema induction The creation of an useful problem-solving schema that can be applied from a base problem to a target problem. References Chase, W. G., &amp; Simon, H. A. (1973). Perception in chess. Cogn. Psychol., 4(1), 55–81. Chi, M. T. H., Feltovich, P. J., &amp; Glaser, R. (1981). Categorization and representation of physics problems by experts and novices. Cogn. Sci., 5(2), 121–152. Ditta, A. S., &amp; Storm, B. C. (2018). A consideration of the seven sins of memory in the context of creative cognition. Creativity Research Journal, 30(4), 402–417. Dominowski, R. L., &amp; Dallob, P. (1995). Insight and problem solving. Duncker, K. (1945). On problem-solving. Psychological Monographs, 58(5), i. George, T., &amp; Wiley, J. (2020). Need something different? Here’s what’s been done: Effects of examples and task instructions on creative idea generation. Memory &amp; Cognition, 48, 226–243. Gick, M. L., &amp; Holyoak, K. J. (1980). Analogical problem solving. Cognitive Psychology, 12(3), 306–355. Gick, M. L., &amp; Holyoak, K. J. (1983). Schema induction and analogical transfer. Cognitive Psychology, 15(1), 1–38. Kounios, J., &amp; Beeman, M. (2009). The aha! Moment: The cognitive neuroscience of insight. Current Directions in Psychological Science, 18(4), 210–216. Luchins, A. S. (1942). Mechanization in problem solving: The effect of einstellung. Psychological Monographs, 54(6), i. Maier, N. R. (1930). Reasoning in humans. I. On direction. Journal of Comparative Psychology, 10(2), 115. Maier, N. R. (1931). Reasoning in humans. II. The solution of a problem and its appearance in consciousness. Journal of Comparative Psychology, 12(2), 181. Runco, M. A., &amp; Jaeger, G. J. (2012). The standard definition of creativity. Creativity Research Journal, 24(1), 92–96. Schacter, D. L. (1999). The seven sins of memory: Insights from psychology and cognitive neuroscience. American Psychologist, 54(3), 182. Silveira, J. M. (1971). Incubation: The effect of interruption timing and length on problem solution and quality of problem processing. Unpublished doctoral dissertation. University of Oregon. Simonton, D. K. (2018). Defining creativity: Don’t we also need to define what is not creative? The Journal of Creative Behavior, 52(1), 80–90. Smith, S. M., Ward, T. B., &amp; Finke, R. A. (1995). The creative cognition approach. MIT press. Smith, S. M., Ward, T. B., &amp; Schumacher, J. S. (1993). Constraining effects of examples in a creative generation task. Memory &amp; Cognition, 21, 837–845. Wertheimer, M. (1945). Productive thinking. Harper New York. Wiley, J. (1998). Expertise as mental set: The effects of domain knowledge in creative problem solving. Memory &amp; Cognition, 26, 716–730. "],["references.html", "References", " References Abbot, E. E. (1909). On the analysis of the memory consciousness in orthography. Psychol. Monogr., 11(1), 127–158. Alba, J. W., &amp; Hasher, L. (1983). Is memory schematic? Psychol. Bull., 93(2), 203–231. Allan, K., &amp; Gabbert, F. (2008). I still think it was a banana: Memorable “lies” and forgettable “truths.” Acta Psychologica, 127(2), 299–308. Allen, S. W., &amp; Brooks, L. R. (1991). Specializing the operation of an explicit rule. J. Exp. Psychol. Gen., 120(1), 3–19. Andrews, I. (1982). Bilinguals out of focus: A critical discussion. International Review of Applied Linguistics in Language Teaching, 20(4), 297–305. Association, A. P. (2000). Diagnostic and statistical manual of mental disorders (4eme ed.) washington. Atkinson, R. C., &amp; Shiffrin, R. M. (1968). Human memory: A proposed system and its control processes. In Psychology of learning and motivation (Vol. 2, pp. 89–195). Elsevier. Atkinson, R. C., &amp; Shiffrin, R. M. (1971). The control of short-term memory. Scientific American, 225(2), 82–91. Baddeley, A. D. (1990). The development of the concept of working memory: Implications and contributions of neuropsychology. In G. Vallar &amp; T. Shallice (Eds.), Neuropsychological impairments of Short-Term memory (pp. 54–73). Cambridge University Press. Baddeley, A. D. (2000). The episodic buffer: A new component of working memory? Trends Cogn. Sci., 4(11), 417–423. Baddeley, A. D., &amp; Hitch, G. (1974). Working memory (G. H. Bower, Ed.; Vol. 8, pp. 47–89). Academic Press. Baddeley, A. D., Thomson, N., &amp; Buchanan, M. (1975). Word length and the structure of short-term memory. Journal of Verbal Learning and Verbal Behavior, 14(6), 575–589. Baldwin, D. A. (1993). Early referential understanding: Infants’ ability to recognize referential acts for what they are. Dev. Psychol., 29(5), 832–843. Bargh, J. A. (2014). Our unconscious mind. Sci. Am., 310(1), 30–37. Bartlett, J. C., &amp; Memon, A. (2007). Eyewitness memory in young and older adults. Lawrence Erlbaum Mahwah, NJ. Beilock, S. L., &amp; Carr, T. H. (2001). On the fragility of skilled performance: What governs choking under pressure? J. Exp. Psychol. Gen., 130(4), 701–725. Benjamin, A. S., &amp; Tullis, J. (2010). What makes distributed practice effective? Cognitive Psychology, 61, 228–247. https://doi.org/10.1016/j.cogpsych.2010.05.004.What Benjamin, L. T. (2007). A brief history of modern psychology. Blackwell Publishing. Berkowitz, S. R., Laney, C., Morris, E. K., Garry, M., &amp; Loftus, E. F. (2008). Pluto behaving badly: False beliefs and their consequences. Am. J. Psychol., 121(4), 643–660. Bernstein, D. M., Laney, C., Morris, E. K., &amp; Loftus, E. F. (2005). False memories about food can lead to food avoidance. Soc. Cogn., 23(1), 11–34. Bernstein, D. M., &amp; Loftus, E. F. (2009a). The consequences of false memories for food preferences and choices. Perspect. Psychol. Sci., 4(2), 135–139. Bernstein, D. M., &amp; Loftus, E. F. (2009b). How to tell if a particular memory is true or false. Perspect. Psychol. Sci., 4(4), 370–374. Bialystok, E. (2009). Bilingualism: The good, the bad, and the indifferent. Biling. (Camb. Engl.), 12(1), 3–11. Bornstein, B. H., Deffenbacher, K. A., Penrod, S. D., &amp; McGorty, E. K. (2012). Effects of exposure time and cognitive operations on facial identification accuracy: A meta-analysis of two variables associated with initial memory strength. Psychology, Crime &amp; Law, 18(5), 473–490. Bower, G. H. (1981). Mood and memory. American Psychologist, 36(2), 129. Boysson-Bardies, B. de, Sagart, L., &amp; Durand, C. (1984). Discernible differences in the babbling of infants according to target language. Journal of Child Language, 11(1), 1–15. Brady, T. F., Störmer, V. S., &amp; Alvarez, G. A. (2016). Working memory is not fixed-capacity: More active storage capacity for real-world objects than for simple stimuli. Proceedings of the National Academy of Sciences, 113(27), 7459–7464. Braun, K. A., Ellis, R., &amp; Loftus, E. F. (2002). Make my memory: How advertising can change our memories of the past. Psychol. Mark., 19(1), 1–23. Bressan, P., &amp; Dal Martello, M. F. (2002). Talis pater, talis filius: Perceived resemblance and the belief in genetic relatedness. Psychological Science, 13, 213–218. Brewer, W. F., &amp; Treyens, J. C. (1981). Role of schemata in memory for places. Cognitive Psychology, 13(2), 207–230. Bridgeman, B., &amp; Morgan, R. (1996). Success in college for students with discrepancies between performance on multiple-choice and essay tests. J. Educ. Psychol., 88(2), 333–340. Brigham, J. C., Bennett, L. B., Meissner, C. A., &amp; Mitchell, T. L. (2007). The influence of race on eyewitness memory. Brigham, J. C., Ready, D. J., &amp; Spier, S. A. (1990). Standards for evaluating the fairness of photograph lineups. Basic and Applied Social Psychology, 11(2), 149–163. Broadbent, D. E., &amp; Dal Martello, M. F. (1958). Perception and communication. Psychological Science. Brooks, L. R. (1968). Spatial and verbal components of the act of recall. Can. J. Psychol., 22(5), 349–368. Brown, A. S. (1991). A review of tip of the tongue experience. Psychological Bulletin, 109, 79–91. Brown, J. R. (1967). Biological foundations of language. Neurology, 17(12), 1219–1219. Brown, R., &amp; Kulik, J. (1977). Flashbulb memories. Cognition, 5(1), 73–99. Burton, A. M., Wilson, S., Cowan, M., &amp; Bruce, V. (1999). Face recognition in poor-quality video: Evidence from security surveillance. Psychological Science, 10(3), 243–248. Calvo, P., &amp; Gomila, T. (2008). Handbook of cognitive science: An embodied approach. Elsevier. Caputo, D., &amp; Dunning, D. (2007). Distinguishing accurate identifications from erroneous ones: Post-dictive indicators of eyewitness accuracy. In The handbook of eyewitness psychology: Volume II (pp. 441–464). Psychology Press. Caselli, N., Pyers, J., &amp; Lieberman, A. M. (2021). Deaf children of hearing parents have age-level vocabulary growth when exposed to american sign language by 6 months of age. The Journal of Pediatrics, 232, 229–236. Ceci, S. J., &amp; Bruck, M. (1995). Jeopardy in the courtroom: A scientific analysis of children’s testimony. American Psychological Association. Cepeda, N. J., Vul, E., Rohrer, D., Wixted, J. T., &amp; Pashler, H. (2008). Spacing effects in learning: A temporal ridgeline of optimal retention. Psychological Science, 19, 1095–1102. https://doi.org/10.1111/j.1467-9280.2008.02209.x Chase, W. G., &amp; Simon, H. A. (1973). Perception in chess. Cogn. Psychol., 4(1), 55–81. Cheesman, J., &amp; Merikle, P. M. (1984). Priming with and without awareness. Percept. Psychophys., 36(4), 387–395. Cheesman, J., &amp; Merikle, P. M. (1986). Distinguishing conscious from unconscious perceptual processes. Can. J. Psychol., 40(4), 343–367. Cheng, P. W., &amp; Holyoak, K. J. (1985). Pragmatic reasoning schemas. Cognitive Psychology, 17(4), 391–416. Chernev, A., Böckenholt, U., &amp; Goodman, J. (2015). Choice overload: A conceptual review and meta-analysis. Journal of Consumer Psychology, 25(2), 333–358. Cherry, E. C. (1953). Some experiments on the recognition of speech, with one and with two ears. Journal of the Acoustical Society of America, 25, 975–979. Chi, M. T. H., Feltovich, P. J., &amp; Glaser, R. (1981). Categorization and representation of physics problems by experts and novices. Cogn. Sci., 5(2), 121–152. Chiao, J. (2009). Culture–gene coevolution of individualism – collectivism and the serotonin transporter gene. Proceedings of the Royal Society B, 277, 529–537. https://doi.org/10.1098/rspb.2009.1650 Choi, J. J., Laibson, D., Madrian, B. C., &amp; Metrick, A. (2002). Defined contribution pensions: Plan rules, participant choices, and the path of least resistance. Tax Policy and the Economy, 16, 67–113. Conrad, R. (1964). Acoustic confusions in immediate memory. British Journal of Psychology, 55(1), 75–84. Cosmides, L. (1989). The logic of social exchange: Has natural selection shaped how humans reason? Studies with the wason selection task. Cognition, 31(3), 187–276. Craik, F. I. M., &amp; Lockhart, R. S. (1972). Levels of processing: A framework for memory research. Journal of Verbal Learning and Verbal Behavior, 11(6), 671–684. Craik, F. I. M., &amp; Tulving, E. (1975). Depth of processing and the retention of words in episodic memory. J. Exp. Psychol. Gen., 104(3), 268–294. Cutler, B. L., &amp; Penrod, S. D. (1995). Mistaken identification: The eyewitness, psychology and the law. Cambridge University Press. Czaykowska-Higgins, E., Burton, S., McIvor, O., &amp; Marinakis, A. (2017). Supporting indigenous language revitalisation through collaborative postsecondary proficiency-building curriculum. Darley, J. M., &amp; Gross, P. H. (1983a). A hypothesis-confirming bias in labeling effects. Journal of Personality and Social Psychology, 44, 20–33. Darley, J. M., &amp; Gross, P. H. (1983b). A hypothesis-confirming bias in labeling effects. Journal of Personality and Social Psychology, 44(1), 20. De Waal, F. B. (1989). Peacemaking among primates. Harvard University Press. Deffenbacher, K. A., Bornstein, B. H., Penrod, S. D., &amp; McGorty, E. K. (2004). A meta-analytic review of the effects of high stress on eyewitness memory. Law and Human Behavior, 28(6), 687. Deutsch, J. A., &amp; Deutsch, D. (1963). Attention: Some theoretical considerations. Psychol. Rev., 70(1), 80–90. Dick, A. S., Garcia, N. L., Pruden, S. M., Thompson, W. K., Hawes, S. W., Sutherland, M. T., Riedel, M. C., Laird, A. R., &amp; Gonzalez, R. (2019). No evidence for a bilingual executive function advantage in the ABCD study. Nature Human Behaviour, 3(7), 692–701. Dinner, I., Johnson, E. J., Goldstein, D. G., &amp; Liu, K. (2011). Partitioning default effects: Why people choose not to choose. Journal of Experimental Psychology: Applied, 17(4), 332. Ditta, A. S., &amp; Storm, B. C. (2018). A consideration of the seven sins of memory in the context of creative cognition. Creativity Research Journal, 30(4), 402–417. Dobrich, W., &amp; Scarborough, H. S. (1992). Phonological characteristics of words young children try to say. J. Child Lang., 19(3), 597–616. Dominowski, R. L., &amp; Dallob, P. (1995). Insight and problem solving. Duncker, K. (1945). On problem-solving. Psychological Monographs, 58(5), i. Dunn, E. W., Aknin, L. B., &amp; Norton, M. I. (2008). Spending money on others promotes happiness. Science, 319(5870), 1687–1688. https://doi.org/10.1126/science.1150952 Enns, J. T., &amp; Lleras, A. (2008). What’s next? New evidence for prediction in human vision. Trends in Cognitive Sciences, 12(9), 327–333. Fancher, R. E. (1987). The intelligence men: Makers of the IQ controversy. W.W. Norton &amp; Company. Flanagan, M. B., May, J. G., &amp; Dobie, T. G. (2004). The role of vection, eye movements, and postural instability in the etiology of motion sickness. Journal of Vestibular Research: Equilibrium and Orientation, 14(4), 335–346. Frenda, S. J., Nichols, R. M., &amp; Loftus, E. F. (2011). Current issues and advances in misinformation research. Current Directions in Psychological Science, 20(1), 20–23. Fuchs, A. H. (2000). Contributions of american mental philosophers to psychology in the united states. History of Psychology, 3, 3–19. Gabbert, F., Memon, A., &amp; Allan, K. (2003). Memory conformity: Can eyewitnesses influence each other’s memories for an event? Applied Cognitive Psychology: The Official Journal of the Society for Applied Research in Memory and Cognition, 17(5), 533–543. Gabbert, F., Memon, A., Allan, K., &amp; Wright, D. B. (2004). Say it to my face: Examining the effects of socially encountered misinformation. Legal and Criminological Psychology, 9(2), 215–227. Gais, S., Lucas, B., &amp; Born, J. (2006). Sleep after learning aids memory recall. Learning &amp; Memory, 13(3), 259–262. Galanter, E. (1962). Contemporary psychophysics. In r. Brown, e. Galanter, e. H. Hess, &amp; g. Mandler (eds.), new directions in psychology. Holt, Rinehart; Winston. Garrod, S., &amp; Sanford, A. (1977). Interpreting anaphoric relations: The integration of semantic information while reading. Journal of Verbal Learning and Verbal Behavior, 16(1), 77–90. Garry, M., French, L., Kinzett, T., &amp; Mori, K. (2008). Eyewitness memory following discussion: Using the MORI technique with a western sample. Applied Cognitive Psychology: The Official Journal of the Society for Applied Research in Memory and Cognition, 22(4), 431–439. George, T., &amp; Wiley, J. (2020). Need something different? Here’s what’s been done: Effects of examples and task instructions on creative idea generation. Memory &amp; Cognition, 48, 226–243. Gibson, E. J., Pick, A. D., et al. (2000). An ecological approach to perceptual learning and development. Oxford University Press, USA. Gick, M. L., &amp; Holyoak, K. J. (1980). Analogical problem solving. Cognitive Psychology, 12(3), 306–355. Gick, M. L., &amp; Holyoak, K. J. (1983). Schema induction and analogical transfer. Cognitive Psychology, 15(1), 1–38. Gigerenzer, G., &amp; Hug, K. (1992). Domain-specific reasoning: Social contracts, cheating, and perspective change. Cognition, 43(2), 127–171. Godden, D. R., &amp; Baddeley, A. D. (1975). Context-dependent memory in two natural environments: On land and underwater. Br. J. Psychol., 66(3), 325–331. Goldstein, E. (2005). Cognitive psychology - connecting, mind research, and everyday experience. Thomson Wadsworth. Goodale, M., &amp; Milner, D. (2006). One brain-two visual systems. Psychologist, 19(11), 660–663. Green, L. (2002). A descriptive study of african american english: Research in linguistics and education. International Journal of Qualitative Studies in Education, 15(6), 673–690. Greenfield, P. M., &amp; Savage-Rumbaugh, E. S. (1991). Imitation, grammatical development, and the invention of protogrammar by an ape. In N. A. Krasnegor, D. M. Rumbaugh, R. L. Schiefelbusch, &amp; &amp;. M. Studdert-Kennedy (Eds.), Biological and behavioral determinants of language development (pp. 235–258). Lawrence Erlbaum Associates. Greenwald, A. G. (1992). New look 3: Unconscious cognition reclaimed. Am. Psychol., 47(6), 766–779. Griggs, R. A., &amp; Cox, J. R. (1982). The elusive thematic-materials effect in wason’s selection task. British Journal of Psychology, 73(3), 407–420. Griggs, R. A., &amp; Cox, J. R. (1983). The effects of problem content and negation on wason’s selection task. Quarterly Journal of Experimental Psychology, 35(3), 519–533. Gupta, M. W., Pan, S. C., &amp; Rickard, T. C. (2022). Prior episodic learning and the efficacy of retrieval practice. Memory &amp; Cognition, 50(4), 722–735. Haist, F., Shimamura, A. P., &amp; Squire, L. R. (1992). On the relationship between recall and recognition memory. J. Exp. Psychol. Learn. Mem. Cogn., 18(4), 691–702. Hakuta, K., Bialystok, E., &amp; Wiley, E. (2003). Critical evidence: A test of the critical-period hypothesis for second-language acquisition. Psychol. Sci., 14(1), 31–38. Hall, M. L., Hall, W. C., &amp; Caselli, N. K. (2019). Deaf children need language, not (just) speech. First Language, 39(4), 367–395. Hall, W. C. (2017). What you don’t know can hurt you: The risk of language deprivation by impairing sign language development in deaf children. Maternal and Child Health Journal, 21(5), 961–965. Hampton, J. A. (1979). Polymorphous concepts in semantic memory. J. Verbal Learning Verbal Behav., 18(4), 441–461. Harris, C. R., Coburn, N., Rohrer, D., &amp; Pashler, H. (2013). Two failures to replicate high-performance-goal priming effects. PloS One, 8(8), e72467. Hartung, C. M., &amp; Widiger, T. A. (1998). Gender differences in the diagnosis of mental disorders: Conclusions and controversies of the DSM–IV. Psychological Bulletin, 123(3), 260. Heaps, C., &amp; Nash, M. (1999). Individual differences in imagination inflation. Psychon. Bull. Rev., 6(2), 313–318. Henley, T. B., &amp; Thorne, B. M. (2005). The lost millennium: Psychology during the middle ages. The Psychological Record, 55(1), 103–113. Howard, I. P. (2002). Seeing in depth, vol. 1: Basic mechanisms. University of Toronto Press. Hu, X., Cheng, L. Y., Chiu, M. H., &amp; Paller, K. A. (2020). Promoting memory consolidation during sleep: A meta-analysis of targeted memory reactivation. Psychol. Bull., 146(3), 218–244. Humphries, T., Kushalnagar, P., Mathur, G., Napoli, D. J., Padden, C., Rathmann, C., &amp; Smith, S. (2016). Avoiding linguistic neglect of deaf children. Social Service Review, 90(4), 589–619. Hyman, I. E., Husband, T. H., &amp; Billings, F. J. (1995). False memories of childhood experiences. Appl. Cogn. Psychol., 9(3), 181–197. Iyengar, S. S., &amp; Lepper, M. R. (2000). When choice is demotivating: Can one desire too much of a good thing? Journal of Personality and Social Psychology, 79(6), 995. Jachimowicz, J. M., Duncan, S., Weber, E. U., &amp; Johnson, E. J. (2019). When and why defaults influence decisions: A meta-analysis of default effects. Behavioural Public Policy, 3(2), 159–186. Jackson, A., Koek, W., &amp; Colpaert, F. C. (1992). NMDA antagonists make learning and recall state-dependent. Behav. Pharmacol., 3(4), 415–421. Jacoby, L. L., &amp; Rhodes, M. G. (2006). False remembering in the aged. Current Directions in Psychological Science, 15(2), 49–53. James, W. (1890). Principles of psychology. Johnson, E. J., &amp; Goldstein, D. (2003). Do defaults save lives? In Science (No. 5649; Vol. 302, pp. 1338–1339). American Association for the Advancement of Science. Johnson, J. S., &amp; Newport, E. L. (1989). Critical period effects in second language learning: The influence of maturational state on the acquisition of english as a second language. Cogn. Psychol., 21(1), 60–99. Johnston, W. A., &amp; Heinz, S. P. (1978). Flexibility and capacity demands of attention. J. Exp. Psychol. Gen., 107(4), 420–435. Jones, M. V., Paull, G. C., &amp; Erskine, J. (2002). The impact of a team’s aggressive reputation on the decisions of association football referees. Journal of Sports Sciences, 20, 991–1000. Kahneman, D., &amp; Tversky, A. (1979). Prospect theory: An analysis of decisions under risk. Econometrica, 47, 278. Karpicke, J. D., Lehman, M., &amp; Aue, W. R. (2014). Retrieval-based learning. An episodic context account. In Psychology of Learning and Motivation - Advances in Research and Theory (Vol. 61, pp. 237–284). https://doi.org/10.1016/B978-0-12-800283-4.00007-1 Kounios, J., &amp; Beeman, M. (2009). The aha! Moment: The cognitive neuroscience of insight. Current Directions in Psychological Science, 18(4), 210–216. Kraft, C. (1978). A psychophysical approach to air safety: Simulator studies of visual illusions in night approaches. In h. L. Pick, h. W. Leibowitz, j. E. Singer, a. Steinschneider, &amp; h. W. Steenson (eds.), psychology: From research to practice (2nd ed.). Plenum Press. Kromann, C. B., Jensen, M. L., &amp; Ringsted, C. (2009). The effect of testing on skills learning. Medical Education, 43, 21–27. https://doi.org/10.1111/j.1365-2923.2008.03245.x Laney, C., &amp; Loftus, E. F. (2008). Emotional content of true and false memories. Memory, 16(5), 500–516. Lantz, M. E. (2010). The use of ’clickers’ in the classroom: Teaching innovation or merely an amusing novelty? Computers in Human Behavior, 26, 556–561. https://doi.org/10.1016/j.chb.2010.02.014 Larsen, D. P., Butler, A. C., &amp; Roediger, H. L. (2009). Repeated testing improves long-term retention relative to repeated study: A randomised controlled trial. Medical Education, 43, 1174–1181. https://doi.org/10.1111/j.1365-2923.2009.03518.x Lee, J., &amp; Strayer, D. (2004). Preface to the special section on driver distraction. Human Factors, 46(4), 583. Levin, I. P., Schneider, S. L., &amp; Gaeth, G. J. (1998). All frames are not created equal: A typology and critical analysis of framing effects. Organizational Behavior and Human Decision Processes, 76(2), 149–188. Lichtenstein, S., Slovic, P., Fischhoff, B., Layman, M., &amp; Combs, B. (1978). Judged frequency of lethal events. Journal of Experimental Psychology: Human Learning and Memory, 4(6), 551. Lillo-Martin, D., &amp; Henner, J. (2021). Acquisition of sign languages. Annual Review of Linguistics, 7, 395. Lindsay, D. S., Hagen, L., Read, J. D., Wade, K. A., &amp; Garry, M. (2004). True photographs and false memories. Psychol. Sci., 15(3), 149–154. Livingstone, M. S. (2000). Is it warm? Is it real? Or just low spatial frequency? Science, 290(5495), 1299–1299. Loftus, E. F. (1975). Leading questions and the eyewitness report. Cognitive Psychology, 7(4), 560–572. Loftus, E. F. (2005). Planting misinformation in the human mind: A 30-year investigation of the malleability of memory. Learning &amp; Memory, 12(4), 361–366. Loftus, E. F., &amp; Ketcham, K. (1996). The myth of repressed memory: False memories and allegations of sexual abuse. Macmillan. Loftus, E. F., Miller, D. G., &amp; Burns, H. J. (1978). Semantic integration of verbal information into a visual memory. Journal of Experimental Psychology: Human Learning and Memory, 4(1), 19. Loftus, E. F., &amp; Pickrell, J. E. (1995). The formation of false memories. Psychiatr. Ann., 25(12), 720–725. Lohnas, L. J., &amp; Kahana, M. J. (2014). A retrieved context account of spacing and repetition effects in free recall. J Exp Psychol Learn Mem Cogn, 40, 755–764. https://doi.org/10.1038/jid.2014.371 Lucas, R. E., Clark, A. E., Georgellis, Y., &amp; Diener, E. (2003). Re-examining adaptation and the setpoint model of happiness: Reactions to changes in marital status. Journal of Personality and Social Psychology, 84, 527–539. Luchins, A. S. (1942). Mechanization in problem solving: The effect of einstellung. Psychological Monographs, 54(6), i. Lukaniec, M., &amp; Palakurthy, K. (2022). Additional language learning in the context of indigenous language reclamation. In The routledge handbook of second language acquisition and sociolinguistics (pp. 341–355). Routledge. MacKenzie, A., Engman, M., &amp; McGurk, O. (2022). Overt and symbolic linguistic violence: Plantation ideology and language reclamation in northern ireland. Teaching in Higher Education, 27(4), 489–501. Maier, N. R. (1930). Reasoning in humans. I. On direction. Journal of Comparative Psychology, 10(2), 115. Maier, N. R. (1931). Reasoning in humans. II. The solution of a problem and its appearance in consciousness. Journal of Comparative Psychology, 12(2), 181. Mandel, D. R., Jusczyk, P. W., &amp; Pisoni, D. B. (1995). Infants’ recognition of the sound patterns of their own names. Psychol. Sci., 6(5), 314–317. Maran, M. (2010). My lie: A true story of false memory. John Wiley &amp; Sons. Mareschal, D., Quinn, P. C., &amp; Lea, S. E. G. (2010). The making of human concepts: A final look. In The making of human concepts (pp. 387–394). Oxford University Press. Marian, V., &amp; Kaushanskaya, M. (2007). Language context guides memory content. Psychon. Bull. Rev., 14(5), 925–933. Mauldin, L. (2019). Don’t look at it as a miracle cure: Contested notions of success and failure in family narratives of pediatric cochlear implantation. Social Science &amp; Medicine, 228, 117–125. Mazzoni, G. A. L., Loftus, E. F., Seitz, A., &amp; Lynn, S. J. (1999). Changing beliefs and memories through dream interpretation. Appl. Cogn. Psychol., 13(2), 125–144. McCann, J. J. (1992). Rules for color constancy. Ophthalmic and Physiologic Optics, 12(2), 175–177. McDaniel, M. A., Anderson, J. L., Derbish, M. H., &amp; Morrisette, N. (2007). Testing the testing effect in the classroom. European Journal of Cognitive Psychology, 19, 494–513. https://doi.org/10.1080/09541440701326154 McIntyre, L. (2019). The scientific attitude: Defending science from denial, fraud, and pseudoscience. Mit Press. McKenzie, C. R., Liersch, M. J., &amp; Finkelstein, S. R. (2006). Recommendations implicit in policy defaults. Psychological Science, 17(5), 414–420. McNeil, B. J., Pauker, S. G., Sox Jr, H. C., &amp; Tversky, A. (1982). On the elicitation of preferences for alternative therapies. New England Journal of Medicine, 306(21), 1259–1262. Mechelli, A., Crinion, J. T., Noppeney, U., O’doherty, J., Ashburner, J., Frackowiak, R. S., &amp; Price, C. J. (2004). Structural plasticity in the bilingual brain: Proficiency in a second language and age at acquisition affect grey-matter density. Nature. Medin, D. L., &amp; Schaffer, M. M. (1978). Context theory of classification learning. Psychol. Rev., 85(3), 207–238. Merikle, P. (2004). Subliminal perception. In Encyclopedia of psychology, vol. 7 (pp. 497–499). American Psychological Association. Mervis, C. B., &amp; Pani, J. R. (1980). Acquisition of basic object categories. Cognitive Psychology, 12(4), 496–522. Meyer, D. E., &amp; Schvaneveldt, R. W. (1971). Facilitation in recognizing pairs of words: Evidence of a dependence between retrieval operations. Journal of Experimental Psychology, 90(2), 227. Miller, G. A. (2003). The cognitive revolution: A historical perspective. Trends in Cognitive Sciences, 7(3), 141–144. Mitchell, R. E., &amp; Karchmer, M. (2004). Chasing the mythical ten percent: Parental hearing status of deaf and hard of hearing students in the united states. Sign Language Studies, 4(2), 138–163. Miyake, A., Friedman, N. P., Emerson, M. J., Witzki, A. H., Howerter, A., &amp; Wager, T. D. (2000). The unity and diversity of executive functions and their contributions to complex “frontal lobe” tasks: A latent variable analysis. Cogn. Psychol., 41(1), 49–100. Monsell, S. (2003). Task switching. Trends in Cognitive Sciences, 7(3), 134–140. Moon, C., Cooper, R. P., &amp; Fifer, W. P. (1993). Two-day-olds prefer their native language. Infant Behav. Dev., 16(4), 495–500. Moray, N. (1959). Attention in dichotic listening: Affective cues and the influence of instructions. Q. J. Exp. Psychol., 11(1), 56–60. Morris, C. D., Bransford, J. D., &amp; Franks, J. J. (1977). Levels of processing versus transfer appropriate processing. J. Verbal Learning Verbal Behav., 16(5), 519–533. Mozer, M. C., Pashler, H., Cepeda, N., Lindsey, R., &amp; Vul, E. (2009). Predicting the optimal spacing of study: A multiscale context model of memory. Advances in Neural Information Processing Systems 22 - Proceedings of the 2009 Conference, 1321–1330. Neisser, U. (1979). The control of information pickup in selective looking. In Perception and its development (pp. 201–219). Psychology Press. Nickerson, R. S. (1998a). Applied psychology: An international review. Applied Experimental Psychology, 47, 155–173. Nickerson, R. S. (1998b). Confirmation bias: A ubiquitous phenomenon in many guises. Review of General Psychology, 2(2), 175–220. Nickerson, R. S., &amp; Adams, M. J. (1979). Long-term memory for a common object. Cogn. Psychol., 11(3), 287–307. Nicoladis, E., &amp; Genesee, F. (1997). Language development in preschool bilingual children. Journal of Speech-Language Pathology and Audiology, 21(4), 258–270. NIDCD, N. (2021). Cochlear implants. National Institute on Deafness; Other Communication Disorders. Nitschke, J. B., Dixon, G. E., Sarinopoulos, I., Short, S. J., Cohen, J. D., Smith, E. E., Kosslyn, S. M., Rose, R. M., &amp; Davidson, R. J. (2006). Altering expectancy dampens neural response to aversive taste in primary taste cortex. Nature Neuroscience, 9(3), 435–442. Nungester, R. J., &amp; Duchastel, P. C. (1982). Testing versus review: Effects on retention. Journal of Educational Psychology, 74, 18–22. https://doi.org/10.1037/0022-0663.74.1.18 Oller, D. K., &amp; Pearson, B. Z. (2002). Chapter 1: Assessing the effects of bilingualism: A background. In K. Oller &amp; R. Eilers (Eds.), Language and literacy in bilingual children (pp. 3–21). Multilingual Matters. Onishi, K. H., Murphy, G. L., &amp; Bock, K. (2008). Prototypicality in sentence production. Cognitive Psychology, 56(2), 103–141. Oreskes, N. (2019). Why trust science? Padilla, L., Hosseinpour, H., Fygenson, R., Howell, J., Chunara, R., &amp; Bertini, E. (2022). Impact of COVID-19 forecast visualizations on pandemic risk perceptions. Scientific Reports, 12(1), 2014. Pan, S. C., Rubin, B. R., &amp; Rickard, T. C. (2015). Does testing with feedback improve adult spelling skills relative to copying and reading? Journal of Experimental Psychology: Applied, 21, 356–369. https://doi.org/10.1037/xap0000062 Pashler, H. (1993). Doing two things at the same time. American Scientist, 81(1), 48–55. Paterson, H. M., &amp; Kemp, R. I. (2006). Co-witnesses talk: A survey of eyewitness discussion. Psychology, Crime &amp; Law, 12(2), 181–191. Penfield, W., &amp; Roberts, L. (1959). Speech &amp; brain mechanisms princeton, NJ princeton uni. Press. Petitto, L. A., &amp; Marentette, P. F. (1991). Babbling in the manual mode: Evidence for the ontogeny of language. Science, 251(5000), 1493–1496. Pickering, M. J., &amp; Ferreira, V. S. (2008). Structural priming: A critical review. Psychological Bulletin, 134(3), 427. Pines, M. (1981). The civilizing of genie. Psychology Today, 15(9), 28–34. Plassmann, H., O’doherty, J., Shiv, B., &amp; Rangel, A. (2008). Marketing actions can modulate neural representations of experienced pleasantness. Proceedings of the National Academy of Sciences, 105(3), 1050–1054. Polyn, S. M., Norman, K. A., &amp; Kahana, M. J. (2009). A context maintenance and retrieval model of organization. Psychology Review, 116, 129–156. https://doi.org/10.1037/a0014420.A Porter, S., Yuille, J. C., &amp; Lehman, D. R. (1999). The nature of real, implanted, and fabricated memories for emotional childhood events: Implications for the recovered memory debate. Law Hum. Behav., 23(5), 517–537. Pratkanis, A. R., Greenwald, A. G., Leippe, M. R., &amp; Baumgardner, M. H. (1988). In search of reliable persuasion effects: III. The sleeper effect is dead: Long live the sleeper effect. J. Pers. Soc. Psychol., 54(2), 203–218. Proctor, R. W., &amp; Van Zandt, T. (2008). Human factors in simple and complex systems. (2nd ed.). CRC Press. Project, I. (2023). Innocence project. Retrieved from https://www.innocenceproject.org/all-cases/. Quillian, M. R. (1967). Word concepts: A theory and simulation of some basic semantic capabilities. Behavioral Science, 12(5), 410–430. Quinn, P. O., &amp; Madhoo, M. (2014). A review of attention-deficit/hyperactivity disorder in women and girls: Uncovering this hidden diagnosis. The Primary Care Companion for CNS Disorders, 16(3), 27250. Ramachandran, V. S., Hubbard, E. M., Robertson, L. C., &amp; Sagiv, N. (2005). The emergence of the human mind: Some clues from synesthesia. In synesthesia: Perspectives from cognitive neuroscience. (2nd ed., pp. 147–190). Oxford University Press. Rasch, B., Büchel, C., Gais, S., &amp; Born, J. (2007). Odor cues during slow-wave sleep prompt declarative memory consolidation. Science, 315(5817), 1426–1429. Rassin, E., Merckelbach, H., &amp; Spaan, V. (2001). When dreams become a royal road to confusion: Realistic dreams, dissociation, and fantasy proneness. J. Nerv. Ment. Dis., 189(7), 478–481. Rickard, T. C. (2020). Extension of the dual-memory model of test-enhanced learning to distributions and individual differences. Psychonomic Bulletin &amp; Review, 27(4), 783–790. Rickard, T. C., &amp; Pan, S. C. (2018). A dual memory theory of the testing effect. Psychonomic Bulletin and Review, 25, 847–869. https://doi.org/10.3758/s13423-017-1298-4 Rips, L. J., Shoben, E. J., &amp; Smith, E. E. (1973). Semantic distance and the verification of semantic relations. Journal of Verbal Learning and Verbal Behavior, 12(1), 1–20. Rogers, T. B., Kuiper, N. A., &amp; Kirker, W. S. (1977). Self-reference and the encoding of personal information. J. Pers. Soc. Psychol., 35(9), 677–688. Rosch, E. H. (1973). On the internal structure of perceptual and semantic categories. In Cognitive development and acquisition of language (pp. 111–144). Elsevier. Rosch, E., &amp; Mervis, C. B. (1975). Family resemblance: Studies in the internal structure of categories. Cognitive Psychology, 7, 573–605. Rosch, E., Simpson, C., &amp; Miller, R. S. (1976). Structural bases of typicality effects. J. Exp. Psychol. Hum. Percept. Perform., 2(4), 491–502. Runco, M. A., &amp; Jaeger, G. J. (2012). The standard definition of creativity. Creativity Research Journal, 24(1), 92–96. Runeson, S. (1988). The distorted room illusion, equivalent configurations, and the specificity of static optic arrays. Journal of Experimental Psychology: Human Perception and Performance, 14(2), 295–304. Saffran, J. R., Aslin, R. N., &amp; Newport, E. L. (1996). Statistical learning by 8-month-old infants. Science, 274(5294), 1926–1928. Sapir, E., &amp; Whorf, B. (1956). Language, thought, and reality. Selected Writings. Savage-Rumbaugh, S., &amp; Lewin, R. (1994). Kanzi: The ape at the brink of the human mind. John Wiley &amp; Sons. Schacter, D. L. (1999). The seven sins of memory: Insights from psychology and cognitive neuroscience. American Psychologist, 54(3), 182. Schill, H. M., Wolfe, J. M., &amp; Brady, T. F. (2021). Relationships between expertise and distinctiveness: Abnormal medical images lead to enhanced memory performance only in experts. Memory &amp; Cognition, 49(6), 1067–1081. Schurgin, M. W., Wixted, J. T., &amp; Brady, T. F. (2020). Psychophysical scaling reveals a unified theory of visual memory strength. Nature Human Behaviour, 4(11), 1156–1172. Seamon, J. G., Philbin, M. M., &amp; Harrison, L. G. (2006). Do you remember proposing marriage to the pepsi machine? False recollections from a campus walk. Psychon. Bull. Rev., 13(5), 752–756. Sekuler, R., &amp; Blake, R. (2006). Perception. McGraw-Hill Companies,Incorporated. https://books.google.com/books?id=jKVFAAAAYAAJ Semenuks, A., Phillips, W., Dalca, I., Kim, C., &amp; Boroditsky, L. (2017). Effects of grammatical gender on object description. CogSci. Seyfarth, R. M., &amp; Cheney, D. L. (1997). Behavioral mechanisms underlying vocal communication in nonhuman primates. Anim. Learn. Behav., 25(3), 249–267. Silveira, J. M. (1971). Incubation: The effect of interruption timing and length on problem solution and quality of problem processing. Unpublished doctoral dissertation. University of Oregon. Silverstein, L. D., Krantz, J. H., Gomer, F. E., Yeh, Y., &amp; Monty, R. W. (1990). The effects of spatial sampling and luminance quantization on the image quality of color matrix displays. Journal of the Optical Society of America, Part A(7), 1955–1968. Silverstein, L. D., &amp; Merrifield, R. M. (1985). The development and evaluation of color systems for airborne applications: Phase i fundamental visual, perceptual, and display systems considerations (tech. Report DOT/FAA/PM085019) (5th ed., pp. 1955–1968). Federal Aviation Administration. Simons, D. J., &amp; Chabris, C. F. (1999). Gorillas in our midst: Sustained inattentional blindness for dynamic events. Perception, 28(9), 1059–1074. Simonton, D. K. (2018). Defining creativity: Don’t we also need to define what is not creative? The Journal of Creative Behavior, 52(1), 80–90. Skaugset, L. M., Farrell, S., Carney, M., Wolff, M., Santen, S. A., Perry, M., &amp; Cico, S. J. (2016). Can you multitask? Evidence and limitations of task switching and multitasking in emergency medicine. Annals of Emergency Medicine, 68(2), 189–195. Smith, J. D., &amp; Minda, J. P. (1998). Prototypes in the mist: The early epochs of category learning. J. Exp. Psychol. Learn. Mem. Cogn., 24(6), 1411–1436. Smith, S. M., Ward, T. B., &amp; Finke, R. A. (1995). The creative cognition approach. MIT press. Smith, S. M., Ward, T. B., &amp; Schumacher, J. S. (1993). Constraining effects of examples in a creative generation task. Memory &amp; Cognition, 21, 837–845. Spelke, E., Hirst, W., &amp; Neisser, U. (1976). Skills of divided attention. Cognition, 4(3), 215–230. Stangor, C., &amp; McMillan, D. (1992). Memory for expectancy-congruent and expectancy-incongruent information: A review of the social and social developmental literatures. Psychological Bulletin, 111(1), 42. State v. guilbert (Vol. 306, p. 705). (2012). (Vol. 306). Connecticut Supreme Court. Stern, M., &amp; Karraker, K. H. (1989). Sex stereotyping of infants: A review of gender labeling studies. Sex Roles, 20(9–10), 501–522. Still, G. F. (1902). The goulstonian lectures. Some Abnormal Psychical Conditions in Children, 1008–1012. Stoffregen, T. A., &amp; Bardy, B. G. (2001). On specification and the senses. Behavioral and Brain Sciences, 24(2), 195–213. Strayer, D. L., &amp; Drews, F. A. (2007). Cell-phone induced inattention blindness. Current Directions in Psychological Science, 16, 128–131. Strayer, D. L., &amp; Johnston, W. A. (2001). Driven to distraction: Dual-task studies of simulated driving and conversing on a cellular telephone. Psychol. Sci., 12(6), 462–466. Strayer, D. L., Watson, J. M., &amp; Drews, F. A. (2011). Cognitive distraction while multitasking in the automobile. In Advances in research and theory (pp. 29–58). Elsevier. Stroop, J. R. (1935). Studies of interference in serial verbal reactions. Journal of Experimental Psychology, 18(6), 643. Takarangi, M. K., Parker, S., &amp; Garry, M. (2006). Modernising the misinformation effect: The development of a new stimulus set. Applied Cognitive Psychology: The Official Journal of the Society for Applied Research in Memory and Cognition, 20(5), 583–590. Talarico, J. M., &amp; Rubin, D. C. (2003). Confidence, not consistency, characterizes flashbulb memories. Psychological Science, 14(5), 455–461. Technical Working Group for Eyewitness Evidence. (1999). Eyewitness evidence: A guide for law enforcement. U.S. Department of Justice, Office of Justice Programs. Thomas, A. K., McKinney de Royston, M., &amp; Powell, S. (2023). Color-evasive cognition: The unavoidable impact of scientific racism in the founding of a field. Current Directions in Psychological Science, 32(2), 137–144. Tiego, J., Testa, R., Bellgrove, M. A., Pantelis, C., &amp; Whittle, S. (2018). A hierarchical model of inhibitory control. Frontiers in Psychology, 9, 1339. Treisman, A. M. (1960). Contextual cues in selective listening. Quarterly Journal of Experimental Psychology, 12(4), 242–248. Trope, Y., &amp; Thompson, E. P. (1997). Looking for truth in all the wrong places? Asymmetric search of individuating information about stereotyped group members. Journal of Personality and Social Psychology, 73(2), 229. Tulving, E., &amp; Thomson, D. M. (1973). Encoding specificity and retrieval processes in episodic memory. Psychological Review, 80(5), 352. Tversky, A., &amp; Kahneman, D. (1973). Availability: A heuristic for judging frequency and probability. Cognitive Psychology, 5(2), 207–232. Tversky, A., &amp; Kahneman, D. (1983). Extensional versus intuitive reasoning: The conjunction fallacy in probability judgment. Psychological Review, 90(4), 293. United states v. greene (Vol. 704, p. 298). (2013). (Vol. 704). United States Court of Appeals, Fourth Circuit. Von Frisch, K. (1950). Bees: Their vision, chemical senses, and language. Cornell University Press. Von Restorff, H. (1933). Über die wirkung von bereichsbildungen im spurenfeld. Psychologische Forschung, 18(1), 299–342. Wade, K. A., Garry, M., Don Read, J., &amp; Lindsay, D. S. (2002). A picture is worth a thousand lies: Using false photographs to create false childhood memories. Psychonomic Bulletin &amp; Review, 9(3), 597–603. Wason, P. C. (1960). On the failure to eliminate hypotheses in a conceptual task. Q. J. Exp. Psychol., 12(3), 129–140. Watkins, O. C., &amp; Watkins, M. J. (1975). Buildup of proactive inhibition as a cue-overload effect. Journal of Experimental Psychology: Human Learning and Memory, 1(4), 442. Watson, J. M., &amp; Strayer, D. L. (2010). Supertaskers: Profiles in extraordinary multitasking ability. Psychon. Bull. Rev., 17(4), 479–485. Waxman, S. R. (1990). Linguistic biases and the establishment of conceptual hierarchies: Evidence from preschool children. Cogn. Dev., 5(2), 123–150. Wells, G. L., Kovera, M. B., Douglass, A. B., Brewer, N., Meissner, C. A., &amp; Wixted, J. T. (2020). Policy and procedure recommendations for the collection and preservation of eyewitness identification evidence. Law and Human Behavior, 44(1), 3–36. https://doi.org/10.1037/lhb0000359 Wells, G. L., Memon, A., &amp; Penrod, S. D. (2006). Eyewitness evidence: Improving its probative value. Psychological Science in the Public Interest, 7(2), 45–75. Wells, G. L., &amp; Olson, E. A. (2003). Eyewitness testimony. Annual Review of Psychology, 54(1), 277–295. Wells, G. L., Small, M., Penrod, S., Malpass, R. S., Fulero, S. M., &amp; Brimacombe, C. A. E. (1998). Eyewitness identification procedures: Recommendations for lineups and photospreads. Law and Human Behavior, 22(6), 603–647. https://doi.org/10.1023/A:1025750605807 Werker, J. F., &amp; Tees, R. C. (2002). Cross-language speech perception: Evidence for perceptual reorganization during the first year of life. Infant Behav. Dev., 25(1), 121–133. Wertheimer, M. (1938). Gestalt theory. In W. D. Ellis (Ed.), A source book of gestalt psychology (pp. 1–11). Harcourt. Wertheimer, M. (1945). Productive thinking. Harper New York. Whitmore, N. W., Bassard, A. M., &amp; Paller, K. A. (2022). Targeted memory reactivation of face-name learning depends on ample and undisturbed slow-wave sleep. NPJ Sci. Learn., 7(1), 1. Wiley, J. (1998). Expertise as mental set: The effects of domain knowledge in creative problem solving. Memory &amp; Cognition, 26, 716–730. Williams, J. R., Markov, Y. A., Tiurina, N. A., &amp; Störmer, V. S. (2022). What you see is what you hear: Sounds alter the contents of visual perception. Psychological Science, 09567976221121348. Wilson, D. K., Kaplan, R. M., &amp; Schneiderman, L. J. (1987). Framing of decisions and selections of alternatives in health care. Social Behaviour, 2(1), 51–59. Winawer, J., Witthoft, N., Frank, M. C., Wu, L., Wade, A. R., &amp; Boroditsky, L. (2007). Russian blues reveal effects of language on color discrimination. Proceedings of the National Academy of Sciences, 104(19), 7780–7785. Winograd, E., Peluso, J. P., &amp; Glover, T. A. (1998). Individual differences in susceptibility to memory illusions. Appl. Cogn. Psychol., 12(7), S5–S27. Wixted, J. T. (2024). Atkinson and shiffrin’s (1968) influential model overshadowed their contemporary theory of human memory. Journal of Memory and Language, 136, 104471. Wixted, J. T., Mickes, L., &amp; Fisher, R. P. (2018). Rethinking the reliability of eyewitness memory. Perspectives on Psychological Science, 13(3), 324–335. Wixted, J. T., &amp; Wells, G. L. (2017). The relationship between eyewitness confidence and identification accuracy: A new synthesis. Psychological Science in the Public Interest, 18(1), 10–65. Wixted, J. T., Wells, G. L., Loftus, E. F., &amp; Garrett, B. L. (2021). Test a witness’s memory of a suspect only once. Psychological Science in the Public Interest, 22(1_suppl), 1S–18S. https://doi.org/10.1177/15291006211026259 Yarbus, A. L. (1967). Eye movements and vision. Plenum Press. Yilmaz, A. S., Shen, K. J., &amp; Wixted, J. T. (2023). The principles of memory versus the federal rules of evidence. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
